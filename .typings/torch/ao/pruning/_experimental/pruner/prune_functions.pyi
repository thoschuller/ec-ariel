from .parametrization import BiasHook as BiasHook, FakeStructuredSparsity as FakeStructuredSparsity
from torch import Tensor as Tensor, nn as nn
from torch.nn.utils import parametrize as parametrize
from torch.nn.utils.parametrize import ParametrizationList as ParametrizationList
from typing import Callable

def _remove_bias_handles(module: nn.Module) -> None: ...
def _get_adjusted_next_layer_bias(next_layer: nn.Module, pruned_biases: Tensor, mask: Tensor) -> nn.Parameter:
    """Returns new adjusted bias for the second supported module"""
def _prune_module_bias(module: nn.Module, mask: Tensor) -> None:
    """Applies mask to given modules bias"""
def _propagate_module_bias(module: nn.Module, mask: Tensor) -> Tensor | None:
    """
    In the case that we need to propagate biases, this function will return the biases we need
    """
def _prune_linear_helper(linear: nn.Linear) -> Tensor: ...
def prune_linear(linear: nn.Linear) -> None: ...
def prune_linear_linear(linear1: nn.Linear, linear2: nn.Linear) -> None: ...
def prune_linear_activation_linear(linear1: nn.Linear, activation: Callable[[Tensor], Tensor] | None, linear2: nn.Linear): ...
def _prune_conv2d_helper(conv2d: nn.Conv2d) -> Tensor: ...
def prune_conv2d_padded(conv2d_1: nn.Conv2d) -> None: ...
def prune_conv2d(conv2d: nn.Conv2d) -> None: ...
def prune_conv2d_conv2d(conv2d_1: nn.Conv2d, conv2d_2: nn.Conv2d) -> None: ...
def prune_conv2d_activation_conv2d(conv2d_1: nn.Conv2d, activation: Callable[[Tensor], Tensor] | None, conv2d_2: nn.Conv2d):
    """
    Fusion Pattern for conv2d -> some activation module / function -> conv2d layers
    """
def prune_conv2d_pool_activation_conv2d(c1: nn.Conv2d, pool: nn.Module, activation: Callable[[Tensor], Tensor] | None, c2: nn.Conv2d) -> None: ...
def prune_conv2d_activation_pool_conv2d(c1: nn.Conv2d, activation: Callable[[Tensor], Tensor] | None, pool: nn.Module, c2: nn.Conv2d) -> None: ...
def prune_conv2d_pool_flatten_linear(conv2d: nn.Conv2d, pool: nn.Module, flatten: Callable[[Tensor], Tensor] | None, linear: nn.Linear) -> None: ...
def prune_lstm_output_linear(lstm: nn.LSTM, getitem: Callable, linear: nn.Linear) -> None: ...
def prune_lstm_output_layernorm_linear(lstm: nn.LSTM, getitem: Callable, layernorm: nn.LayerNorm | None, linear: nn.Linear) -> None: ...
