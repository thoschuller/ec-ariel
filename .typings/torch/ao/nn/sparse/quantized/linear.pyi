import torch
from _typeshed import Incomplete

__all__ = ['LinearPackedParams', 'Linear']

class LinearPackedParams(torch.nn.Module):
    _version: int
    dtype: Incomplete
    def __init__(self, row_block_size: int = 1, col_block_size: int = 4, dtype=...) -> None: ...
    def _get_name(self): ...
    _packed_params: Incomplete
    @torch.jit.export
    def set_weight_bias(self, weight: torch.Tensor, bias: torch.Tensor | None, row_block_size: int | None, col_block_size: int | None) -> None: ...
    @torch.jit.export
    def _weight_bias(self): ...
    def forward(self, x): ...
    def _save_to_state_dict(self, destination, prefix, keep_vars) -> None: ...
    def _load_from_state_dict(self, state_dict, prefix, local_metadata, strict, missing_keys, unexpected_keys, error_msgs) -> None: ...
    @torch.jit.export
    def __getstate__(self): ...
    @torch.jit.export
    def __setstate__(self, state) -> None: ...
    def __repr__(self) -> str: ...

class Linear(torch.nn.Module):
    """
    A quantized sparse linear module with quantized tensor as inputs and outputs.
    """
    _version: int
    _FLOAT_MODULE = torch.nn.Linear
    in_features: Incomplete
    out_features: Incomplete
    _packed_params: Incomplete
    scale: float
    zero_point: int
    def __init__(self, in_features, out_features, row_block_size, col_block_size, bias: bool = True, dtype=...) -> None: ...
    @classmethod
    def _get_name(cls): ...
    def extra_repr(self): ...
    def __repr__(self) -> str: ...
    def forward(self, x: torch.Tensor) -> torch.Tensor: ...
    def _save_to_state_dict(self, destination, prefix, keep_vars) -> None: ...
    def _load_from_state_dict(self, state_dict, prefix, local_metadata, strict, missing_keys, unexpected_keys, error_msgs) -> None: ...
    def _weight_bias(self): ...
    def weight(self): ...
    def bias(self): ...
    def set_weight_bias(self, w: torch.Tensor, b: torch.Tensor | None, row_block_size: int | None, col_block_size: int | None) -> None: ...
    @classmethod
    def from_float(cls, mod, use_precomputed_fake_quant: bool = False):
        """Create a quantized sparse module from a float module.

        We only care about the convert at this stage, no need for observers just yet.

        TODO(zaf): Need to add the sparse params to the qconfig
        """
