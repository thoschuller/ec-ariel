import torch
from _typeshed import Incomplete
from dataclasses import dataclass
from torch import Tensor as Tensor
from torch._logging import getArtifactLogger as getArtifactLogger
from torch._subclasses.fake_tensor import FakeTensor as FakeTensor
from torch._subclasses.functional_tensor import FunctionalTensor as FunctionalTensor
from torch._subclasses.meta_utils import is_sparse_any as is_sparse_any
from torch.fx.experimental.symbolic_shapes import SymIntEqByExpr as SymIntEqByExpr, guard_or_false as guard_or_false, sym_eq as sym_eq
from torch.multiprocessing.reductions import StorageWeakRef as StorageWeakRef
from torch.utils._python_dispatch import is_traceable_wrapper_subclass as is_traceable_wrapper_subclass, transform_subclass as transform_subclass

aot_joint_log: Incomplete

def to_fun(t): ...
def sync_functional_tensor(t) -> None: ...
def from_fun(t): ...
def is_fun(t): ...
def has_data_mutation(t): ...
def are_all_mutations_hidden_from_autograd(t): ...
def are_all_mutations_under_no_grad_or_inference_mode(t): ...
def was_inductor_storage_resized(t): ...
def has_metadata_mutation(f_arg, arg, *, check_only_storage_mutation: bool): ...
def gen_alias_from_base(aliased_base_tensor, target_meta_tensor, target_requires_grad, target_functional_tensor: FunctionalTensorMetadataEq | None = None, *, replay_views): ...
def has_same_metadata(t1, t2): ...

@dataclass(frozen=True)
class MetadataKey:
    """
    This should be equal whenever has_same_metadata would return True
    """
    size: tuple[SymIntEqByExpr, ...]
    layout: torch.layout
    is_sparse: bool
    stride: tuple[SymIntEqByExpr, ...] | None
    storage_offset: SymIntEqByExpr | None
    is_conj: bool
    is_neg: bool
    @staticmethod
    def make(t): ...

class FunctionalTensorMetadataEq:
    tensor: Incomplete
    def __init__(self, tensor: torch.Tensor) -> None: ...
    def __eq__(self, other: object) -> bool: ...

def was_tensor_updated(arg, new_arg): ...
def was_tensor_metadata_updated(arg, new_arg): ...
def assert_functional_graph(fx_g: torch.fx.Graph) -> int: ...
def propagate_input_mutation_stacktraces(fx_g: torch.fx.Graph) -> None: ...
def _check_if_mutation_can_be_in_graph(keep_input_mutations: bool, mutates_data, mutates_metadata, mutations_hidden_from_autograd, mutations_under_no_grad_or_inference_mode, mutates_storage_metadata, mutation_inductor_storage_resize, requires_grad): ...
