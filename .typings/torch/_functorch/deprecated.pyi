import torch.nn as nn
from torch._functorch.eager_transforms import argnums_t as argnums_t
from torch._functorch.vmap import in_dims_t as in_dims_t, out_dims_t as out_dims_t
from typing import Any, Callable

def get_warning(api, new_api=None, replace_newlines: bool = False): ...
def warn_deprecated(api, new_api=None) -> None: ...
def setup_docs(functorch_api, torch_func_api=None, new_api_name=None) -> None: ...
def vmap(func: Callable, in_dims: in_dims_t = 0, out_dims: out_dims_t = 0, randomness: str = 'error', *, chunk_size=None) -> Callable: ...
def grad(func: Callable, argnums: argnums_t = 0, has_aux: bool = False) -> Callable: ...
def grad_and_value(func: Callable, argnums: argnums_t = 0, has_aux: bool = False) -> Callable: ...
def vjp(func: Callable, *primals, has_aux: bool = False): ...
def jvp(func: Callable, primals: Any, tangents: Any, *, strict: bool = False, has_aux: bool = False): ...
def jacrev(func: Callable, argnums: int | tuple[int] = 0, *, has_aux: bool = False, chunk_size: int | None = None, _preallocate_and_copy: bool = False): ...
def jacfwd(func: Callable, argnums: argnums_t = 0, has_aux: bool = False, *, randomness: str = 'error'): ...
def hessian(func, argnums: int = 0): ...
def functionalize(func: Callable, *, remove: str = 'mutations') -> Callable: ...
def make_functional(model: nn.Module, disable_autograd_tracking: bool = False): ...
def make_functional_with_buffers(model: nn.Module, disable_autograd_tracking: bool = False): ...
def combine_state_for_ensemble(models): ...
