from _typeshed import Incomplete
from torch._C._functorch import TransformType as TransformType, _unwrap_for_grad as _unwrap_for_grad, _wrap_for_grad as _wrap_for_grad, current_level as current_level
from torch._functorch.apis import vmap as vmap
from torch._functorch.utils import enable_single_level_autograd_function as enable_single_level_autograd_function
from torch._functorch.vmap import _add_batch_dim as _add_batch_dim, _broadcast_to_and_flatten as _broadcast_to_and_flatten, restore_vmap as restore_vmap, unwrap_batched as unwrap_batched, wrap_batched as wrap_batched
from torch._ops import HigherOrderOperator as HigherOrderOperator
from torch.autograd.forward_ad import _set_fwd_grad_enabled as _set_fwd_grad_enabled
from typing import NamedTuple

class CustomFunctionHigherOrderOperator(HigherOrderOperator):
    def __init__(self) -> None: ...
    def __call__(self, autograd_function, *args, **kwargs): ...

custom_function_call: Incomplete

def custom_function_call_grad(interpreter, autograd_function, *operands): ...
def generate_single_level_function(interpreter, autograd_function): ...

NO_OUT_DIMS: str

def wrap_outputs_maintaining_identity(outputs, unwrapped_inputs, orig_inputs, wrap_fn, out_dims=...): ...

class VmapInfo(NamedTuple):
    batch_size: int
    randomness: str

def has_overridden_vmap_rule(autograd_function): ...
def validate_vmap_returns_tuple_of_two_elements(result) -> None: ...
def custom_function_call_vmap(interpreter, autograd_function, *operands, **kwargs): ...
def custom_function_call_vmap_helper(interpreter, vmap_function, op, *operands, **kwargs): ...
def unpack_outputs(outputs): ...
def custom_function_call_vmap_generate_rule(interpreter, autograd_function, *operands): ...
def custom_function_call_functionalize(interpreter, autograd_function, generate_vmap_rule, *operands) -> None: ...
def vmapify_autograd_function(autograd_function, in_dims, batch_size, randomness): ...
def get_tangents_in_dims(input_dims, tangents): ...

class WrappedCtx:
    _pt_reserved_attrs: tuple[str, ...]
    _pt_inner_ctx: Incomplete
    def __init__(self, ctx) -> None: ...
    def __getattr__(self, name): ...
    def __setattr__(self, name, value): ...

class CtxWithSavedTensors(WrappedCtx):
    _pt_reserved_attrs: Incomplete
    _pt_new_saved_tensors: Incomplete
    def __init__(self, ctx, new_saved_tensors) -> None: ...
    @property
    def saved_tensors(self): ...

class CtxCustomSave(WrappedCtx):
    _pt_reserved_attrs: Incomplete
    _pt_saved_tensors_bdims: Incomplete
    _pt_current_level: Incomplete
    def __init__(self, ctx, current_level) -> None: ...
    def save_for_backward(self, *tensors) -> None: ...
    def save_for_forward(self, *tensors) -> None: ...

def reductify(grad_input, grad_input_bdim, input_bdim, batch_size, target_shape_without_bdim_to_reduce_to=None): ...
def reductify_leaf(grad_input, grad_input_bdim, input_bdim, batch_size, target_shape_without_bdim_to_reduce_to=None): ...
def autograd_function_forward_rewritten(original_forward, original_setup_context): ...

class AutogradFunctionApply(HigherOrderOperator):
    def __init__(self) -> None: ...
    def __call__(self, fwd, bwd, *fwd_args, **fwd_kwargs): ...

autograd_function_apply: Incomplete
