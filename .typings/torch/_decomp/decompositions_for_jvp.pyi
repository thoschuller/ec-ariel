import torch
import torch._decomp
from _typeshed import Incomplete
from torch import Tensor as Tensor
from torch._prims_common.wrappers import _maybe_remove_out_wrapper as _maybe_remove_out_wrapper
from typing import Callable

decomposition_table: Incomplete
decomposition_table_for_jvp: dict[torch._ops.OperatorBase, Callable]
register_decomposition = torch._decomp.register_decomposition
aten: Incomplete

def maybe_register_decomposition(op): ...
def register_decomposition_for_jvp(fn): ...
def _register_jit_decomposition_for_jvp(decomp, use_python: bool = False): ...
def trace(self) -> Tensor: ...
def log_sigmoid_forward(self) -> tuple[Tensor, Tensor]: ...
def recompute_mean_var(input: Tensor, rstd: Tensor, inner_dim_indices: list[int], keepdim: bool): ...
def native_layer_norm_backward(grad_out: Tensor, input: Tensor, normalized_shape: list[int], mean: Tensor, rstd: Tensor, weight: Tensor | None, bias: Tensor | None, output_mask: list[bool]) -> tuple[Tensor | None, Tensor | None, Tensor | None]: ...
def prod(x: list[int]): ...
def native_batch_norm_backward(grad_out: Tensor, input: Tensor, weight: Tensor | None, running_mean: Tensor | None, running_var: Tensor | None, save_mean: Tensor | None, save_invstd: Tensor | None, train: bool, eps: float, output_mask: list[bool]) -> tuple[Tensor, Tensor | None, Tensor | None]: ...
def batch_norm_backward(grad_out: Tensor, input: Tensor, weight: Tensor, running_mean: Tensor | None, running_var: Tensor | None, save_mean: Tensor | None, save_var: Tensor | None, update: bool, eps: float, output_mask: list[bool], reserve: Tensor) -> tuple[Tensor, Tensor | None, Tensor | None]: ...
