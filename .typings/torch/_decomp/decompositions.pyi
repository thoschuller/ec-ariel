import torch
import torch._prims_common as utils
from _typeshed import Incomplete
from collections.abc import Iterable
from enum import Enum
from torch import Tensor as Tensor, sym_float as sym_float, sym_int as sym_int
from torch._decomp import register_decomposition as register_decomposition
from torch._higher_order_ops.out_dtype import out_dtype as out_dtype
from torch._prims_common import IntLike as IntLike, NumberType as NumberType, TensorLike as TensorLike, TensorSequenceType as TensorSequenceType, suggest_memory_format as suggest_memory_format
from torch._prims_common.wrappers import _maybe_convert_to_dtype as _maybe_convert_to_dtype, _maybe_resize_out as _maybe_resize_out, _safe_copy_out as _safe_copy_out, out_wrapper as out_wrapper
from torch.utils._pytree import tree_map as tree_map
from typing import Callable

DispatchKey = torch._C.DispatchKey
__all__: list[str]
aten: Incomplete

class Reduction(Enum):
    NONE = 0
    MEAN = 1
    SUM = 2

def type_casts(f: Callable, type_promotion: utils.ELEMENTWISE_TYPE_PROMOTION_KIND, compute_dtype_only: bool = False, include_non_tensor_args: bool = False): ...

compute_only_pw_cast_for_opmath: Incomplete
pw_cast_for_opmath: Incomplete
pw_cast_for_opmath_non_tensor_args: Incomplete
pw_cast_for_int_to_real: Incomplete

def _unsqueeze_to_dim(x: Tensor, dim: int) -> Tensor: ...
@pw_cast_for_opmath
def tanh_backward(out_grad: Tensor, y: Tensor): ...
@pw_cast_for_opmath
def sigmoid_backward(out_grad: Tensor, y: Tensor): ...
@pw_cast_for_opmath
def softplus_backward(out_grad: Tensor, x: Tensor, beta: float, threshold: float): ...
@pw_cast_for_opmath
def elu_backward(grad_output: Tensor, alpha: float, scale: float, input_scale: float, is_result: bool, self_or_result: Tensor): ...
def fill_scalar(self, value): ...
def fill_tensor(self, value: Tensor): ...
@pw_cast_for_opmath
def hardsigmoid(self) -> Tensor: ...
@pw_cast_for_opmath
def hardsigmoid_backward(grad_output: Tensor, self: Tensor): ...
def hardtanh_backward(grad_output: Tensor, self: Tensor, min_val: float, max_val: float): ...
@pw_cast_for_opmath
def hardswish(self) -> Tensor: ...
@pw_cast_for_opmath
def hardswish_backward(grad_output: Tensor, self: Tensor) -> Tensor: ...
def threshold_backward(grad_output: Tensor, self: Tensor, threshold: float): ...
@pw_cast_for_opmath
def leaky_relu_backward(grad_output: Tensor, self: Tensor, negative_slope: float, self_is_result: bool): ...
@pw_cast_for_opmath
def gelu_backward(grad: Tensor, self: Tensor, approximate: str = 'none'): ...
@pw_cast_for_opmath
def mish_backward(grad_output: Tensor, input: Tensor): ...
@pw_cast_for_opmath
def silu(self) -> Tensor: ...
@pw_cast_for_opmath
def silu_backward(grad_output: Tensor, self: Tensor) -> Tensor: ...
def _prelu_kernel(self, weight: Tensor) -> Tensor: ...
def _prelu_kernel_backward(grad_output: Tensor, self: Tensor, weight: Tensor) -> tuple[Tensor, Tensor]: ...
@pw_cast_for_opmath
def rrelu_with_noise_backward(grad_output: Tensor, self: Tensor, noise: Tensor, lower: float, upper: float, training: bool, self_is_result: bool) -> Tensor: ...
@pw_cast_for_opmath
def log_sigmoid_backward(grad_output: Tensor, self: Tensor, buffer: Tensor) -> Tensor: ...
def apply_loss_reduction(loss: Tensor, reduction: int): ...
def to_real_dtype(dtype: torch.dtype): ...
@pw_cast_for_opmath
def mse_loss(self, target: Tensor, reduction: int = ...) -> Tensor: ...
@pw_cast_for_opmath
def mse_loss_backward(grad_output: Tensor, input: Tensor, target: Tensor, reduction: int): ...
def safe_softmax(self, dim, dtype=None): ...
@pw_cast_for_opmath
def smooth_l1_loss(self, target: Tensor, reduction: int = ..., beta: float = 1.0): ...
@pw_cast_for_opmath
def smooth_l1_loss_backward(grad_output: Tensor, self: Tensor, target: Tensor, reduction: int, beta: float): ...
@pw_cast_for_opmath
def smooth_l1_loss_backward_out(grad_output: Tensor, self: Tensor, target: Tensor, reduction: int, beta: float, grad_input: Tensor): ...
@pw_cast_for_opmath
def huber_loss_backward(grad_output: Tensor, self: Tensor, target: Tensor, reduction: int, delta: float): ...
@pw_cast_for_opmath
def huber_loss_backward_out(grad_output: Tensor, self: Tensor, target: Tensor, reduction: int, delta: float, grad_input: Tensor): ...
def _nll_loss_backward(grad_output: Tensor, self: Tensor, target: Tensor, weight: Tensor | None, reduction: int, ignore_index: int, total_weight: Tensor) -> Tensor: ...
@pw_cast_for_opmath
def glu_backward(grad_output: Tensor, self: Tensor, dim: int) -> Tensor: ...
def nll_loss_backward(grad_output: Tensor, self: Tensor, target: Tensor, weight: Tensor | None, reduction: int, ignore_index: int, total_weight: Tensor) -> Tensor: ...
def nll_loss2d_backward(grad_output: Tensor, self: Tensor, target: Tensor, weight: Tensor | None, reduction: int, ignore_index: int, total_weight: Tensor) -> Tensor: ...
@pw_cast_for_opmath
def binary_cross_entropy(self, target: Tensor, weight: Tensor | None = None, reduction: int = ...) -> Tensor: ...
@pw_cast_for_opmath
def binary_cross_entropy_backward(grad_output: Tensor, self: Tensor, target: Tensor, weight: Tensor | None = None, reduction: int = ...) -> Tensor: ...
@pw_cast_for_opmath
def soft_margin_loss(input: Tensor, target: Tensor, reduction: int = ...) -> Tensor: ...
@pw_cast_for_opmath
def soft_margin_loss_backward(grad_output: Tensor, self: Tensor, target: Tensor, reduction: int = ...) -> Tensor: ...
def dist(input: Tensor, other: Tensor, p: float = 2): ...
def _euclidean_dist(x1: Tensor, x2: Tensor) -> Tensor: ...
def slice_backward(grad_output: Tensor, input_sizes: list[int], dim: int, start: int, end: int, step: int): ...
def slice_forward(self, dim: int = 0, start: int | None = None, end: int | None = None, step: int = 1): ...
def _normalize_start_end(x: Tensor, dim: int, start: int | None, end: int | None) -> tuple[int, int]:
    """
    Normalize start and end such that both are in the range
    [0, x.get_size()[dim]] and start <= end.
    """
def slice_scatter(input: Tensor, src: Tensor, dim: int = 0, start: int | None = None, end: int | None = None, step: int = 1): ...
def select_backward(grad_output: Tensor, input_sizes: list[int], dim: int, index: int): ...
def diagonal_backward(grad_output: Tensor, input_sizes: list[int], offset: int, dim1: int, dim2: int): ...
def _cast_grad_to_input_dtype(grad_output: Tensor, grad_input: Tensor, input_dtype: torch.dtype): ...
@compute_only_pw_cast_for_opmath
def _softmax_backward_data(grad_output: Tensor, output: Tensor, dim: int, input_dtype: torch.dtype): ...
@compute_only_pw_cast_for_opmath
def _log_softmax_backward_data(grad_output: Tensor, output: Tensor, dim: int, input_dtype: torch.dtype): ...
def _im2col_col2im_indices_along_dim(input_d, kernel_d, dilation_d, padding_d, stride_d, device):
    """Utility function to implement im2col and col2im"""
def im2col(input: Tensor, kernel_size: list[int], dilation: list[int], padding: list[int], stride: list[int]) -> Tensor: ...
@pw_cast_for_opmath
def col2im(input: Tensor, output_size: list[int], kernel_size: list[int], dilation: list[int], padding: list[int], stride: list[int]) -> Tensor: ...
def native_dropout_backward(grad_output: Tensor, mask: Tensor, scale: float): ...
def unfold_backward(grad: Tensor, input_size: list[int], dimension: int, size: int, step: int) -> Tensor: ...
@pw_cast_for_opmath
def logit_backward(grad_output: Tensor, self: Tensor, eps: float | None = None) -> Tensor: ...
def dropout(input: Tensor, p: float, train: bool | None): ...
def native_dropout(input: Tensor, p: float, train: bool | None): ...
def _softmax(x: Tensor, dim: int, half_to_float: bool): ...
def _log_softmax(x: Tensor, dim: int, half_to_float: bool): ...
def embedding(weight: Tensor, indices: Tensor, padding_idx: int = -1, scale_grad_by_freq: bool = False, sparse: bool = False) -> Tensor: ...
def embedding_dense_backward(grad_output: Tensor, indices: Tensor, num_weights: int, padding_idx: int, scale_grad_by_freq: bool): ...
def prod(x: list[int]): ...
def _pad_chunk(tensors: list[Tensor], dim: int, num_chunks: int) -> list[Tensor]: ...
def have_same_ndims(tensors: list[Tensor]): ...
def leading_dimension_matches(tensors: list[Tensor], dim: int): ...
def _preprocess_chunk_cat_inputs(tensors: list[Tensor], dim: int, num_chunks: int): ...
def _chunk_cat(tensors: list[Tensor], dim: int, num_chunks: int, out: Tensor | None = None) -> Tensor: ...
def split_with_sizes_copy(self, split_sizes: list[int], dim: int = 0, out: list[Tensor] | None = None) -> list[Tensor] | None: ...
def unsafe_split(input: Tensor, split_size: int, dim: int = 0) -> tuple[Tensor, ...]: ...
def unsafe_split_with_sizes(input: Tensor, split_sizes: list[int], dim: int = 0) -> tuple[Tensor, ...]: ...
def split(self, split_size: int, dim: int = 0) -> tuple[Tensor, ...]: ...
def tensor_split_tensor_indices_or_sections_py_impl(self, tensor_indices_or_sections: Tensor, dim: int = 0) -> tuple[Tensor, ...]: ...
@pw_cast_for_opmath
def addmm(self, mat1: Tensor, mat2: Tensor, beta: int = 1, alpha: int = 1): ...
@pw_cast_for_opmath
def _addmm_activation(self, mat1: Tensor, mat2: Tensor, beta: int = 1, alpha: int = 1, use_gelu: bool = False): ...
@pw_cast_for_opmath
def addmv(self, mat1: Tensor, vec: Tensor, beta: int = 1, alpha: int = 1): ...
@pw_cast_for_opmath
def native_group_norm_backward(grad_output: Tensor, input: Tensor, mean: Tensor, rstd: Tensor, gamma: Tensor | None, N: int, C: int, HxW: int, group: int, output_mask: list[bool]) -> tuple[Tensor | None, Tensor | None, Tensor | None]: ...
def native_group_norm_backward_out(grad_output: Tensor, input: Tensor, mean: Tensor, rstd: Tensor, gamma: Tensor | None, N: int, C: int, HxW: int, group: int, output_mask: list[bool], *, out0: torch.Tensor, out1: torch.Tensor, out2: torch.Tensor) -> tuple[Tensor | None, Tensor | None, Tensor | None]: ...
def _maybe_cast(x: Tensor | None, dtype) -> Tensor | None: ...
def native_layer_norm_backward(grad_out: Tensor, input: Tensor, normalized_shape: list[int], mean: Tensor, rstd: Tensor, weight: Tensor | None, bias: Tensor | None, output_mask: list[bool]) -> tuple[Tensor | None, Tensor | None, Tensor | None]: ...
def native_layer_norm_backward_out(grad_out: Tensor, input: Tensor, normalized_shape: list[int], mean: Tensor, rstd: Tensor, weight: Tensor | None, bias: Tensor | None, output_mask: list[bool], *, out0: torch.Tensor, out1: torch.Tensor, out2: torch.Tensor) -> tuple[Tensor | None, Tensor | None, Tensor | None]: ...
def native_batch_norm_helper(input: Tensor, weight: Tensor | None, bias: Tensor | None, running_mean: Tensor | None, running_var: Tensor | None, training: bool, momentum: float, eps: float, functional: bool) -> tuple[Tensor, Tensor, Tensor, Tensor | None, Tensor | None]: ...
def native_batch_norm(input: Tensor, weight: Tensor | None, bias: Tensor | None, running_mean: Tensor | None, running_var: Tensor | None, training: bool, momentum: float, eps: float) -> tuple[Tensor, Tensor, Tensor]: ...
def native_batch_norm_decomposition(input: Tensor, weight: Tensor | None, bias: Tensor | None, running_mean: Tensor | None, running_var: Tensor | None, training: bool, momentum: float, eps: float) -> tuple[Tensor, Tensor, Tensor]: ...
def unsafe_chunk_py_impl(tensor, chunks, dim: int = 0) -> list[Tensor]: ...
def _native_batch_norm_legit_no_training(input: Tensor, weight: Tensor | None, bias: Tensor | None, running_mean: Tensor, running_var: Tensor, momentum: float, eps: float) -> tuple[Tensor, Tensor, Tensor]: ...
def _native_batch_norm_legit(input: Tensor, weight: Tensor | None, bias: Tensor | None, running_mean: Tensor, running_var: Tensor, training: bool, momentum: float, eps: float) -> tuple[Tensor, Tensor, Tensor]: ...
def _native_batch_norm_legit_no_stats(input: Tensor, weight: Tensor | None, bias: Tensor | None, training: bool, momentum: float, eps: float) -> tuple[Tensor, Tensor, Tensor]: ...
def _native_batch_norm_legit_functional(input: Tensor, weight: Tensor | None, bias: Tensor | None, running_mean: Tensor, running_var: Tensor, training: bool, momentum: float, eps: float) -> tuple[Tensor, Tensor, Tensor, Tensor, Tensor]: ...
def _get_batch_norm_reserve_tensor(input: Tensor, weight: Tensor | None, bias: Tensor | None, running_mean: Tensor, running_var: Tensor, eps: float, training: bool) -> Tensor:
    """
    Return a reserve tensor for batch norm, used only by cudnn to pass forward state to the
    backward pass. This is needed for `_batch_norm_with_update` and `_batch_norm_no_update`,
    which support a variety of backends including cudnn. We create this tensor here to get
    the correct shape in the traced graph if we detect that will call the cudnn kernel,
    and rely on DCE to avoid materializing this tensor.
    """
def _batch_norm_with_update(input: Tensor, weight: Tensor | None, bias: Tensor | None, running_mean: Tensor, running_var: Tensor, momentum: float, eps: float) -> tuple[Tensor, Tensor, Tensor, Tensor]: ...
def _batch_norm_with_update_functional(input: Tensor, weight: Tensor | None, bias: Tensor | None, running_mean: Tensor, running_var: Tensor, momentum: float, eps: float) -> tuple[Tensor, Tensor, Tensor, Tensor, Tensor, Tensor]: ...
def _batch_norm_no_update(input: Tensor, weight: Tensor | None, bias: Tensor | None, running_mean: Tensor, running_var: Tensor, momentum: float, eps: float) -> tuple[Tensor, Tensor, Tensor, Tensor]: ...
@pw_cast_for_opmath
def _fused_dropout_decomposition(input, p, generator=None): ...
def _to_copy(x: Tensor | NumberType, *, dtype: torch.dtype | None = None, layout=None, device: torch.device | None = None, pin_memory: bool = False, non_blocking: bool = False, memory_format: torch.memory_format | None = None): ...
def nop_decomposition(x): ...
def cudnn_batch_norm(input: Tensor, weight: Tensor, bias: Tensor | None, running_mean: Tensor | None, running_var: Tensor | None, training: bool, exponential_average_factor: float, epsilon: float): ...
def _broadcast_batch_norm_backward(x, broadcast_mask): ...
def batch_norm_backward(grad_out: Tensor, input: Tensor, weight: Tensor | None, running_mean: Tensor | None, running_var: Tensor | None, save_mean: Tensor | None, save_invstd: Tensor | None, train: bool, eps: float, output_mask: list[bool], reserve: Tensor) -> tuple[Tensor, Tensor | None, Tensor | None]: ...
def native_batch_norm_backward(grad_out: Tensor, input: Tensor, weight: Tensor | None, running_mean: Tensor | None, running_var: Tensor | None, save_mean: Tensor | None, save_invstd: Tensor | None, train: bool, eps: float, output_mask: list[bool]) -> tuple[Tensor, Tensor | None, Tensor | None]: ...
def native_batch_norm_backward_out(grad_out: Tensor, input: Tensor, weight: Tensor | None, running_mean: Tensor | None, running_var: Tensor | None, save_mean: Tensor | None, save_invstd: Tensor | None, train: bool, eps: float, output_mask: list[bool], *, out0: torch.Tensor, out1: torch.Tensor, out2: torch.Tensor) -> tuple[Tensor, Tensor | None, Tensor | None]: ...
def miopen_batch_norm_backward(input: Tensor, grad_output: Tensor, weight: Tensor, running_mean: Tensor | None, running_var: Tensor | None, save_mean: Tensor | None, save_var: Tensor | None, epsilon: float): ...
def cudnn_batch_norm_backward(input: Tensor, grad_output: Tensor, weight: Tensor, running_mean: Tensor | None, running_var: Tensor | None, save_mean: Tensor | None, save_var: Tensor | None, epsilon: float, reserveSpace: Tensor): ...
@pw_cast_for_opmath
def adaptive_avg_pool2d(input: Tensor, output_size: tuple[int, int]): ...
def _max_unpoolnd(self, indices: TensorLike, output_size: list[int], dim: int): ...
def max_unpool2d(self, indices: TensorLike, output_size: list[int]): ...
def max_unpool3d(input: TensorLike, indices: TensorLike, output_size: list[int], stride: list[int], padding: list[int]): ...
def index_add_(x: TensorLike, dim: int, index: TensorLike, tensor: TensorLike, *, alpha: NumberType = 1): ...
def index_add(x: TensorLike, dim: int, index: TensorLike, tensor: TensorLike, *, alpha: NumberType = 1): ...
def _index_add(x: TensorLike, dim: int, index: TensorLike, tensor: TensorLike, *, inplace: bool, alpha: NumberType = 1): ...
def pad_sequence(sequences, batch_first: bool = False, padding_value: float = 0.0): ...
def index_copy_(x: TensorLike, dim: int, index: TensorLike, tensor: TensorLike): ...
def index_copy(x: TensorLike, dim: int, index: TensorLike, tensor: TensorLike): ...
def _index_copy(x: TensorLike, dim: int, index: TensorLike, tensor: TensorLike, *, inplace: bool): ...
@pw_cast_for_opmath
def log_sigmoid_forward(self) -> tuple[Tensor, Tensor]: ...
def uniform(x: Tensor, low: bool | int | float = 0.0, high: bool | int | float = 1.0, generator: torch.Generator | None = None): ...
def uniform_(self, low: int = 0, high: int = 1, generator=None): ...
def upsample_compute_output_size(input_size, output_size, scale_factors): ...
def get_scale_value(scales, idx): ...
def _upsample_nearest_vec(input: Tensor, output_size: list[int] | None, scale_factors: list[float] | None) -> Tensor: ...
def _upsample_nearest_exact_vec(input: Tensor, output_size: list[int] | None, scale_factors: list[float] | None) -> Tensor: ...
def _compute_upsample_nearest_indices(input, output_size, scales, exact: bool = False): ...
def upsample_nearest1d(input: Tensor, output_size: list[int], scales: float | None = None) -> Tensor: ...
def upsample_nearest_exact1d(input: Tensor, output_size: list[int], scales: float | None = None) -> Tensor: ...
def upsample_nearest2d(input: Tensor, output_size: list[int], scales_h: float | None = None, scales_w: float | None = None) -> Tensor: ...
def _upsample_nearest_exact2d(input: Tensor, output_size: list[int], scales_h: float | None = None, scales_w: float | None = None) -> Tensor: ...
def upsample_nearest3d(input: Tensor, output_size: list[int], scales_d: float | None = None, scales_h: float | None = None, scales_w: float | None = None) -> Tensor: ...
def _upsample_nearest_exact3d(input: Tensor, output_size: list[int], scales_d: float | None = None, scales_h: float | None = None, scales_w: float | None = None) -> Tensor: ...
@pw_cast_for_opmath
def _upsample_nearest(input: Tensor, output_size: list[int], scales: list[float | None], exact: bool = False) -> Tensor: ...
def gather_params(params, has_biases, has_projections): ...
def params_hiddens(params, hiddens, i, bidirectional): ...
def update_hidden_for_packed(cur_hidden, last_batch_size, batch_size, hiddens): ...
def update_hidden_for_packed_reverse(cur_hidden, last_batch_size, batch_size, inp_hidden): ...
def one_layer_rnn_data(inp, hidden, params, has_biases, hidden_fn, batch_sizes, reverse: bool = False): ...
def rnn_cell(nonlinearity): ...
def rnn_cell_data(nonlinearity): ...
def one_layer_rnn(inp, hidden, params, has_biases, hidden_fn, reverse: bool = False): ...
def mkldnn_one_layer_lstm(inp, hidden, params, has_biases, reverse: bool = False): ...
def _rnn_helper(input, hidden, params, has_biases, num_layers, dropout, train, bidirectional, batch_first, layer_fn): ...
def rnn_tanh_input(input, hx, params, has_biases, num_layers, dropout, train, bidirectional, batch_first): ...
def rnn_relu_input(input, hx, params, has_biases, num_layers, dropout, train, bidirectional, batch_first): ...
def rnn_relu_data(data, batch_sizes, hx, params, has_biases, num_layers, dropout, train, bidirectional): ...
def rnn_tanh_data(data, batch_sizes, hx, params, has_biases, num_layers, dropout, train, bidirectional): ...
def lstm_cell(inp, hx, cx, hh_weight, hh_bias, hr_weight, chunk_dim): ...
def one_layer_lstm(inp, hidden, params, has_biases, reverse: bool = False): ...
def one_layer_lstm_data(inp, hidden, params, has_biases, batch_sizes, reverse: bool = False): ...
def select_one_layer_lstm_function(input, hx, params):
    """Check whether we could use decompose lstm with mkldnn_rnn_layer.
    All the below conditions need to be met:
        * ``torch._C._get_mkldnn_enabled()`` returns ``True``.
        * All the input args are on CPU.
        * The dtypes of args are either torch.float or torch.bfloat16.
        * Inference.
        * ``has_projections`` returns ``False``.

    Args:
        * input: the input sequence to LSTM
        * hx: a tuple of the input hidden state and cell state ``(h_0, c_0)`` to LSTM
        * params: the weight and bias tensors of LSTM
    """
def lstm_impl(input, hx, params, has_biases, num_layers, dropout, train, bidirectional, batch_first): ...
def lstm_data_impl(data, batch_sizes, hx, params, has_biases, num_layers, dropout, train, bidirectional): ...
def gru_cell(inp, cur_hidden, ih_weight, ih_bias, hh_weight, hh_bias): ...
def gru_cell_data(inp, cur_hidden, ih_weight, ih_bias, hh_weight, hh_bias): ...
def gru_impl_data(data, batch_sizes, hx, params, has_biases, num_layers, dropout, train, bidirectional): ...
def gru_impl(input, hx, params, has_biases, num_layers, dropout, train, bidirectional, batch_first): ...
def upsample_bilinear2d_aa_vec(input, output_size, align_corners, scale_factors): ...
def upsample_bicubic2d_aa_vec(input, output_size, align_corners, scale_factors): ...
def _upsample_linear_vec(input, output_size, align_corners, scale_factors): ...
def upsample_linear1d(input: Tensor, output_size: list[int], align_corners: bool, scales_w: float | None = None) -> Tensor: ...
def upsample_bilinear2d(input: Tensor, output_size: list[int], align_corners: bool, scales_h: float | None = None, scales_w: float | None = None) -> Tensor: ...
def upsample_trilinear3d(input: Tensor, output_size: list[int], align_corners: bool, scales_d: float | None = None, scales_h: float | None = None, scales_w: float | None = None) -> Tensor: ...
def _compute_scale(in_size, out_size, align_corners, scale=None): ...
def _compute_source_index(scale, dst_index, align_corners): ...
def _sum_tensors_uint8(src: Iterable[Tensor], weights: Iterable[Tensor], weights_precision: Tensor) -> Tensor: ...
def _compute_weight_precision(weights: TensorSequenceType) -> Tensor: ...
@pw_cast_for_opmath
def _upsample_linear(input: Tensor, output_size: list[int], align_corners: bool, scales: list[float | None]) -> Tensor: ...
def is_same_size(a: Tensor, b: Tensor) -> bool: ...
def _reshape_alias(x, shape, *args): ...
def _unsafe_index(x, indices): ...
def _unsafe_index_put(x, indices, value, accumulate: bool = False): ...
def _unsafe_masked_index(x, mask, indices, fill): ...
def _unsafe_masked_index_put_accumulate(x, mask, indices, values): ...
def _nll_loss_forward(self, target: Tensor, weight: Tensor | None, reduction: int, ignore_index: int) -> tuple[Tensor, Tensor]: ...
def nll_loss_forward(self, target: Tensor, weight: Tensor | None, reduction: int, ignore_index: int) -> tuple[Tensor, Tensor]: ...
def nll_loss2d_forward(self, target: Tensor, weight: Tensor | None, reduction: int, ignore_index: int) -> tuple[Tensor, Tensor]: ...
def _upsample_cubic_convolution1(x: Tensor, A: float) -> Tensor: ...
def _upsample_cubic_convolution2(x: Tensor, A: float) -> Tensor: ...
def _upsample_get_cubic_coefficients(t: Tensor) -> TensorSequenceType: ...
def _upsample_cubic_interp1d(coeffs: TensorSequenceType, ts: Tensor) -> Tensor: ...
def _sum_tensors(ts: Iterable[Tensor]) -> Tensor: ...
def _linspace_from_neg_one(num_steps: int, align_corners: bool, dtype: torch.dtype, device: torch.device): ...
def _make_base_grid_4d(theta: Tensor, h: int, w: int, align_corners: bool): ...
def _make_base_grid_5d(theta: Tensor, d: int, h: int, w: int, align_corners: bool): ...
def _affine_grid_generator_4d(theta: Tensor, size: list[int], align_corners: bool): ...
def _affine_grid_generator_5d(theta: Tensor, size: list[int], align_corners: bool): ...
@pw_cast_for_opmath
def affine_grid_generator(theta: Tensor, size: list[int], align_corners: bool): ...
def _grid_sampler_2d(a: Tensor, grid: Tensor, interpolation_mode: int = 0, padding_mode: int = 0, align_corners: bool = False, _expand_grid: bool = True) -> Tensor: ...
@pw_cast_for_opmath
def grid_sampler_2d(a: Tensor, grid: Tensor, interpolation_mode: int = 0, padding_mode: int = 0, align_corners: bool = False) -> Tensor: ...
@pw_cast_for_opmath
def mv(self, vec): ...
def binary_cross_entropy_with_logits(self, target, weight=None, pos_weight=None, reduction=...): ...
def should_fold(tensor1: torch.Tensor, tensor2: torch.Tensor, is_out: bool) -> bool: ...
def matmul(tensor1, tensor2, *, is_out: bool = False): ...
@pw_cast_for_opmath
def upsample_bicubic2d_default(input: Tensor, output_size: tuple[int, int], align_corners: bool, scale_h: float | None = None, scale_w: float | None = None) -> Tensor: ...
@pw_cast_for_opmath
def upsample_bicubic2d_vec(a: Tensor, output_size: tuple[int, int] | None, align_corners: bool, scale_factors: tuple[float, float] | None = None) -> Tensor: ...
@pw_cast_for_opmath
def _reflection_pad(a: Tensor, padding: tuple[int, ...]) -> Tensor: ...
@pw_cast_for_opmath
def _replication_pad(a: Tensor, padding: tuple[int, ...]) -> Tensor: ...
def _reflection_or_replication_pad(a: Tensor, padding: tuple[int, ...], idx_fn: Callable[[int, int, int], Tensor]) -> Tensor: ...
def _reflection_pad_backward(grad_output, x, padding): ...
def aminmax(self, *, dim=None, keepdim: bool = False): ...
def nansum(self, dim=None, keepdim: bool = False, *, dtype=None): ...
def arange_default(end: NumberType, *, dtype: torch.dtype | None = None, layout: torch.layout = ..., device: torch.device | None = None, pin_memory: bool = False): ...
def arange_start(start: NumberType, end: NumberType, *, dtype: torch.dtype | None = None, layout: torch.layout = ..., device: torch.device | None = None, pin_memory: bool = False): ...
def out_dtype_decomp(*args, **kwargs): ...
def multi_margin_loss(input: Tensor, target: Tensor, p: NumberType = 1, margin: NumberType = 1, weight: Tensor | None = None, reduction: int = ...) -> Tensor: ...
def multilabel_margin_loss_forward(input: Tensor, target: Tensor, reduction: int) -> tuple[Tensor, Tensor]: ...
def scaled_dot_product_flash_attention_for_cpu(query: Tensor, key: Tensor, value: Tensor, dropout_p: float = 0.0, is_causal: bool = False, *, attn_mask: Tensor | None = None, scale: float | None = None) -> tuple[Tensor, Tensor]: ...
def register_inplace(aten_op, outplace_op): ...
@pw_cast_for_opmath
def baddbmm(self, batch1, batch2, beta: int = 1, alpha: int = 1): ...
def floor_divide(self, other): ...
def sym_numel(t): ...
def sum_default(self, *, dtype: torch.dtype | None = None, out: Tensor | None = None) -> Tensor: ...
def squeeze_default(self, dim: int | None = None): ...
def _weight_norm_interface(v, g, dim: int = 0): ...
def isin(elements, test_elements, *, assume_unique: bool = False, invert: bool = False): ...
def bernoulli(self, *, generator: torch.Generator | None = None) -> torch.Tensor: ...
def isin_default(elements, test_elements, *, invert: bool = False): ...
def isin_sorting(elements, test_elements, *, assume_unique: bool = False, invert: bool = False): ...
def take(self, index): ...
def resize_as(self, other, memory_format=None): ...
