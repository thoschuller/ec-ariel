import torch
import types
from _typeshed import Incomplete
from torch._decomp import get_decompositions as get_decompositions
from torch._ops import OpOverload as OpOverload
from typing import Callable

aten: Incomplete
rng_decompositions: dict[str, dict[OpOverload, Callable]]

def register_rng_decomposition(aten_op): ...
def throw_on_non_cuda(device) -> None: ...
def rand(shape, dtype=None, layout=..., device=None, pin_memory: bool = False): ...
def rand_like(x: torch.Tensor, dtype=None, layout=None, device=None, pin_memory: bool = False, memory_format=...): ...

class PhiloxState:
    """
    Represents a PhiloxRngState - (seed, offset) where offset = base_offset +
    relative_offset. seed and base_offset basically point to the rng state just
    before tracing starts. relative offset tracks the totally consumed offset at
    trace time.
    """
    def __init__(self) -> None: ...
    seed: Incomplete
    base_offset: Incomplete
    relative_offset: int
    offset_advanced_alteast_once: bool
    def reset(self) -> None: ...
    def validate_state(self) -> None: ...
    def advance_offset(self, consumed_offset) -> None: ...
    def set_state(self, seed, base_offset, relative_offset: int = 0) -> None: ...
    def get_state_as_tuple(self): ...
    def get_state_as_tensor(self): ...
    def set_state_from_tensor(self, state) -> None: ...

class PhiloxStateTracker:
    """
    Singleton class to track the philox rng state during AOT Autograd tracing.
    For each aot tracing instance, AOT Autograd resets this tracker and keeps
    track of both forward and backward offsets. At runtime, we only care about
    the total consumed forward and backward offsets. For dynamic shapes, these
    offsets are a function of input shapes. Therefore, the AOT generated graphs
    have additional outputs that compute total consumed forward and backward
    offsets.
    """
    running_state: PhiloxState
    fwd_state: PhiloxState
    bwd_state: PhiloxState
    def __enter__(self): ...
    def __exit__(self, exc_type: type[BaseException] | None, exc_cal: BaseException | None, exc_tb: types.TracebackType | None) -> None: ...
    @classmethod
    def reset(cls) -> None: ...
    @classmethod
    def mark_beginning_of_forward(cls) -> None: ...
    @classmethod
    def mark_beginning_of_backward(cls) -> None: ...
    @classmethod
    def record_state(cls, seed, offset, mode) -> None: ...
    @classmethod
    def get_state_as_tensor(cls): ...
    @classmethod
    def get_state_as_tuple(cls): ...
    @classmethod
    def set_state_from_tensor(cls, x) -> None: ...
    @classmethod
    def advance_offset(cls, consumed_offset) -> None: ...
    @classmethod
    def get_current_relative_offset(cls): ...
    @staticmethod
    def multiple_of_4(offset): ...
    @classmethod
    def get_updated_fwd_offset(cls): ...
    @classmethod
    def get_updated_bwd_offset(cls): ...

extra_random_decomps: Incomplete
register_extra_random_decomp: Incomplete

def bernoulli_(self, p: float = 0.5): ...
def bernoulli_p(self, p: float = 0.5, *, generator=None): ...
