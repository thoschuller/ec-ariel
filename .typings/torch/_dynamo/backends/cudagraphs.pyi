import torch
from .registry import register_backend as register_backend
from torch._dynamo import config as config
from torch._dynamo.backends.common import aot_autograd as aot_autograd
from torch._dynamo.backends.debugging import boxed_nop as boxed_nop
from torch._inductor.cudagraph_utils import BoxedDeviceIndex as BoxedDeviceIndex, check_multiple_devices_or_any_cpu_nodes as check_multiple_devices_or_any_cpu_nodes, format_default_skip_message as format_default_skip_message, get_mutation_stack_trace as get_mutation_stack_trace, get_placeholder_info as get_placeholder_info, log_cudagraph_skip_and_bump_counter as log_cudagraph_skip_and_bump_counter
from torch._inductor.utils import BoxedBool as BoxedBool, count_tangents as count_tangents, get_first_incompatible_cudagraph_node as get_first_incompatible_cudagraph_node, num_fw_fixed_arguments as num_fw_fixed_arguments, output_node as output_node
from torch.multiprocessing.reductions import StorageWeakRef as StorageWeakRef

def find_input_mutations(g): ...
def get_device_node_mapping(gm: torch.fx.GraphModule): ...
def check_for_mutation_ignore_cuda_graph_managed_tensor(aot_model: torch.fx.GraphModule, num_fixed) -> str | None: ...
def check_for_skip(aot_model: torch.fx.GraphModule, num_fixed) -> str | None: ...
def get_device_index(gm) -> int: ...
def get_stack_traces(gm) -> list[str | None]: ...
def cudagraphs(dynamo_model, dynamo_inputs): ...

class CudagraphsBackend:
    compiler_name: str
    @staticmethod
    def reset() -> None: ...
    @staticmethod
    def __call__(model, inputs): ...

def cudagraphs_inner(model, inputs, copy_outputs: bool = True, copy_inputs: bool = True):
    """This isn't registered as a backend, but is used in some benchmarks"""
