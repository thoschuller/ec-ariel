import dataclasses
import torch.nn
import types
from . import config as config, graph_break_hints as graph_break_hints, utils as utils
from .bytecode_transformation import Instruction as Instruction, add_push_null as add_push_null, add_push_null_call_function_ex as add_push_null_call_function_ex, create_call_function as create_call_function, create_call_method as create_call_method, create_dup_top as create_dup_top, create_instruction as create_instruction, create_load_const as create_load_const, create_load_method as create_load_method, create_rot_n as create_rot_n
from .exc import IncorrectUsage as IncorrectUsage, unimplemented_v2 as unimplemented_v2
from .source import AttrSource as AttrSource, ChainedSource as ChainedSource, DictGetItemSource as DictGetItemSource, Source as Source
from .symbolic_convert import InstructionTranslatorBase as InstructionTranslatorBase
from .utils import is_safe_constant as is_safe_constant, rot_n_helper as rot_n_helper
from .variables.base import ValueMutationExisting as ValueMutationExisting, VariableTracker as VariableTracker
from .variables.functions import ContextlibContextManagerLocalGeneratorObjectVariable as ContextlibContextManagerLocalGeneratorObjectVariable, LocalGeneratorObjectVariable as LocalGeneratorObjectVariable
from .variables.nn_module import NNModuleVariable as NNModuleVariable
from .variables.tensor import NumpyNdarrayVariable as NumpyNdarrayVariable, SymNodeVariable as SymNodeVariable, TensorVariable as TensorVariable, UnspecializedPythonVariable as UnspecializedPythonVariable
from .variables.torch_function import TensorWithTFOverrideVariable as TensorWithTFOverrideVariable
from _typeshed import Incomplete
from collections import Counter
from torch.utils._ordered_set import OrderedSet as OrderedSet

@dataclasses.dataclass
class GraphOutputEntry:
    index: int
    variable: VariableTracker

class PyCodegen:
    """
    Helper class uses for constructing Python bytecode
    """
    root: Incomplete
    top_of_stack: VariableTracker | Source | None
    uses: Counter[VariableTracker | Source]
    graph_outputs: dict[int, GraphOutputEntry]
    _output: list[Instruction]
    tempvars: Incomplete
    tx: Incomplete
    graph_output_var: Incomplete
    code_options: Incomplete
    cell_and_freevars: Incomplete
    new_var: Incomplete
    value_from_source: bool
    overridden_sources: dict[Source, Source]
    def __init__(self, tx: InstructionTranslatorBase, root: torch.nn.Module | None = None, graph_output_var: str | None = None, tempvars=None, overridden_sources=None) -> None: ...
    def restore_stack(self, stack_values, *, value_from_source: bool = True) -> None: ...
    def graph_output_vars(self): ...
    def call_reconstruct(self, value) -> None: ...
    def add_push_null(self, gen_fn, call_function_ex: bool = False) -> None:
        """
        `gen_fn` generates instructions via PyCodegen methods
        that push a single callable to the stack.

        `add_push_null` pushes a NULL to the stack before or after the
        instructions generated by `gen_fn`, depending on Python version.

        Will attempt to use the NULL push bit for instructions
        with such bits (LOAD_GLOBAL 3.11+, LOAD_ATTR 3.12+, LOAD_SUPER_ATTR).
        """
    def __call__(self, value, allow_cache: bool = True):
        """
        Generate code such that top-of-stack (TOS) is set to value.

        `allow_cache` controls the behavior in the following manner. `value` can
        either be a VariableTracker or a Source.

        If `value` is a `Source`, `allow_cache` must be True (invariant asserted
        below). If the source was reconstructed earlier, we will reuse the
        generated code by loading from top of stack or tempvars.

        If `value` is a `VariableTracker`, we have the following cases:

        1) `allow_cache=True`
            a) If the value.source is not None, we will emit the code based on
            `value.source` to handle aliasing.
            b) If value.source is None (example reconstructing a local list
            returned by the compiled function), we will reconstruct the variable
            tracker (w/o any source) to emit bytecode that generates a new
            python object.

            In both cases of value.source being None or not, if the value was
            reconstructed earlier, we will reuse the generated code by loading from
            top of stack or tempvars.

        2) `allow_cache=False` - This is a special case (allow_cache defaults to
        True).
            a) If the value.source is not None, we reconstruct the variable
            tracker and emit a new python object. You might wonder what about
            aliasing? The place where we use this config also has the followup
            code where the original python object is assigned to this new python
            value to handle aliasing (check side_effects.py and search for
            allow_cache=False).

            b) If value.source is None, this is not allowed. TODO - assert this.

        Notable effects:
        1. `self.top_of_stack` will be set to `value`, if we don't codegen
           `value` based on source.
        2. `self.uses[value]` will increment, unless (a). we codegen via
            `top_of_stack` or cached `tempvars`, or (b). `value` has special VT
            types like `NNModuleVariable`, etc.
        """
    def add_graph_output(self, value): ...
    def load_graph_output(self, index) -> None: ...
    def add_cache(self, value) -> None: ...
    def foreach(self, items) -> None: ...
    def create_binary_subscr(self) -> Instruction: ...
    def setup_globally_cached(self, name, value):
        """Store value in a new global"""
    def clear_tos(self) -> None: ...
    def append_output(self, inst) -> None: ...
    def extend_output(self, insts) -> None: ...
    def get_instructions(self) -> list[Instruction]: ...
    def create_load(self, name) -> Instruction: ...
    def create_load_closure(self, name) -> Instruction: ...
    def create_load_deref(self, name) -> Instruction: ...
    def create_store(self, name) -> Instruction: ...
    def create_store_deref(self, name) -> Instruction: ...
    def create_load_global(self, name, add: bool = False) -> Instruction: ...
    def create_load_const(self, value) -> Instruction: ...
    def create_load_const_unchecked(self, value) -> Instruction: ...
    def load_method(self, name) -> None: ...
    def call_method(self, nargs) -> None: ...
    def create_load_attr(self, name) -> Instruction: ...
    def load_attr(self, name) -> None: ...
    def create_load_attrs(self, names): ...
    def create_store_attr(self, name) -> Instruction: ...
    def store_attr(self, name) -> None: ...
    def load_function_name(self, fn_name, push_null, num_on_stack: int = 0):
        """Load the global fn_name on the stack num_on_stack down"""
    def rot_n(self, n): ...
    def pop_top(self) -> None: ...
    def call_function(self, nargs: int, push_null: bool): ...
    def dup_top(self) -> None: ...
    def store(self, varname) -> None: ...
    def load_deref(self, varname) -> None: ...
    def make_function_with_closure(self, fn_name: str, code: types.CodeType, push_null: bool, num_on_stack: int = 0): ...
    def create_load_python_module(self, mod) -> Instruction:
        """
        Generate a LOAD_GLOBAL instruction to fetch a given python module.
        """
    def mark_source_temp(self, source: Source) -> None:
        """
        Mark a source as a temp variable, so that it can be reused.
        """
    def make_call_generated_code(self, fn_name: str) -> None:
        """Call the generated code function stored in fn_name"""
    def load_import_from(self, module_name, object_name) -> None: ...
    def create_call_function_kw(self, nargs, kw_names, push_null) -> list[Instruction]: ...
    def create_delete(self, value) -> Instruction: ...
