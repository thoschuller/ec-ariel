import torch
from _typeshed import Incomplete
from typing import Any, Callable, TypeVar
from typing_extensions import ParamSpec

_P = ParamSpec('_P')
_R = TypeVar('_R')

def is_compiling() -> bool: ...
def wrap_inline(fn: Callable[_P, _R]) -> Callable[_P, _R]:
    """
    Create an extra frame around fn that is not in skipfiles.
    """
def call_hook(hook: Callable[..., torch.Tensor | None], *args: Any, **kwargs: Any) -> torch.Tensor:
    """
    Used by compiled autograd to handle hook returning None.
    """
def wrap_numpy(f: Callable[_P, _R]) -> Callable[_P, _R]:
    """Decorator that turns a function from ``np.ndarray``s to ``np.ndarray``s into a function
    from ``torch.Tensor``s to ``torch.Tensor``s.
    """

class FakeBackwardCFunction:
    real: Incomplete
    saved_tensors: Incomplete
    def __init__(self, real: torch.autograd.function.BackwardCFunction, saved_tensors: list[torch.Tensor]) -> None: ...
    def __getattr__(self, name: str) -> Any: ...

def call_backward(backward_c_function: torch.autograd.function.BackwardCFunction, saved_tensors: list[torch.Tensor], *args: Any) -> torch.Tensor | tuple[torch.Tensor, ...]: ...
def normalize_as_list(x: Any) -> list[Any]: ...
def untyped_storage_size(x: torch.Tensor) -> int: ...

class FakeCompiledAutogradEngine:
    @staticmethod
    def queue_callback(final_callbacks: list[Callable[[], None]], cb: Callable[[], None]) -> None: ...
    @staticmethod
    def exec_final_callbacks(final_callbacks: list[Callable[[], None]]) -> None: ...
    @staticmethod
    def _exec_final_callbacks_stub() -> None: ...

def call_hook_from_backward_state(*args: Any, bw_state: Any, hook_name: str, **kwargs: Any) -> Any: ...
def call_module_hooks_from_backward_state(_: Any, result: Any, *args: Any, bw_state: Any, hooks_name: str, module_name: str) -> Any: ...
def get_nonrecursive_disable_wrapper(fn: Callable[_P, _R]) -> Callable[_P, _R]: ...
def _dynamo_config_patch_proxy_dunder_call(self, func: Callable[_P, _R]) -> Callable[_P, _R]: ...
def unwrap_maybe_dynamic_int(x: torch.Tensor | int) -> int: ...
def call_accumulate_grad(variable: torch.Tensor, grad: torch.Tensor, has_post_hooks: bool) -> None: ...
