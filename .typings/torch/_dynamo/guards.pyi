import ast
import dataclasses
import enum
import functools
import pickle
import torch
import types
from . import config as config, convert_frame as convert_frame, exc as exc
from .eval_frame import set_guard_error_hook as set_guard_error_hook
from .source import AttrProxySource as AttrProxySource, AttrSource as AttrSource, CallFunctionNoArgsSource as CallFunctionNoArgsSource, CallMethodItemSource as CallMethodItemSource, ChainedSource as ChainedSource, ConstDictKeySource as ConstDictKeySource, ConstantSource as ConstantSource, DataclassFieldsSource as DataclassFieldsSource, DefaultsSource as DefaultsSource, DictGetItemSource as DictGetItemSource, DictSubclassGetItemSource as DictSubclassGetItemSource, FSDPNNModuleSource as FSDPNNModuleSource, FlattenScriptObjectSource as FlattenScriptObjectSource, FloatTensorSource as FloatTensorSource, GenericAttrSource as GenericAttrSource, GetItemSource as GetItemSource, GlobalSource as GlobalSource, GlobalStateSource as GlobalStateSource, GlobalWeakRefSource as GlobalWeakRefSource, GradSource as GradSource, ListGetItemSource as ListGetItemSource, LocalSource as LocalSource, NNModuleSource as NNModuleSource, NumpyTensorSource as NumpyTensorSource, OptimizerSource as OptimizerSource, ScriptObjectQualifiedNameSource as ScriptObjectQualifiedNameSource, ShapeEnvSource as ShapeEnvSource, SubclassAttrListSource as SubclassAttrListSource, TorchFunctionModeStackSource as TorchFunctionModeStackSource, TupleIteratorGetItemSource as TupleIteratorGetItemSource, TypeSource as TypeSource, UnspecializedBuiltinNNModuleSource as UnspecializedBuiltinNNModuleSource, UnspecializedNNModuleSource as UnspecializedNNModuleSource, UnspecializedParamBufferSource as UnspecializedParamBufferSource, WeakRefCallSource as WeakRefCallSource
from .types import CacheEntry as CacheEntry, DynamoFrameType as DynamoFrameType, ExtraState as ExtraState, GuardFail as GuardFail, GuardFilterEntry as GuardFilterEntry, GuardFn as GuardFn, GuardedCode as GuardedCode
from .utils import builtin_dict_keys as builtin_dict_keys, common_constant_types as common_constant_types, dataclass_fields as dataclass_fields, dict_keys as dict_keys, get_custom_getattr as get_custom_getattr, get_torch_function_mode_stack as get_torch_function_mode_stack, get_torch_function_mode_stack_at as get_torch_function_mode_stack_at, guard_failures as guard_failures, istype as istype, key_is_id as key_is_id, key_to_id as key_to_id, normalize_range_iter as normalize_range_iter, orig_code_map as orig_code_map, tensor_always_has_static_shape as tensor_always_has_static_shape, tuple_iterator_getitem as tuple_iterator_getitem, tuple_iterator_len as tuple_iterator_len, unpatched_nn_module_getattr as unpatched_nn_module_getattr, verify_guard_fn_signature as verify_guard_fn_signature
from _typeshed import Incomplete
from collections.abc import Generator
from contextlib import contextmanager
from sympy import Symbol as Symbol
from torch._C._dynamo.eval_frame import code_framelocals_names as code_framelocals_names
from torch._C._dynamo.guards import DictGuardManager as DictGuardManager, RootGuardManager as RootGuardManager, check_obj_id as check_obj_id, check_type_id as check_type_id, dict_version as dict_version, install_no_tensor_aliasing_guard as install_no_tensor_aliasing_guard, install_object_aliasing_guard as install_object_aliasing_guard, install_storage_overlapping_guard as install_storage_overlapping_guard, install_symbolic_shape_guard as install_symbolic_shape_guard, profile_guard_manager as profile_guard_manager
from torch._dynamo.output_graph import OutputGraphGuardsState as OutputGraphGuardsState
from torch._dynamo.source import IndexedSource as IndexedSource, TensorProperty as TensorProperty, TensorPropertySource as TensorPropertySource, get_global_source_name as get_global_source_name, get_local_source_name as get_local_source_name, is_from_flatten_script_object_source as is_from_flatten_script_object_source, is_from_local_source as is_from_local_source, is_from_optimizer_source as is_from_optimizer_source
from torch._dynamo.utils import CompileEventLogger as CompileEventLogger, get_metrics_context as get_metrics_context
from torch._guards import CompileContext as CompileContext, CompileId as CompileId, DuplicateInputs as DuplicateInputs, Guard as Guard, GuardBuilderBase as GuardBuilderBase, GuardEnvExpr as GuardEnvExpr, GuardSource as GuardSource, Source as Source, StorageOverlap as StorageOverlap
from torch._logging import structured as structured
from torch._utils_internal import justknobs_check as justknobs_check
from torch.fx.experimental.symbolic_shapes import EqualityConstraint as EqualityConstraint, SYMPY_INTERP as SYMPY_INTERP, _CppShapeGuardsHelper as _CppShapeGuardsHelper, _ShapeGuardsHelper as _ShapeGuardsHelper, is_symbolic as is_symbolic
from torch.utils._ordered_set import OrderedSet as OrderedSet
from torch.utils._traceback import format_frame as format_frame, report_compile_source_on_error as report_compile_source_on_error
from torch.utils.weak import TensorWeakRef as TensorWeakRef
from typing import Any, Callable, NoReturn
from weakref import ReferenceType

guard_manager_testing_hook_fn: Callable[[Any, Any], Any] | None
log: Incomplete
guards_log: Incomplete
recompiles_log: Incomplete
recompiles_verbose_log: Incomplete
verbose_guards_log: Incomplete

class GuardManagerWrapper:
    '''
    A helper class that contains the root guard manager. An instance of this
    class is stored in the Dynamo cache entry, so that the cache entry can
    access the RootGuardManager stored in the "root" attribute and directly call
    the check_nopybind from C++.
    '''
    root: Incomplete
    diff_guard_root: Incomplete
    closure_vars: Incomplete
    args: Incomplete
    code_parts: Incomplete
    verbose_code_parts: Incomplete
    global_scope: Incomplete
    guard_fail_fn: Incomplete
    cache_entry: Incomplete
    extra_state: Incomplete
    id_matched_objs: Incomplete
    no_tensor_aliasing_sources: Incomplete
    printed_relational_guards: Incomplete
    diff_guard_sources: OrderedSet[str]
    def __init__(self, root=None) -> None: ...
    @contextmanager
    def _preserve_printed_relational_guards(self) -> Generator[None]: ...
    def collect_diff_guard_sources(self): ...
    def finalize(self) -> None: ...
    def populate_diff_guard_manager(self) -> None: ...
    def clone_with_chosen_sources(self, chosen_sources): ...
    def get_guard_lines(self, guard): ...
    def get_manager_line(self, guard_manager, accessor_str=None): ...
    def construct_dict_manager_string(self, mgr, body) -> None: ...
    def construct_manager_string(self, mgr, body) -> None: ...
    def __str__(self) -> str: ...
    def check(self, x): ...
    def check_verbose(self, x): ...
    def populate_code_parts_for_debugging(self): ...

def from_numpy(a): ...
@functools.cache
def uninteresting_files(): ...

_CLOSURE_VARS: dict[str, object] | None

def _get_closure_vars(): ...
def _ast_unparse(node: ast.AST) -> str: ...
strip_function_call = torch._C._dynamo.strip_function_call

def get_verbose_code_part(code_part: str, guard: Guard) -> str: ...
def get_verbose_code_parts(code_parts: str | list[str], guard: Guard) -> list[str]: ...
def convert_int_to_concrete_values(dim) -> int | None: ...
def convert_to_concrete_values(size_or_stride): ...
def get_tensor_guard_code_part(value, name, sizes, strides, pytype, dispatch_keys): ...
def get_key_index(dct, key): ...
def get_key_index_source(source, index): ...
def raise_local_type_error(obj: Any) -> NoReturn: ...

@dataclasses.dataclass(frozen=True)
class NNModuleAttrAccessorInfo:
    present_in_generic_dict: bool = ...
    l1_key: str | None = ...
    l2_key: str | None = ...

def getitem_on_dict_manager(source, base_guard_manager, base_example_value, example_value, guard_manager_enum): ...
def match_on_id_for_tensor(guard): ...

@dataclasses.dataclass
class GuardCodeList:
    code_list: list[str]
    guard: Guard

class GuardManagerType(enum.Enum):
    GUARD_MANAGER = 1
    DICT_GUARD_MANAGER = 2

@functools.cache
def code_framelocals_names_reversed_cached(code: types.CodeType): ...

class GuardBuilder(GuardBuilderBase):
    f_code: Incomplete
    id_ref: Incomplete
    source_ref: Incomplete
    lookup_weakrefs: Incomplete
    scope: dict[str, dict[str, object]]
    guard_manager: Incomplete
    argnames: list[str]
    code: list[GuardCodeList]
    shape_env_code: list[GuardCodeList]
    no_tensor_aliasing_names: list[str]
    no_tensor_aliasing_guard_managers: list[GuardManagerWrapper]
    check_fn_manager: CheckFunctionManager
    key_order_guarded_dict_ids: Incomplete
    id_matched_objs: dict[str, ReferenceType[object]]
    _cached_guard_managers: dict[str, torch._C._dynamo.guards.GuardManager]
    _cached_duplicate_input_guards: set[tuple[str, str]]
    serialization_mode: Incomplete
    def __init__(self, f_code: types.CodeType, id_ref: Callable[[Any, str], str], source_ref: Callable[[Source], str], lookup_weakrefs: Callable[[object], ReferenceType[object]], local_scope: dict[str, object], global_scope: dict[str, object], guard_manager: GuardManagerWrapper, check_fn_manager: CheckFunctionManager, serialization_mode: str | None = None) -> None: ...
    def guard_on_dict_keys_and_ignore_order(self, example_value, guard) -> None: ...
    def guard_on_dict_keys_and_order(self, value, guard) -> None: ...
    @staticmethod
    def _get_generic_dict_manager_example_value(example_value): ...
    def getattr_on_nn_module(self, source, base_guard_manager, base_example_value, example_value, base_source_name, source_name, guard_manager_enum):
        '''
        This tries to avoid calling the expensive nn module custom getattr method by
        checking if the attribute is accessible via __dict__. For attributes that
        are not accessible via __dict__ (like descriptors), we fallback to
        PyObject_GetAttr.

        There are two cases that we optimize for
        1) attributes present directly in __dict__, e.g training.
        2) parameters/buffers/modules - they can be accessed via _parameters,
        _buffers, _modules keys in __dict__. For example, mod.linear can be
        accessed as mod.__dict__["_parameters"]["linear"]

        The most common and expensive case for nn module guards is of type
        mod.submod1.submod2.submod3.training. We avoid the python getattr of nn
        modules by going through the __dict__.
        '''
    def requires_key_order_guarding(self, source): ...
    def get_guard_manager_type(self, source, example_value): ...
    def manager_guards_on_keys(self, mgr_enum): ...
    def get_global_guard_manager(self): ...
    def get_guard_manager_from_source(self, source): ...
    def get_guard_manager(self, guard: Guard): ...
    def add_python_lambda_leaf_guard_to_root(self, code_parts, verbose_code_parts, closure_vars=None, is_epilogue: bool = True) -> None: ...
    def get(self, name: str, closure_vars: dict[str, Any] | None = None) -> Any: ...
    def arg_ref(self, guard: str | Guard) -> str: ...
    def _guard_on_attribute(self, guard: Guard, attr_name: str, guard_fn): ...
    def HASATTR(self, guard: Guard): ...
    def NOT_PRESENT_IN_GENERIC_DICT(self, guard: Guard, attr=None) -> None: ...
    def TYPE_MATCH(self, guard: Guard) -> None: ...
    def DICT_VERSION(self, guard: Guard): ...
    def DICT_CONTAINS(self, guard: Guard, key: str, invert: bool): ...
    def BOOL_MATCH(self, guard: Guard): ...
    def NONE_MATCH(self, guard: Guard): ...
    def ID_MATCH(self, guard: Guard): ...
    def NOT_NONE_MATCH(self, guard: Guard, value=None): ...
    def DISPATCH_KEY_SET_MATCH(self, guard: Guard): ...
    def NAME_MATCH(self, guard: Guard): ...
    def DUAL_LEVEL(self, guard: Guard): ...
    def FUNCTORCH_STACK_MATCH(self, guard: Guard): ...
    def AUTOGRAD_SAVED_TENSORS_HOOKS(self, guard: Guard): ...
    def TENSOR_SUBCLASS_METADATA_MATCH(self, guard: Guard): ...
    def EQUALS_MATCH(self, guard: Guard, recompile_hint: str | None = None): ...
    def CONSTANT_MATCH(self, guard: Guard): ...
    def NN_MODULE(self, guard: Guard): ...
    def FUNCTION_MATCH(self, guard: Guard):
        """things like torch.add and user defined functions"""
    def CLOSURE_MATCH(self, guard: Guard):
        """matches a closure by __code__ id."""
    def BUILTIN_MATCH(self, guard: Guard): ...
    def SEQUENCE_LENGTH(self, guard) -> None: ...
    def TUPLE_ITERATOR_LEN(self, guard) -> None: ...
    def RANGE_ITERATOR_MATCH(self, guard) -> None: ...
    def DUPLICATE_INPUT(self, guard, source_b) -> None: ...
    def WEAKREF_ALIVE(self, guard) -> None: ...
    def MAPPING_KEYS_CHECK(self, guard) -> None:
        """Guard on the key order of types.MappingProxyType object"""
    def DICT_KEYS_MATCH(self, guard) -> None:
        """Insert guard to check that the keys of a dict are same"""
    def EMPTY_NN_MODULE_HOOKS_DICT(self, guard) -> None:
        """Special guard to skip guards on empty hooks. This is controlled by skip_nnmodule_hook_guards"""
    def GRAD_MODE(self, guard: Guard): ...
    def DETERMINISTIC_ALGORITHMS(self, guard: Guard): ...
    def TORCH_FUNCTION_STATE(self, guard: Guard): ...
    def FSDP_TRAINING_STATE(self, guard: Guard): ...
    def DEFAULT_DEVICE(self, guard: Guard):
        """Guard on CURRENT_DEVICE per torch.utils._device"""
    def SHAPE_ENV(self, guard: Guard): ...
    def TENSOR_MATCH(self, guard: Guard, value=None): ...
    def _set_guard_export_info(self, guard, code_list, provided_guarded_object=None) -> None: ...

class PyExprCSEPass:
    USE_THRESHOLD: int
    ALLOWED_NODE_TYPES: Incomplete
    @dataclasses.dataclass
    class Config:
        expr_count: dict[str, int]
        expr_to_name: dict[str, str]
    class ExprCounter(ast.NodeVisitor):
        _config: Incomplete
        def __init__(self, config: PyExprCSEPass.Config) -> None: ...
        def visit(self, node: ast.AST) -> Any: ...
    class Replacer(ast.NodeTransformer):
        _config: Incomplete
        _gen_name: Incomplete
        preface: list[str]
        def __init__(self, config: PyExprCSEPass.Config, gen_name: Callable[[], str]) -> None: ...
        def visit(self, node: ast.AST) -> Any: ...
    _counter: int
    _config: Incomplete
    def __init__(self) -> None: ...
    def _new_var(self, prefix: str = '_var') -> str: ...
    def count(self, exprs: list[str]) -> None: ...
    def replace(self, expr: str) -> tuple[list[str], str]: ...

def must_add_nn_module_guards(guard): ...

class DeletedGuardManagerWrapper(GuardManagerWrapper):
    invalidation_reason: Incomplete
    def __init__(self, reason) -> None: ...
    diff_guard_root: Incomplete
    def populate_diff_guard_manager(self) -> None: ...

@dataclasses.dataclass
class ShapeCodeParts:
    python_code_parts: _ShapeGuardsHelper
    verbose_code_parts: _ShapeGuardsHelper
    cpp_code_parts: _CppShapeGuardsHelper | None
    python_fallback: bool
    shape_env_sources: list[Source]

@dataclasses.dataclass
class GuardsState:
    output_graph: OutputGraphGuardsState
    shape_code_parts: ShapeCodeParts | None

class GuardsStatePickler(pickle.Pickler):
    fake_mode: Incomplete
    tensor_converter: Incomplete
    def __init__(self, *args, **kwargs) -> None: ...
    @classmethod
    def _unpickle_module(cls, state): ...
    @classmethod
    def _unpickle_tensor(cls, meta_tensor, device, pytype, dispatch_keys_raw): ...
    @classmethod
    def _unpickle_traceable_wrapper_subclass(cls, meta_tensor, device, pytype, dispatch_keys_raw, ctx, inner_data): ...
    @classmethod
    def _unpickle_python_module(cls, alias: str): ...
    @classmethod
    def _unpickle_dispatch_key_set(cls, raw_repr: int): ...
    @classmethod
    def _unpickle_functorch_interpreter(cls, json: bytes): ...
    @classmethod
    def _unpickle_mapping_proxy(cls, d): ...
    def reducer_override(self, obj): ...

def pickle_guards_state(state: GuardsState) -> bytes: ...

class CheckFunctionManager:
    _weakrefs: dict[int, ReferenceType[object]]
    output_graph: Incomplete
    shape_code_parts: Incomplete
    torch_function_mode_stack: Incomplete
    guards_serialization_mode: Incomplete
    guard_manager: Incomplete
    guards_state: bytes | None
    def __init__(self, f_code, output_graph=None, cache_entry=None, guard_fail_fn: Callable[[GuardFail], None] | None = None, guard_filter_fn: Callable[[list[GuardFilterEntry]], list[bool]] | None = None, guards_serialization_mode: str | None = None, shape_code_parts: ShapeCodeParts | None = None) -> None: ...
    def build_guards(self, sorted_guards, existing_diff_guard_sources, f_code, output_graph, serialization_mode=None): ...
    def compile_check_fn(self, builder, guards_out, guard_fail_fn): ...
    def invalidate(self, obj_str) -> None: ...
    def id_ref(self, obj, obj_str):
        """add a weakref, return the id"""
    def lookup_weakrefs(self, obj):
        """Lookup the _weakrefs created in id_ref function for ID_MATCH'd objects"""

def build_guard_function(code_parts, closure_args) -> tuple[str, str]: ...
def is_recompiles_enabled(): ...
def is_recompiles_verbose_enabled(): ...
def make_torch_function_mode_stack_guard(initial_stack): ...
def recompilation_reason_for_no_tensor_aliasing_guard(guard_manager, scope): ...
def strip_local_scope(s: str) -> str:
    """
    Replace occurrences of L[...] with just the inner content.
    Handles both single and double quotes.

    This is to generate user friendly recompilation messages.
    """
def get_guard_fail_reason_helper(guard_manager: GuardFn, f_locals: dict[str, object], compile_id: CompileId) -> str:
    """
    Return the reason why `guard_manager` failed.
    Updates `guard_failures` with the generated reason.
    Only the first failed check of guard_manager is reported.
    """
def get_guard_fail_reason(guard_manager: GuardFn, code: types.CodeType, f_locals: dict[str, object], compile_id: CompileId) -> str: ...
def get_and_maybe_log_recompilation_reasons(cache_entry, frame: DynamoFrameType) -> list[str]:
    """
    Return the list of guard failure reasons using cache_entry.
    Logs the recompilation reason if `recompiles` logging is enabled.
    Raises a RecompileError if `config.error_on_recompile` is enabled.
    """
def update_diff_guard_managers_for_existing_cache_entries(cache_entry): ...
def guard_error_hook(guard_manager: GuardFn, code: types.CodeType, f_locals: dict[str, object], index: int, last: bool): ...
def unique(seq) -> Generator[Incomplete]: ...
def make_dupe_guard(obj_source, dupe_source): ...
def install_guard(*guards, skip: int = 0) -> None:
    """
    Add dynamo guards to the current tracing context.

    Args:
        guards: guard(s) to add
        skip: number of stack frames to ignore for debug stack trace
    """
