import functools
import torch
import torch.fx
from .. import config as config, graph_break_hints as graph_break_hints, variables as variables
from .._trace_wrapped_higher_order_op import trace_wrapped as trace_wrapped
from ..exc import UnknownPropertiesDuringBackwardTrace as UnknownPropertiesDuringBackwardTrace, UserError as UserError, UserErrorType as UserErrorType, unimplemented_v2 as unimplemented_v2
from ..external_utils import call_hook_from_backward_state as call_hook_from_backward_state
from ..guards import GuardBuilder as GuardBuilder, install_guard as install_guard
from ..source import AttrSource as AttrSource
from ..utils import fqn as fqn, get_custom_getattr as get_custom_getattr, get_fake_value as get_fake_value, get_real_value as get_real_value, guard_if_dyn as guard_if_dyn, object_has_getattribute as object_has_getattribute, product as product, proxy_args_kwargs as proxy_args_kwargs, set_example_value as set_example_value, tensortype_to_dtype as tensortype_to_dtype
from .base import AttributeMutationNew as AttributeMutationNew, VariableTracker as VariableTracker
from .constant import ConstantVariable as ConstantVariable
from .lists import SizeVariable as SizeVariable
from .user_defined import UserDefinedClassVariable as UserDefinedClassVariable
from _typeshed import Incomplete
from torch._dynamo import compiled_autograd as compiled_autograd
from torch._dynamo.codegen import PyCodegen as PyCodegen
from torch._dynamo.symbolic_convert import InstructionTranslator as InstructionTranslator
from torch._subclasses.meta_utils import is_sparse_any as is_sparse_any
from torch.fx.experimental.symbolic_shapes import GuardOnDataDependentSymNode as GuardOnDataDependentSymNode, SymTypes as SymTypes, guard_scalar as guard_scalar, has_free_symbols as has_free_symbols, is_symbolic as is_symbolic
from torch.utils._python_dispatch import is_traceable_wrapper_subclass as is_traceable_wrapper_subclass

log: Incomplete
supported_tensor_comparison_ops: Incomplete
supported_const_comparison_ops: Incomplete
supported_comparison_ops: Incomplete
supported_tensor_comparison_op_values: Incomplete
supported_const_comparison_op_values: Incomplete

def is_bound_tensor_method(value): ...

all_tensor_attrs: Incomplete

class TensorVariable(VariableTracker):
    """A torch.Tensor input or an intermediate value in the FX graph"""
    _nonvar_fields: Incomplete
    def get_real_value(self):
        """
        Get the actual value represented by this variable if computation is run
        using the user-provided inputs.
        NOTE: this runs actual tensor computation and may be
        slow and memory-intensive.
        """
    proxy: Incomplete
    dtype: Incomplete
    device: Incomplete
    layout: Incomplete
    ndim: Incomplete
    _size: Incomplete
    stride: Incomplete
    requires_grad: Incomplete
    is_quantized: Incomplete
    is_contiguous: Incomplete
    is_nested: Incomplete
    is_sparse: Incomplete
    class_type: Incomplete
    has_grad_fn: Incomplete
    _is_name_set: bool
    def __init__(self, proxy: torch.fx.Proxy, *, dtype, device, layout, ndim, requires_grad, is_nested, is_quantized, is_sparse, class_type, has_grad_fn, _size=None, stride=None, is_contiguous=None, _is_name_set=None, **kwargs) -> None: ...
    def debug_repr(self): ...
    def as_proxy(self): ...
    def python_type(self): ...
    @staticmethod
    def specialize(value: torch.Tensor): ...
    def dynamic_getattr(self, tx: InstructionTranslator, name): ...
    def method_attr_ndim(self, tx): ...
    def method_attr_dtype(self, tx): ...
    def method_attr_device(self, tx): ...
    def method_attr_layout(self, tx): ...
    def method_attr_is_cuda(self, tx): ...
    def method_attr_shape(self, tx): ...
    def method_attr_requires_grad(self, tx): ...
    def method_attr_is_quantized(self, tx): ...
    def method_attr_is_sparse(self, tx): ...
    def method_attr_is_nested(self, tx): ...
    def method_attr_retain_grad(self, tx) -> None: ...
    def method_attr_data(self, tx): ...
    def method_attr_grad_fn(self, tx): ...
    def method_attr__version(self, tx): ...
    def call_obj_hasattr(self, tx: InstructionTranslator, name): ...
    def var_getattr(self, tx: InstructionTranslator, name): ...
    def call_id(self, tx): ...
    def has_unpack_var_sequence(self, tx): ...
    def unpack_var_sequence(self, tx: InstructionTranslator, idxes=None): ...
    def valid_size(self): ...
    @property
    def size(self): ...
    def _strict_mode_banned_ops(self): ...
    def _strict_mode_conditional_banned_ops(self): ...
    def call_method(self, tx, name, args: list[VariableTracker], kwargs: dict[str, VariableTracker]) -> VariableTracker: ...
    def method_size(self, *args, **kwargs): ...
    def method_stride(self, *args, **kwargs): ...
    def _method_size_stride(self, name, dim=None): ...
    def method_numel(self): ...
    method_nelement = method_numel
    def method_dim(self): ...
    method_ndimension = method_dim
    def method_is_floating_point(self): ...
    def method_is_inference(self): ...
    def method_is_complex(self): ...
    def method_is_contiguous(self, memory_format=None): ...
    def method_type(self, dtype=None, non_blocking: bool = False, **kwargs): ...
    def method_as_subclass(self, cls): ...
    def method_get_device(self): ...
    def method_element_size(self): ...
    def method_numpy(self, *, force: bool = False): ...
    def method_tolist(self): ...
    def method_backward(self, *args, **kwargs) -> None: ...
    def method_data_ptr(self, *args, **kwargs): ...
    def method_item(self, *args, **kwargs) -> None: ...
    def method___getitem__(self, *args, **kwargs): ...
    @staticmethod
    @functools.cache
    def _warn_capture_scalar_outputs() -> None: ...
    def method___len__(self): ...
    def method_addcmul_(self, tensor1, tensor2, *, value=None): ...
    def method___setitem__(self, key, value): ...
    def method_resize_(self, *args, **kwargs) -> None: ...
    def method_resize_as_(self, *args, **kwargs) -> None: ...
    def method_sparse_resize_(self, *args, **kwargs) -> None: ...
    def method_sparse_resize_and_clear_(self, *args, **kwargs) -> None: ...
    def method_set_(self, *args, **kwargs) -> None: ...
    def method_add_(self, other, *, alpha=None): ...
    def method_addcdiv_(self, tensor1, tensor2, *, value=None): ...
    def method___contains__(self, arg): ...
    def method_redistribute(self, *args, **kwargs): ...
    def method_to_local(self, *args, **kwargs): ...
    def method_register_hook(self, *args, **kwargs): ...
    def method_register_post_accumulate_grad_hook(self, *args, **kwargs): ...
    def _method_register_hook(self, name: str, hook: VariableTracker): ...
    def method_requires_grad_(self, requires_grad: bool = True): ...
    def method_new(self, *args, **kwargs): ...
    def method_untyped_storage(self): ...
    def set_name_hint(self, name: str): ...

class SymNodeVariable(VariableTracker):
    """
    Represents a symbolic scalar, either int, float or bool.  This is most commonly used to
    handle symbolic size computation, e.g., tensor.size(0), but it is also used to
    handle logic like float_tensor.item() or unspecialized float inputs.
    """
    _nonvar_fields: Incomplete
    def debug_repr(self): ...
    @classmethod
    def create(cls, tx, proxy, sym_num=None, **options): ...
    proxy: Incomplete
    sym_num: Incomplete
    _tensor_var: Incomplete
    def __init__(self, proxy, sym_num, **kwargs) -> None: ...
    def python_type(self): ...
    def as_proxy(self): ...
    def as_tensor(self, tx, dtype): ...
    def evaluate_expr(self, output_graph=None): ...
    def call_method(self, tx, name, args: list[VariableTracker], kwargs: dict[str, VariableTracker]) -> VariableTracker: ...

class NumpyNdarrayVariable(TensorVariable):
    """
    Represents a np.ndarray, but backed by torch Tensor via torch._numpy.ndarray.
    Use this for Tensor.numpy() call.
    """
    @staticmethod
    def create(tx: InstructionTranslator, proxy, **options): ...
    def var_getattr(self, tx: InstructionTranslator, name): ...
    @staticmethod
    def patch_args(name, args, kwargs): ...
    def call_method(self, tx, name, args: list[VariableTracker], kwargs: dict[str, VariableTracker]) -> VariableTracker: ...
    def python_type(self): ...

class UnspecializedPythonVariable(TensorVariable):
    """
    This is a 1-element tensor represents unspecialized python float/int.
    """
    _nonvar_fields: Incomplete
    raw_value: Incomplete
    need_unwrap: Incomplete
    def __init__(self, proxy: torch.fx.Proxy, *, raw_value=None, need_unwrap: bool = True, **kwargs) -> None: ...
    @classmethod
    def from_tensor_variable(cls, tensor_variable, raw_value, need_unwrap: bool = True): ...

class FakeItemVariable(TensorVariable):
    """An unspecialized python variable which prevents access to the underlying raw value.
    This is needed if item is called on a FakeTensor."""
    _nonvar_fields: Incomplete
    need_unwrap: Incomplete
    def __init__(self, proxy: torch.fx.Proxy, **kwargs) -> None: ...
    @classmethod
    def from_tensor_variable(cls, tensor_variable): ...

class TensorSubclassVariable(UserDefinedClassVariable):
    def call_function(self, tx: InstructionTranslator, args: list[VariableTracker], kwargs: dict[str, VariableTracker]) -> VariableTracker: ...
    def as_python_constant(self): ...

class UntypedStorageVariable(VariableTracker):
    _nonvar_fields: Incomplete
    from_tensor: Incomplete
    example_value: Incomplete
    def __init__(self, from_tensor: TensorVariable, example_value: torch.UntypedStorage, **kwargs) -> None: ...
    def call_method(self, tx, name, args: list[VariableTracker], kwargs: dict[str, VariableTracker]) -> VariableTracker: ...
    def reconstruct(self, codegen: PyCodegen): ...

class DataPtrVariable(VariableTracker):
    from_tensor: Incomplete
    def __init__(self, from_tensor: TensorVariable, **kwargs) -> None: ...
    def reconstruct(self, codegen: PyCodegen): ...
