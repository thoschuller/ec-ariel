import torch
from .. import graph_break_hints as graph_break_hints, variables as variables
from ..bytecode_transformation import create_call_function as create_call_function, create_instruction as create_instruction, create_setup_with as create_setup_with
from ..device_interface import get_interface_for_device as get_interface_for_device
from ..exc import unimplemented_v2 as unimplemented_v2
from ..guards import GuardBuilder as GuardBuilder, install_guard as install_guard
from ..source import AttrSource as AttrSource, GlobalStateSource as GlobalStateSource
from .base import VariableTracker as VariableTracker
from .functions import NestedUserFunctionVariable as NestedUserFunctionVariable, SkipFunctionVariable as SkipFunctionVariable, UserFunctionVariable as UserFunctionVariable, UserMethodVariable as UserMethodVariable, WrappedNestedUserFunctionVariable as WrappedNestedUserFunctionVariable, WrappedSkipFunctionVariable as WrappedSkipFunctionVariable, WrappedUserFunctionVariable as WrappedUserFunctionVariable, WrappedUserMethodVariable as WrappedUserMethodVariable
from .user_defined import UserDefinedObjectVariable as UserDefinedObjectVariable
from _typeshed import Incomplete
from torch._dynamo.codegen import PyCodegen as PyCodegen
from torch._dynamo.symbolic_convert import InstructionTranslator as InstructionTranslator
from torch._guards import Guard as Guard

class ContextWrappingVariable(VariableTracker):
    _nonvar_fields: Incomplete
    target_values: Incomplete
    initial_values: Incomplete
    def __init__(self, target_values, initial_values=None, **kwargs) -> None: ...
    def enter(self, tx): ...
    cleanup_fn: Incomplete
    def set_cleanup_hook(self, tx: InstructionTranslator, fn=None): ...
    def exit(self, tx: InstructionTranslator, *args): ...
    def reconstruct_type(self, codegen: PyCodegen): ...
    def reconstruct(self, codegen: PyCodegen): ...
    def module_name(self) -> None: ...
    def fn_name(self) -> None: ...
    def call_function(self, tx: InstructionTranslator, args: list[VariableTracker], kwargs: dict[str, VariableTracker]) -> VariableTracker: ...
    def supports_graph_breaks(self): ...
    def exit_on_graph_break(self): ...
    def cleanup(self) -> None: ...
    def cleanup_assert(self) -> None: ...

class GenericContextWrappingVariable(UserDefinedObjectVariable):
    cm_obj: Incomplete
    def __init__(self, cm_obj, **kwargs) -> None: ...
    def module_name(self): ...
    def fn_name(self): ...
    def enter(self, tx): ...
    def exit(self, tx: InstructionTranslator, *args): ...
    def supports_graph_breaks(self): ...
    def exit_on_graph_break(self): ...

class GradInplaceRequiresGradCtxManagerVariable(ContextWrappingVariable):
    """represents torch grad requires grad"""
    @staticmethod
    def create(tx: InstructionTranslator, target_values, **kwargs): ...
    prev_state: Incomplete
    proxy: Incomplete
    def enter(self, tx): ...
    def exit(self, tx: InstructionTranslator, *args): ...

class TemporarilyPopInterpreterStackCtxManagerVariable(ContextWrappingVariable):
    """represents torch._functorch.pyfunction.temporarily_pop_interpreter_stack()"""
    @staticmethod
    def create(tx: InstructionTranslator, target_values, **kwargs): ...
    saved: Incomplete
    proxy: Incomplete
    def enter(self, tx): ...
    def exit(self, tx: InstructionTranslator, *args): ...

class JvpIncrementNestingCtxManagerVariable(ContextWrappingVariable):
    """represents torch.func.jvp increment/decrement nesting"""
    _guards_singleton: Incomplete
    @staticmethod
    def create(tx: InstructionTranslator, **kwargs): ...
    proxy: Incomplete
    def enter(self, tx): ...
    def exit(self, tx: InstructionTranslator, *args): ...

class SetFwdGradEnabledContextManager(ContextWrappingVariable):
    """represents torch.autograd.forward_ad._set_fwd_grad_enabled() to enable/disable fwd grad"""
    @staticmethod
    def create(tx: InstructionTranslator, target_values, **kwargs): ...
    prev_state: Incomplete
    proxy: Incomplete
    def enter(self, tx): ...
    def exit(self, tx: InstructionTranslator, *args): ...

class DualLevelContextManager(ContextWrappingVariable):
    """Represents torch.autograd.forward_ad.dual_level ctx manager"""
    _guards_singleton: Incomplete
    @staticmethod
    def create(tx: InstructionTranslator, **kwargs): ...
    new_level: Incomplete
    proxy: Incomplete
    def enter(self, tx): ...
    def exit(self, tx: InstructionTranslator, *args): ...

class GradIncrementNestingCtxManagerVariable(ContextWrappingVariable):
    """represents torch.func.grad increment/decrement nesting"""
    _guards_singleton: Incomplete
    @staticmethod
    def create(tx: InstructionTranslator, **kwargs): ...
    proxy: Incomplete
    def enter(self, tx): ...
    def exit(self, tx: InstructionTranslator, *args): ...

class CatchWarningsCtxManagerVariable(ContextWrappingVariable):
    """Delay a call to warnings.catch_warnings"""
    @staticmethod
    def create(tx: InstructionTranslator, catch_warnings_args): ...
    catch_warnings_args: Incomplete
    def __init__(self, catch_warnings_args, **kwargs) -> None: ...
    def enter(self, tx): ...
    def reconstruct(self, cg): ...

class VmapIncrementNestingCtxManagerVariable(ContextWrappingVariable):
    """represents torch VMap increment/decrement nesting"""
    _guards_singleton: Incomplete
    @staticmethod
    def create(tx: InstructionTranslator, target_values, **kwargs): ...
    proxy: Incomplete
    def enter(self, tx): ...
    def exit(self, tx: InstructionTranslator, *args): ...

class GradModeVariable(ContextWrappingVariable):
    """represents torch.{no_grad,enable_grad,set_grad_mode}()"""
    _guards_singleton: Incomplete
    @staticmethod
    def create(tx: InstructionTranslator, target_value, initialized: bool = False, **kwargs): ...
    def __init__(self, target_values, initial_values=None, initialized: bool = True, **kwargs) -> None: ...
    def enter(self, tx): ...
    def exit(self, tx: InstructionTranslator, *args): ...
    def call_function(self, tx: InstructionTranslator, args: list[VariableTracker], kwargs: dict[str, VariableTracker]): ...
    def _call_func(self, tx: InstructionTranslator, values): ...
    def module_name(self): ...
    def fn_name(self): ...

class InferenceModeVariable(ContextWrappingVariable):
    @staticmethod
    def create(tx: InstructionTranslator, target_value, **kwargs): ...
    target_values: Incomplete
    def __init__(self, target_values, initial_values=None, **kwargs) -> None: ...
    def exit(self, tx: InstructionTranslator, *args): ...
    proxy: Incomplete
    def enter(self, tx) -> None: ...
    def module_name(self): ...
    def fn_name(self): ...

class CUDADeviceVariable(ContextWrappingVariable):
    """represents torch.cuda.device"""
    @staticmethod
    def create(tx: InstructionTranslator, device, **kwargs): ...
    target_values: Incomplete
    def __init__(self, target_values, initial_values=None, **kwargs) -> None: ...
    def exit(self, tx: InstructionTranslator, *args): ...
    proxy: Incomplete
    def enter(self, tx): ...
    def module_name(self): ...
    def fn_name(self): ...

class TorchFunctionDisableVariable(ContextWrappingVariable):
    """represents whether torch function overrides are enabled or not"""
    _guards_singleton: Incomplete
    @staticmethod
    def create(tx: InstructionTranslator, **kwargs): ...
    only_subclass: Incomplete
    initial_torch_function_subclass_enabled: Incomplete
    initial_torch_function_mode_enabled: Incomplete
    def __init__(self, target_values, initial_values=None, only_subclass: bool = True, **kwargs) -> None: ...
    cleanup_fn: Incomplete
    def set_cleanup_hook(self, tx: InstructionTranslator, fn=None): ...
    def _call_func(self, tx: InstructionTranslator, values): ...
    def module_name(self): ...
    def fn_name(self): ...

class DeterministicAlgorithmsVariable(ContextWrappingVariable):
    """represents torch.{are_deterministic_algorithms_enabled,use_deterministic_algorithms}()"""
    _guards_singleton: Incomplete
    @staticmethod
    def create(tx: InstructionTranslator, target_value, **kwargs): ...
    def __init__(self, target_values, initial_values=None, **kwargs) -> None: ...
    def enter(self, tx): ...
    def _call_func(self, tx: InstructionTranslator, values): ...
    def module_name(self): ...
    def fn_name(self): ...

class DisabledSavedTensorsHooksVariable(ContextWrappingVariable):
    """represents torch.autograd.graph.disable_saved_tensors_hook."""
    @staticmethod
    def create(tx: InstructionTranslator, target_value, **kwargs): ...
    def __init__(self, target_values, initial_values=None, **kwargs) -> None: ...
    def enter(self, tx): ...
    def _call_func(self, tx: InstructionTranslator, values): ...
    def module_name(self): ...
    def fn_name(self): ...

class AutocastModeVariable(ContextWrappingVariable):
    @staticmethod
    def create(func, args, kwargs): ...
    target_values: Incomplete
    def __init__(self, target_values, initial_values=None, **kwargs) -> None: ...
    def exit(self, tx: InstructionTranslator, *args): ...
    proxy: Incomplete
    def enter(self, tx): ...
    def module_name(self): ...
    def fn_name(self): ...

class NullContextVariable(ContextWrappingVariable):
    """
    This class represents Python contextlib.nullcontext.
    """
    def __init__(self, target_values=None, **kwargs) -> None: ...
    def enter(self, tx): ...
    def exit(self, tx: InstructionTranslator, *args): ...
    def module_name(self): ...
    def fn_name(self): ...

class ProfilerContextVariable(ContextWrappingVariable):
    """
    This class represents a set of torch profiler context objects, where Dynamo
    ignores all the side-effects in the __init__, __enter__ and __exit__ methods
    by treating the object mostly as a `contextlib.nullcontext`, except for edge
    cases like the `__enter__` method which returns the object itself rather
    than `None`, per implementation of the torch objects.
    """
    def __init__(self, **kwargs) -> None: ...
    def enter(self, tx): ...
    def exit(self, tx: InstructionTranslator, *args): ...
    def module_name(self): ...
    def fn_name(self): ...
    def reconstruct(self, cg) -> None: ...

class StreamContextVariable(ContextWrappingVariable):
    @staticmethod
    def create(tx: InstructionTranslator, target_value, **kwargs): ...
    device: Incomplete
    set_stream: Incomplete
    set_stream_id: Incomplete
    def __init__(self, target_values, device, initial_values=None, **kwargs) -> None: ...
    def enter(self, tx): ...
    def exit(self, tx: InstructionTranslator, *args): ...

class PreserveVersionContextVariable(ContextWrappingVariable):
    """
    Wraps torch.autograd._unsafe_preserve_version_counter
    """
    @staticmethod
    def _create_lambda_from_tensors(tx, tensors): ...
    @staticmethod
    def constructor(tx): ...
    tensors: Incomplete
    prev_versions: Incomplete
    def __init__(self, tensors, prev_versions, **kwargs) -> None: ...
    def enter(self, tx) -> None: ...
    def exit(self, tx: InstructionTranslator, *args): ...
    def reconstruct(self, codegen: PyCodegen): ...

class FSDPParamGroupUseTrainingStateVariable(ContextWrappingVariable):
    _guards_singleton: Incomplete
    @staticmethod
    def create(tx: InstructionTranslator, param_group_var, target_value, **kwargs): ...
    param_group_var: Incomplete
    def __init__(self, param_group_var, target_values, initial_values=None, **kwargs) -> None: ...
    def enter(self, tx): ...
    def exit(self, tx: InstructionTranslator, *args): ...
    def call_function(self, tx: InstructionTranslator, args: list[VariableTracker], kwargs: dict[str, VariableTracker]): ...
    def _call_func(self, tx: InstructionTranslator, values): ...
    def module_name(self): ...
    def fn_name(self): ...

class SDPAKernelVariable(ContextWrappingVariable):
    """represents torch.nn.attention.sdpa_kernel"""
    @staticmethod
    def create(tx: InstructionTranslator, backends, set_priority: bool = False, **kwargs): ...
    set_priority: Incomplete
    def __init__(self, target_values: list[torch.nn.attention.SDPBackend], initial_values=None, set_priority: bool = False, **kwargs) -> None: ...
    @staticmethod
    def _backends_to_nodes(tx, backends): ...
    prev_backends: Incomplete
    def enter(self, tx): ...
    def exit(self, tx: InstructionTranslator, *args): ...
    def module_name(self): ...
    def fn_name(self): ...

class StreamVariable(VariableTracker):
    proxy: Incomplete
    value: Incomplete
    device: Incomplete
    def __init__(self, proxy, value, device, **kwargs) -> None: ...
    def python_type(self): ...
    def call_method(self, tx, name, args: list[VariableTracker], kwargs: dict[str, VariableTracker]) -> VariableTracker: ...
    def as_proxy(self): ...
    def reconstruct(self, codegen: PyCodegen): ...

class EventVariable(VariableTracker):
    proxy: Incomplete
    value: Incomplete
    def __init__(self, proxy, value, **kwargs) -> None: ...
    def call_method(self, tx, name, args: list[VariableTracker], kwargs: dict[str, VariableTracker]) -> VariableTracker: ...
    def as_proxy(self): ...
    def reconstruct(self, codegen: PyCodegen): ...

class DynamoConfigPatchVariable(ContextWrappingVariable):
    """represents torch._dynamo.patch_dynamo_config"""
    initial_values: Incomplete
    def __init__(self, target_values, **kwargs) -> None: ...
    def enter(self, tx): ...
    def exit(self, tx: InstructionTranslator, *args): ...
    def _call_func(self, tx: InstructionTranslator, values): ...
    def module_name(self): ...
    def fn_name(self): ...

class WithExitFunctionVariable(VariableTracker):
    _nonvar_fields: Incomplete
    ctx: Incomplete
    target: Incomplete
    def __init__(self, ctx: ContextWrappingVariable | GenericContextWrappingVariable, target, **kwargs) -> None: ...
    def call_function(self, tx: InstructionTranslator, args: list[VariableTracker], kwargs: dict[str, VariableTracker]) -> VariableTracker: ...
    def reconstruct(self, codegen: PyCodegen): ...
