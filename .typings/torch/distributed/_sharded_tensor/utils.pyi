import torch
from .metadata import ShardedTensorMetadata as ShardedTensorMetadata, TensorProperties as TensorProperties
from .shard import Shard as Shard
from collections.abc import Sequence
from torch.distributed import distributed_c10d as c10d, rpc as rpc
from torch.distributed._shard.metadata import ShardMetadata as ShardMetadata
from torch.distributed._shard.sharding_spec._internals import check_tensor as check_tensor, validate_non_overlapping_shards_metadata as validate_non_overlapping_shards_metadata

def _parse_and_validate_remote_device(pg, remote_device): ...
def _validate_output_tensor_for_gather(my_rank: int, dst_rank: int, size: torch.Size, dst_tensor: torch.Tensor | None) -> None: ...
def _flatten_tensor_size(size) -> torch.Size:
    """
    Checks if tensor size is valid, then flatten/return a torch.Size object.
    """
def _raise_if_mismatch(expected, actual, prop_name, ranks, is_local: bool = True) -> None: ...
def build_metadata_from_local_shards(local_shards: list[Shard], global_size: torch.Size, current_rank: int, pg: c10d.ProcessGroup) -> ShardedTensorMetadata: ...
def build_global_metadata(gathered_metadatas: Sequence[ShardedTensorMetadata | None], recalc_metadata: bool = False): ...
def recalc_global_sharded_tensor_metadata(global_sharded_tensor_metadata: ShardedTensorMetadata, sharded_dim: int) -> None: ...
