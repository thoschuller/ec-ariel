import torch
import torch.nn as nn
from dataclasses import dataclass
from enum import Enum
from torch.distributed._composable.contract import _get_registry as _get_registry
from torch.distributed.tensor import DTensor as DTensor, DeviceMesh as DeviceMesh
from torch.distributed.tensor._dtensor_spec import DTensorSpec as DTensorSpec
from typing import Any

_compiled_autograd_enabled: bool

def detect_compiled_autograd() -> None: ...
def compiled_autograd_enabled(): ...

@dataclass
class DataParallelMeshInfo:
    mesh: DeviceMesh
    shard_mesh_dim: int | None = ...
    replicate_mesh_dim: int | None = ...
    def __post_init__(self) -> None: ...

@dataclass
class FSDPMeshInfo(DataParallelMeshInfo):
    shard_mesh_size: int = ...
    shard_process_group = ...
    shard_mesh_rank: int = ...
    def __post_init__(self) -> None: ...

@dataclass
class DDPMeshInfo(DataParallelMeshInfo):
    replicate_mesh_size: int = ...
    replicate_process_group = ...
    replicate_mesh_rank: int = ...
    def __post_init__(self) -> None: ...

@dataclass
class HSDPMeshInfo(FSDPMeshInfo, DDPMeshInfo):
    def __post_init__(self) -> None: ...

class TrainingState(Enum):
    """Describes the training state of one FSDP state / parameter group."""
    FORWARD = ...
    PRE_BACKWARD = ...
    POST_BACKWARD = ...
    IDLE = ...

def _raise_assert_with_print(*args: Any, **kwargs: Any): ...
def _is_composable_with_fsdp(module: nn.Module) -> bool: ...
def _get_dim0_padded_size(tensor_size: torch.Size, dim0_factor: int) -> torch.Size: ...
def _chunk_with_empty(tensor: torch.Tensor, num_chunks: int, dim: int) -> list[torch.Tensor]: ...
def _get_dim_chunked_size(chunk: torch.Tensor, unchunked_size: torch.Size, dim: int) -> torch.Size: ...
def _from_local_no_grad(local_tensor: torch.Tensor, sharding_spec: DTensorSpec) -> DTensor:
    """
    This method is similar to ``DTensor.from_local()`` except that in eager mode
    it avoids some CPU overhead by avoiding default args and not being differentiable.
    """
def _to_dtype_if_needed(tensor: torch.Tensor, dtype: torch.dtype | None) -> torch.Tensor: ...
def _cast_fp_tensor(dtype: torch.dtype, x: torch.Tensor) -> torch.Tensor: ...
