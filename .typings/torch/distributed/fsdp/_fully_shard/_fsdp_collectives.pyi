import torch
import torch.distributed as dist
from ._fsdp_common import _get_dim0_padded_size as _get_dim0_padded_size, _raise_assert_with_print as _raise_assert_with_print, _to_dtype_if_needed as _to_dtype_if_needed, compiled_autograd_enabled as compiled_autograd_enabled
from ._fsdp_param import FSDPParam as FSDPParam, ShardedState as ShardedState
from _typeshed import Incomplete
from torch.distributed.device_mesh import _get_device_handle as _get_device_handle
from torch.distributed.distributed_c10d import ReduceOp as ReduceOp, _resolve_process_group as _resolve_process_group
from torch.distributed.tensor import DTensor as DTensor
from typing import Callable, NamedTuple

class AllGatherResult(NamedTuple):
    all_gather_output: torch.Tensor
    all_gather_event: torch.Event | None
    all_gather_work: dist.distributed_c10d.Work | None
    param_all_gather_input_dtypes: list[list[torch.dtype]]
    param_all_gather_input_numels: list[list[int]]
    all_gather_input_split_sizes: list[int]

def allocate_memory(size: int, dtype: torch.dtype, device: torch.device, group: dist.ProcessGroup, from_process_group: bool) -> torch.Tensor: ...

lib: Incomplete

def all_gather_copy_in_meta(all_gather_inputs: list[torch.Tensor], inp_split_sizes: list[int], all_gather_input_numel: int, world_size: int, rank: int, dtype: torch.dtype, device: torch.device, group_name: str, allocate_memory_from_process_group: bool) -> tuple[torch.Tensor, torch.Tensor]: ...
def all_gather_copy_in_cuda(all_gather_inputs: list[torch.Tensor], inp_split_sizes: list[int], all_gather_input_numel: int, world_size: int, rank: int, dtype: torch.dtype, device: torch.device, group_name: str, allocate_memory_from_process_group: bool) -> tuple[torch.Tensor, torch.Tensor]: ...
def split_with_sizes_copy(all_gather_output: torch.Tensor, all_gather_input_split_sizes: list[int], dim: int, out: list[torch.Tensor]) -> None: ...
def chunk_cat(tensors: list[torch.Tensor], dim: int, num_chunks: int, out: torch.Tensor) -> None: ...
def foreach_all_gather(fsdp_params: list[FSDPParam], group: dist.ProcessGroup, async_op: bool, all_gather_copy_in_stream: torch.Stream, all_gather_stream: torch.Stream, device: torch.device, allocate_memory_from_process_group: bool = False) -> AllGatherResult | None: ...
def _get_param_all_gather_inputs(fsdp_params: list[FSDPParam]) -> list[list[torch.Tensor]]: ...
def foreach_all_gather_copy_out(all_gather_result: AllGatherResult, fsdp_params: list[FSDPParam], group: dist.ProcessGroup) -> None: ...
def foreach_reduce(fsdp_params: list[FSDPParam], unsharded_grads: list[torch.Tensor], reduce_scatter_group: dist.ProcessGroup, reduce_scatter_stream: torch.Stream, orig_dtype: torch.dtype | None, reduce_dtype: torch.dtype | None, device: torch.device, gradient_divide_factor: float | None, all_reduce_group: dist.ProcessGroup | None, all_reduce_stream: torch.Stream, all_reduce_grads: bool, partial_reduce_output: torch.Tensor | None, all_reduce_hook: Callable[[torch.Tensor], None] | None, allocate_memory_from_process_group: bool = False, force_sum_reduction_for_comms: bool = False) -> tuple[torch.Tensor, torch.Event, torch.Event, torch.Tensor | None, torch.Event | None, torch.Tensor | None]:
    """
    ``unsharded_grads`` owns the references to the gradients computed by
    autograd, so clearing the list frees the gradients.
    """
def foreach_reduce_scatter_copy_in(unsharded_grads: list[torch.Tensor], reduce_scatter_input: torch.Tensor, world_size: int) -> None: ...
def _get_all_gather_input_metadatas(param_all_gather_inputs: list[list[torch.Tensor]]) -> tuple[list[list[torch.dtype]], list[list[int]], torch.dtype]: ...
def _get_gradient_divide_factors(reduce_scatter_group: dist.ProcessGroup, all_reduce_group: dist.ProcessGroup | None, reduce_dtype: torch.dtype, device_type: str = '', factor: float | None = None, force_sum_reduction_for_comms: bool = False) -> tuple[float | None, float | None, dist.ReduceOp | dist.ReduceOp.RedOpType, dist.ReduceOp | dist.ReduceOp.RedOpType]: ...
def _div_if_needed(tensor: torch.Tensor, div_factor: float | None) -> None: ...
