from torch.ao.quantization.fake_quantize import FakeQuantize as FakeQuantize, FakeQuantizeBase as FakeQuantizeBase, FixedQParamsFakeQuantize as FixedQParamsFakeQuantize, FusedMovingAvgObsFakeQuantize as FusedMovingAvgObsFakeQuantize, _is_fake_quant_script_module as _is_fake_quant_script_module, _is_per_channel as _is_per_channel, _is_per_tensor as _is_per_tensor, _is_symmetric_quant as _is_symmetric_quant, default_fake_quant as default_fake_quant, default_fixed_qparams_range_0to1_fake_quant as default_fixed_qparams_range_0to1_fake_quant, default_fixed_qparams_range_neg1to1_fake_quant as default_fixed_qparams_range_neg1to1_fake_quant, default_fused_act_fake_quant as default_fused_act_fake_quant, default_fused_per_channel_wt_fake_quant as default_fused_per_channel_wt_fake_quant, default_fused_wt_fake_quant as default_fused_wt_fake_quant, default_histogram_fake_quant as default_histogram_fake_quant, default_per_channel_weight_fake_quant as default_per_channel_weight_fake_quant, default_weight_fake_quant as default_weight_fake_quant, disable_fake_quant as disable_fake_quant, disable_observer as disable_observer, enable_fake_quant as enable_fake_quant, enable_observer as enable_observer
