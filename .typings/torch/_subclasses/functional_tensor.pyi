import abc
import contextlib
import torch
import types
import weakref
from _typeshed import Incomplete
from abc import ABC, abstractmethod
from contextlib import AbstractContextManager
from torch._ops import _get_dispatch_mode_pre_dispatch as _get_dispatch_mode_pre_dispatch
from torch._subclasses.meta_utils import is_sparse_any as is_sparse_any
from torch.utils._python_dispatch import TorchDispatchMode as TorchDispatchMode, _detect_infra_mode as _detect_infra_mode, _disable_infra_mode as _disable_infra_mode, return_and_correct_aliasing as return_and_correct_aliasing
from typing import Any, Callable

not_implemented_log: Incomplete

def _conversion_method_template(**extra_kwargs): ...

class FunctionalTensor(torch.Tensor):
    """
    Functional tensors represent tensors that will remove mutations
    from a program. If you perform a mutable operation on a functional tensor,
    it will re-dispatch to the functional variant of that operation.

    Historically, functionalization is implemented in C++ in the dispatcher.
    This class is a lightweight python shim around the C++ functionalization logic.

    FunctionalTensor is required to be used with a corresponding
    FunctionalTensormode active, because it relies
    on using the mode for dispatch (which can properly handle factory functions).
    """
    elem: torch.Tensor
    _mode_key: Incomplete
    _extra_dispatch_keys: Incomplete
    metadata_fns: Incomplete
    _inference_mode_base: FunctionalTensor | None
    def __new__(cls, elem, mode): ...
    def __torch_dispatch__(self, func, types, args=(), kwargs=None): ...
    def __repr__(self) -> str: ...
    @staticmethod
    def to_functional(x): ...
    def from_functional(self): ...
    def is_base_tensor(self) -> bool: ...
    def replace_(self, output) -> None: ...
    def commit_update(self) -> None: ...
    def sync(self) -> None: ...
    def mark_mutation_hidden_from_autograd(self) -> None: ...
    def tolist(self) -> Any: ...
    def to(self, *args, **kwargs): ...
    def cuda(self, device=None, *args, **kwargs): ...
    char: Incomplete
    cpu: Incomplete
    bfloat16: Incomplete
    byte: Incomplete
    double: Incomplete
    float: Incomplete
    bool: Incomplete
    half: Incomplete
    int: Incomplete
    long: Incomplete
    def to_dense(self): ...
    @property
    def layout(self): ...
    def __bool__(self) -> bool: ...

class FunctionalTensorMode(TorchDispatchMode):
    export: Incomplete
    is_on_stack: bool
    enter_stack: Incomplete
    _mode_key: Incomplete
    pre_dispatch: Incomplete
    _dispatch_key: Incomplete
    _tokens: dict[Any, torch.Tensor]
    _tokens_forward_output: dict[Any, torch.Tensor]
    _allow_token_discovery: Incomplete
    _storage_to_base: weakref.WeakKeyDictionary[torch.storage.UntypedStorage, FunctionalTensor | None]
    def __init__(self, pre_dispatch: bool = False, export: bool = False, _allow_token_discovery: bool = False) -> None: ...
    def __enter__(self): ...
    def __exit__(self, a: type[BaseException] | None, b: BaseException | None, c: types.TracebackType | None) -> None: ...
    def __torch_dispatch__(self, func, types, args=(), kwargs=None): ...
    @classmethod
    def is_infra_mode(cls) -> bool: ...

@contextlib.contextmanager
def disable_functional_mode(): ...
def dispatch_functionalize(func, mode: FunctionalTensorMode = ...): ...

class BaseFunctionalizeAPI(ABC, metaclass=abc.ABCMeta):
    @abstractmethod
    def wrap_tensors(self, args: tuple[Any]) -> tuple[Any]: ...
    @abstractmethod
    def unwrap_tensors(self, args: torch.Tensor | tuple[torch.Tensor, ...]) -> Any: ...
    @abstractmethod
    def functionalize(self, inner_f: Callable) -> Callable: ...
    @abstractmethod
    def redispatch_to_next(self) -> AbstractContextManager: ...
    @abstractmethod
    def replace(self, input_tensor, output_tensor) -> None: ...
    @abstractmethod
    def commit_update(self, tensor) -> None: ...
    @abstractmethod
    def sync(self, tensor) -> None: ...
    @abstractmethod
    def mark_mutation_hidden_from_autograd(self, tensor) -> None: ...

class PythonFunctionalizeAPI(BaseFunctionalizeAPI):
    mode: Incomplete
    pre_dispatch: Incomplete
    def __init__(self, mode: FunctionalTensorMode | None = None, pre_dispatch: bool = False) -> None: ...
    def wrap_tensors(self, args: tuple[Any]) -> tuple[Any]: ...
    def unwrap_tensors(self, args: torch.Tensor | tuple[torch.Tensor, ...] | list[torch.Tensor]) -> Any: ...
    def functionalize(self, inner_f: Callable) -> Callable: ...
    def redispatch_to_next(self) -> AbstractContextManager: ...
    def replace(self, input_tensor, output_tensor) -> None: ...
    def commit_update(self, tensor) -> None: ...
    def sync(self, tensor) -> None: ...
    def mark_mutation_hidden_from_autograd(self, tensor) -> None: ...

class CppFunctionalizeAPI(BaseFunctionalizeAPI):
    def wrap_tensors(self, args: tuple[Any]) -> tuple[Any]: ...
    def unwrap_tensors(self, args: torch.Tensor | tuple[torch.Tensor, ...]) -> torch.Tensor | tuple[torch.Tensor, ...]: ...
    def functionalize(self, inner_f: Callable) -> Callable: ...
    def redispatch_to_next(self) -> AbstractContextManager: ...
    def replace(self, input_tensor, output_tensor) -> None: ...
    def commit_update(self, tensor) -> None: ...
    def sync(self, tensor) -> None: ...
    def mark_mutation_hidden_from_autograd(self, tensor) -> None: ...

class FunctorchFunctionalizeAPI(BaseFunctionalizeAPI):
    interpreter: Incomplete
    def __init__(self, interpreter) -> None: ...
    def wrap_tensors(self, args: tuple[Any]) -> tuple[Any]: ...
    def unwrap_tensors(self, args: torch.Tensor | tuple[torch.Tensor, ...]) -> torch.Tensor | tuple[torch.Tensor, ...]: ...
    def functionalize(self, inner_f: Callable) -> Callable: ...
    def redispatch_to_next(self) -> AbstractContextManager: ...
    def replace(self, input_tensor, output_tensor) -> None: ...
    def commit_update(self, tensor) -> None: ...
    def sync(self, tensor) -> None: ...
    def mark_mutation_hidden_from_autograd(self, tensor) -> None: ...

def mb_unwrap_functional_tensor(tensor: torch.Tensor): ...
