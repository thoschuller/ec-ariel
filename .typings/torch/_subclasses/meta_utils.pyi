import abc
import torch
import weakref
from _typeshed import Incomplete
from abc import abstractmethod
from collections.abc import Generator
from contextlib import contextmanager
from dataclasses import dataclass
from torch._C._autograd import CreationMeta as CreationMeta
from torch._C._functorch import CInterpreter as CInterpreter, _add_batch_dim as _add_batch_dim, _unwrap_functional_tensor as _unwrap_functional_tensor, _wrap_functional_tensor as _wrap_functional_tensor, get_unwrapped as get_unwrapped, is_batchedtensor as is_batchedtensor, is_functorch_wrapped_tensor as is_functorch_wrapped_tensor, is_gradtrackingtensor as is_gradtrackingtensor, is_legacy_batchedtensor as is_legacy_batchedtensor, maybe_get_bdim as maybe_get_bdim, maybe_get_level as maybe_get_level, peek_interpreter_stack as peek_interpreter_stack
from torch._dispatch.python import enable_python_dispatcher as enable_python_dispatcher
from torch._guards import Source as Source
from torch._logging import trace_structured as trace_structured
from torch._subclasses.fake_tensor import FakeTensor as FakeTensor, FakeTensorMode as FakeTensorMode
from torch.fx.experimental.symbolic_shapes import ShapeEnv as ShapeEnv, SymbolicContext as SymbolicContext
from torch.utils._mode_utils import no_dispatch as no_dispatch
from torch.utils._python_dispatch import is_traceable_wrapper_subclass as is_traceable_wrapper_subclass
from torch.utils.weak import WeakIdKeyDictionary as WeakIdKeyDictionary
from typing import Callable, ClassVar, Generic, Protocol, TypeVar
from typing_extensions import TypeGuard, TypeIs, TypedDict, Unpack, override

def _is_fake_tensor(t: object) -> TypeIs[FakeTensor]: ...
DimList = list
_TensorLikeT = TypeVar('_TensorLikeT', 'MetaTensorDesc', torch.Tensor)
_T = TypeVar('_T')
_TensorT = TypeVar('_TensorT', bound=torch.Tensor)
_TensorT_cov = TypeVar('_TensorT_cov', bound=torch.Tensor, covariant=True)

def safe_is_leaf(t: MetaTensorDesc | torch.Tensor) -> bool: ...
def safe_grad(t: _TensorLikeT) -> _TensorLikeT | None: ...
def _expect_safe_grad(t: _TensorLikeT) -> _TensorLikeT: ...
def assert_eq(a: _T, b: _T) -> None: ...

tls: Incomplete

@contextmanager
def disable_inference_mode_for_fake_prop() -> Generator[None, None, None]: ...
def assert_metadata_eq(assert_eq: Callable[[object, object], None], m1: MetaTensorDesc | torch.Tensor, m2: torch.Tensor, *, skip_symbolic: bool = False, skip_leaf: bool = False) -> None: ...
def is_sparse_coo(t: object) -> TypeGuard[torch.Tensor]: ...
def is_sparse_compressed_layout(layout: torch.layout) -> bool: ...
def is_sparse_compressed(t: object) -> TypeGuard[torch.Tensor]: ...
def is_sparse_any(t: object) -> TypeGuard[torch.Tensor]: ...
def _checked_cast(ty: type[_T], obj: object) -> _T: ...
def _get_real_storage(base: torch.UntypedStorage) -> torch.UntypedStorage: ...
def _set_real_storage(base: torch.UntypedStorage, real_storage: torch.UntypedStorage) -> None: ...

MetaStorageId: Incomplete
MetaTensorId: Incomplete
_DescriberId: Incomplete
DESCRIBER_NEXT_ID: Incomplete

class MetaTensorDescriber:
    """
    Given a Tensor/Storage, generate a MetaTensorDesc/MetaStorageDesc
    for it, which is enough information to reconstruct a meta tensor/fake tensor
    corresponding to a Tensor as faithfully as possible.

    This is a stateful conversion object because we keep track of the IDs
    of the tensors/storages passed to us, so we can consistently give
    the same ID when we see the same tensor/storage.
    """
    id: Incomplete
    next_tensor_id: MetaTensorId
    next_storage_id: MetaStorageId
    lookup_tensor: Incomplete
    lookup_storage: Incomplete
    copy_data: Incomplete
    traced_tensors: set[int]
    traced_storages: set[int]
    def __init__(self, *, copy_data: bool = False) -> None: ...
    def get_tensor_id(self, t: torch.Tensor) -> MetaTensorId: ...
    def get_storage_id(self, s: torch.UntypedStorage) -> MetaStorageId: ...
    def describe_storage(self, s: torch.UntypedStorage, *, trace: bool = False) -> MetaStorageDesc: ...
    def describe_tensor(self, t: torch.Tensor, *, recurse: bool = True, trace: bool = False) -> MetaTensorDesc: ...

@dataclass(frozen=True)
class MetaStorageDesc:
    id: MetaStorageId
    size: int
    data: torch.UntypedStorage | None
    def as_json(self, describer_id: _DescriberId) -> dict[str, object]: ...

@dataclass(frozen=True)
class ViewFunc(Generic[_TensorT], metaclass=abc.ABCMeta):
    @abstractmethod
    def apply(self, t: _TensorT, new_base: _TensorT, symint_visitor_fn: Callable[[int], int] | None = None, tensor_visitor_fn: Callable[[torch.Tensor], _TensorT] | None = None) -> _TensorT: ...
    @staticmethod
    def from_tensor(t: torch.Tensor) -> ViewFunc: ...

@dataclass(frozen=True)
class _FakeTensorViewFunc(ViewFunc['FakeTensor']):
    @override
    def apply(self, t: torch.Tensor, new_base: torch.Tensor, symint_visitor_fn: Callable[[int], int] | None = None, tensor_visitor_fn: Callable[[torch.Tensor], FakeTensor] | None = None) -> FakeTensor: ...

@dataclass(frozen=True)
class _CustomViewFunc(ViewFunc[_TensorT], Generic[_TensorT]):
    func: Callable[[torch.Tensor, Callable[[int], int] | None, Callable[[torch.Tensor], _TensorT] | None], _TensorT]
    @override
    def apply(self, t: torch.Tensor, new_base: torch.Tensor, symint_visitor_fn: Callable[[int], int] | None = None, tensor_visitor_fn: Callable[[torch.Tensor], _TensorT] | None = None) -> _TensorT: ...

class _MetaTensorCallback(Generic[_TensorT_cov], Protocol):
    def __call__(self, arg: Callable[[], torch.Tensor], /, *, device: torch.device | str) -> _TensorT_cov: ...

class _MetaTensorCallbackKwargs(TypedDict, total=False):
    device: torch.device | str

class _MetaTensorCallbackOptDevice(Generic[_TensorT_cov], Protocol):
    def __call__(self, arg: Callable[[], torch.Tensor], /, **kwargs: Unpack[_MetaTensorCallbackKwargs]) -> _TensorT_cov: ...

@dataclass(frozen=True)
class MetaTensorDesc(Generic[_TensorT]):
    id: MetaTensorId
    ndim: int
    dtype: torch.dtype
    device: torch.device
    size: tuple[int, ...]
    dynamo_dynamic_indices: list[int]
    layout: torch.layout = ...
    is_inference: bool = ...
    is_leaf: bool = ...
    requires_grad: bool = ...
    is_sparse: bool = ...
    is_mkldnn: bool = ...
    is_functorch_wrapped: bool = ...
    is_batchedtensor: bool = ...
    is_legacy_batchedtensor: bool = ...
    is_gradtrackingtensor: bool = ...
    is_view: bool = ...
    is_nested: bool = ...
    nested_int: int | None = ...
    is_traceable_wrapper_subclass: bool = ...
    is_functional: bool = ...
    is_conj: bool = ...
    is_neg: bool = ...
    is_parameter: bool = ...
    stride: tuple[int, ...] | None = ...
    storage_offset: int = ...
    storage: MetaStorageDesc | None = ...
    sparse_dim: int | None = ...
    dense_dim: int | None = ...
    is_coalesced: bool | None = ...
    crow_indices: MetaTensorDesc | None = ...
    col_indices: MetaTensorDesc | None = ...
    ccol_indices: MetaTensorDesc | None = ...
    row_indices: MetaTensorDesc | None = ...
    values: MetaTensorDesc | None = ...
    unwrapped: MetaTensorDesc | None = ...
    bdim: int | None = ...
    base: MetaTensorDesc | None = ...
    attrs: dict[str, MetaTensorDesc] | None = ...
    creation_meta: CreationMeta | None = ...
    grad: MetaTensorDesc | None = ...
    _UNSERIALIZABLE: ClassVar[set[str]] = ...
    ctx: object | None = ...
    type: type | None = ...
    fake_mode: FakeTensorMode | None = ...
    view_func: ViewFunc | None = ...
    level: int | None = ...
    current_level: int | None = ...
    functorch_stack: list[CInterpreter] | None = ...
    autograd_meta_from: torch.Tensor | None = ...
    data: torch.Tensor | None = ...
    def as_json(self, describer_id: _DescriberId) -> dict[str, object]: ...
    @property
    def shape(self) -> tuple[int, ...]: ...

def _safe_copy(dst: torch.Tensor, src: torch.Tensor | None) -> None: ...
def _safe_clone(src: torch.Tensor) -> torch.Tensor | None: ...

class MetaConverter(Generic[_TensorT]):
    storage_memo: weakref.WeakValueDictionary[MetaStorageId, torch.UntypedStorage]
    tensor_memo: weakref.WeakValueDictionary[MetaTensorId, _TensorT]
    hit: int
    miss: int
    del_hook: Incomplete
    arg_cnt: int
    copy_data: Incomplete
    describer: Incomplete
    def __init__(self, *, copy_data: bool = False) -> None: ...
    def successful(self) -> bool: ...
    def get_tensor_memo(self, t: MetaTensorDesc) -> torch.Tensor | None: ...
    def _checked_get_tensor_memo(self, t: MetaTensorDesc) -> _TensorT: ...
    def set_tensor_memo(self, t: MetaTensorDesc, v: _TensorT) -> None: ...
    def get_storage_memo(self, s: MetaStorageDesc) -> torch.UntypedStorage | None: ...
    def set_storage_memo(self, s: MetaStorageDesc, v: torch.UntypedStorage) -> None: ...
    def meta_storage(self, s: MetaStorageDesc, callback: Callable[[Callable[[], torch.Tensor]], _TensorT]) -> torch.UntypedStorage: ...
    @classmethod
    def _checked_cast_tensor_t(cls, t: torch.Tensor) -> _TensorT: ...
    @classmethod
    def _identity_callable(cls, t: Callable[[], torch.Tensor], device: torch.device | str | None = None) -> _TensorT: ...
    @classmethod
    def _backward_error(cls, t: _TensorT) -> _TensorT: ...
    def meta_tensor(self, t: MetaTensorDesc, shape_env: ShapeEnv | None, callback_: _MetaTensorCallback[_TensorT], source: Source | None, symbolic_context: SymbolicContext | None) -> _TensorT: ...
    def __call__(self, t: torch.Tensor, shape_env: ShapeEnv | None = None, *, callback: _MetaTensorCallback[_TensorT] | None = None, source: Source | None = None, symbolic_context: SymbolicContext | None = None, trace: bool = True) -> _TensorT: ...
