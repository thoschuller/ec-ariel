import torch
from collections.abc import Iterable
from torch import Tensor
from typing import TypeVar

__all__ = ['bartlett', 'blackman', 'cosine', 'exponential', 'gaussian', 'general_cosine', 'general_hamming', 'hamming', 'hann', 'kaiser', 'nuttall']

_T = TypeVar('_T')

def exponential(M: int, *, center: float | None = None, tau: float = 1.0, sym: bool = True, dtype: torch.dtype | None = None, layout: torch.layout = ..., device: torch.device | None = None, requires_grad: bool = False) -> Tensor: ...
def cosine(M: int, *, sym: bool = True, dtype: torch.dtype | None = None, layout: torch.layout = ..., device: torch.device | None = None, requires_grad: bool = False) -> Tensor: ...
def gaussian(M: int, *, std: float = 1.0, sym: bool = True, dtype: torch.dtype | None = None, layout: torch.layout = ..., device: torch.device | None = None, requires_grad: bool = False) -> Tensor: ...
def kaiser(M: int, *, beta: float = 12.0, sym: bool = True, dtype: torch.dtype | None = None, layout: torch.layout = ..., device: torch.device | None = None, requires_grad: bool = False) -> Tensor: ...
def hamming(M: int, *, sym: bool = True, dtype: torch.dtype | None = None, layout: torch.layout = ..., device: torch.device | None = None, requires_grad: bool = False) -> Tensor: ...
def hann(M: int, *, sym: bool = True, dtype: torch.dtype | None = None, layout: torch.layout = ..., device: torch.device | None = None, requires_grad: bool = False) -> Tensor: ...
def blackman(M: int, *, sym: bool = True, dtype: torch.dtype | None = None, layout: torch.layout = ..., device: torch.device | None = None, requires_grad: bool = False) -> Tensor: ...
def bartlett(M: int, *, sym: bool = True, dtype: torch.dtype | None = None, layout: torch.layout = ..., device: torch.device | None = None, requires_grad: bool = False) -> Tensor: ...
def general_cosine(M, *, a: Iterable, sym: bool = True, dtype: torch.dtype | None = None, layout: torch.layout = ..., device: torch.device | None = None, requires_grad: bool = False) -> Tensor: ...
def general_hamming(M, *, alpha: float = 0.54, sym: bool = True, dtype: torch.dtype | None = None, layout: torch.layout = ..., device: torch.device | None = None, requires_grad: bool = False) -> Tensor: ...
def nuttall(M: int, *, sym: bool = True, dtype: torch.dtype | None = None, layout: torch.layout = ..., device: torch.device | None = None, requires_grad: bool = False) -> Tensor: ...
