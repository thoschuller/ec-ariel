from typing import *
import torch
from .nested_tensor import NestedTensor as NestedTensor
from _typeshed import Incomplete
from torch.fx.graph_module import GraphModule as GraphModule
from torch.fx.operator_schemas import normalize_function as normalize_function
from torch.nested._internal.sdpa import jagged_scaled_dot_product_attention as jagged_scaled_dot_product_attention
from typing import Any

__all__: list[Any]
JAGGED_OPS_TABLE: Dict[Any, Any]

def _outer_to_inner_dim(ndim, dim, ragged_dim, canonicalize: bool = False): ...
def _wrap_jagged_dim(ndim, dim, ragged_dim, op_name, convert_to_inner_dim: bool = True, allow_ragged_dim: bool = False, allow_batch_dim: bool = False): ...
def _wrap_jagged_dims(ndim, dims, op_name, ragged_idx: int = 1):
    """
    For NestedTensor operators,
    wraps dimensions to non-negative values,
    and returns metadata related to reduction dimension(s).
    """
def check_schema(schema_str: str, func, *args, **kwargs) -> None: ...
def check_ragged_dim_same(func, a: NestedTensor, a_name: str, b: NestedTensor, b_name: str) -> None: ...
def raggedness_matches(nt, size): ...
def squeeze_leading_ones(t): ...
def register_func(tables, aten_ops, schema_str): ...

register_jagged_func: Incomplete

def lookup_jagged(func, *args, **kwargs) -> Callable | None: ...
def extract_kwargs(arg): ...
def jagged_unary_pointwise(func, *args, **kwargs): ...
def jagged_binary_pointwise(func, *args, **kwargs): ...
def jagged_torch_function(func, *args, **kwargs): ...
def tensor_attr_supported_getter(func, *args, **kwargs): ...
def prim_layout_default(func, *args, **kwargs): ...
def tensor_attr_unsupported_getter(func, *args, **kwargs) -> None: ...
def is_contiguous_general(func, *args, **kwargs): ...
def clone_default(func, *args, **kwargs): ...
def linear_default(func, *args, **kwargs): ...
def linear_backward_default(func, *args, **kwargs): ...
def to_dtype(func, *args, **kwargs): ...
def to_copy_default(func, *args, **kwargs): ...
def copy_default(func, *args, **kwargs): ...
def like_factory_default(func, *args, **kwargs): ...
def zero__default(func, *args, **kwargs): ...
def _softmax_default(func, *args, **kwargs): ...
def _softmax_backward(func, *args, **kwargs): ...
def native_dropout_default(func, *args, **kwargs): ...
def native_dropout_backward_default(func, *args, **kwargs): ...
def prod_dim_int(func, *args, **kwargs): ...
def prod_default(func, *args, **kwargs): ...
def split_tensor(func, *args, **kwargs): ...
def split_with_sizes_default(func, *args, **kwargs): ...
def narrow(func, *args, **kwargs): ...
def chunk_default(func, *args, **kwargs): ...
def unbind_int(func, *args, **kwargs): ...
def squeeze_dim(func, *args, **kwargs): ...
def unsqueeze_default(func, *args, **kwargs): ...
def cat_default(func, *args, **kwargs): ...
def matmul_default(func, *args, **kwargs): ...
def bmm_default(func, *args, **kwargs): ...
def expand_default(func, *args, **kwargs): ...
def expand_as_default(func, *args, **kwargs): ...
def broadcast_to(func, *args, **kwargs): ...
def broadcast_tensors(func, *args, **kwargs): ...
def where_self(func, *args, **kwargs): ...
def _pin_memory_default(func, *args, **kwargs): ...
def is_pinned_default(func, *args, **kwargs): ...
def is_same_size_default(func, *args, **kwargs): ...
def _apply_reduction(func, func_name, identity_element, *args, **kwargs): ...
def sum_default(func, *args, **kwargs): ...
def sum_dim_IntList(func, *args, **kwargs): ...
def transpose_int(func, *args, **kwargs): ...
def permute_default(func, *args, **kwargs): ...
def view_default(func, *args, **kwargs): ...
def native_layer_norm_default(func, *args, **kwargs): ...
def native_layer_norm_backward_default(func, *args, **kwargs): ...
def select_int(func, *args, **kwargs): ...
def slice_tensor(func, *args, **kwargs): ...
def index_put_(func, *args, **kwargs): ...
def convolution_default(func, *args, **kwargs): ...
def mean_dim(func, *args, **kwargs): ...
def mean_default(func, *args, **kwargs): ...
def any_dims(func, *args, **kwargs): ...
def any_dim(func, *args, **kwargs): ...
def all_dims(func, *args, **kwargs): ...
def all_dim(func, *args, **kwargs): ...
def all_any_max_min_default(func, *args, **kwargs): ...
def min_dim(func, *args, **kwargs): ...
def max_dim(func, *args, **kwargs): ...
def amin_default(func, *args, **kwargs): ...
def amax_default(func, *args, **kwargs): ...
def argmin_default(func, *args, **kwargs): ...
def argmax_default(func, *args, **kwargs): ...
def value_selecting_reduction_backward_default(func, *args, **kwargs): ...
def stack_default(func, *args, **kwargs): ...
def embedding_default(func, *args, **kwargs): ...
def embedding_dense_backward_default(func, *args, **kwargs): ...
def values_default(func, *args, **kwargs): ...
def all_default(func, *args, **kwargs): ...
def to_padded_tensor_default(func, *args, **kwargs): ...
def _nested_from_padded_tensor_default(func, *args, **kwargs): ...
def _nested_view_from_jagged_default(func, *args, **kwargs): ...
def _nested_get_offsets(func, *args, **kwargs): ...
def _nested_get_lengths(func, *args, **kwargs): ...
def _nested_get_ragged_idx(func, *args, **kwargs): ...
def _nested_get_min_seqlen(func, *args, **kwargs): ...
def _nested_get_max_seqlen(func, *args, **kwargs): ...
def masked_select_default(func, *args, **kwargs): ...
def _nested_select_backward_default(func, *args, **kwargs): ...
def record_stream_default(func, *args, **kwargs) -> None: ...
def new_empty_default(func, *args, **kwargs): ...
def activation_backward(func, *args, **kwargs): ...
def fill_Scalar(func, *args, **kwargs): ...
def fill__Scalar(func, *args, **kwargs): ...
def frexp_Tensor(func, *args, **kwargs): ...
def matmul_backward_default(func, *args, **kwargs): ...
def flex_njt(query: torch.Tensor, key: torch.Tensor, value: torch.Tensor, score_mod: Callable, block_mask: Tuple, scale: float, kernel_options: Dict[str, Any], score_mod_other_buffers: Tuple = (), mask_mod_other_buffers: Tuple = ()) -> Tuple[torch.Tensor, torch.Tensor]: ...
def flex_njt_backward(query: torch.Tensor, key: torch.Tensor, value: torch.Tensor, out: torch.Tensor, logsumexp: torch.Tensor, grad_out: torch.Tensor, grad_logsumexp: torch.Tensor, fw_graph: Union[Callable, GraphModule], joint_graph: GraphModule, block_mask: Tuple, scale: float, kernel_options: Dict[str, Any], score_mod_other_buffers: Tuple = (), mask_mod_other_buffers: Tuple = ()) -> Tuple[torch.Tensor, torch.Tensor, torch.Tensor, Tuple[torch.Tensor | None, ...]]: ...
def _nested_get_jagged_dummy(func, *args, **kwargs): ...
