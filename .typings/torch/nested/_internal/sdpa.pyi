import torch
from .nested_tensor import NestedTensor as NestedTensor
from _typeshed import Incomplete
from torch.backends.cuda import SDPAParams as SDPAParams, can_use_cudnn_attention as can_use_cudnn_attention, can_use_efficient_attention as can_use_efficient_attention, can_use_flash_attention as can_use_flash_attention, cudnn_sdp_enabled as cudnn_sdp_enabled, flash_sdp_enabled as flash_sdp_enabled, math_sdp_enabled as math_sdp_enabled, mem_efficient_sdp_enabled as mem_efficient_sdp_enabled
from torch.nn.attention import SDPBackend as SDPBackend

log: Incomplete

def _validate_sdpa_input(query: torch.Tensor, key: torch.Tensor, value: torch.Tensor, attn_mask: torch.Tensor | None = None, dropout_p: float = 0.0, is_causal: bool = False, scale=None): ...
def _check_batch_size_nested(params: SDPAParams, debug: bool = False) -> bool: ...
def _check_head_dim_size_flash_nested(params: SDPAParams, debug: bool = False) -> bool: ...
def _check_head_dim_size_cudnn_nested(params: SDPAParams, debug: bool = False) -> bool: ...
def _check_for_seq_len_0_and_consistent_head_dim_nested_helper(param: torch.Tensor, param_name: str, debug: bool = False) -> bool: ...
def _try_broadcast_param_size(q_size, k_size, v_size, param_name, debug: bool = False) -> bool: ...
def _check_for_seq_len_0_nested(params: SDPAParams, debug: bool = False) -> bool: ...
def _can_use_flash_sdpa_jagged(params: SDPAParams, debug: bool = False) -> bool: ...
def _can_use_efficient_sdpa_jagged(params: SDPAParams, debug: bool = False) -> bool: ...
def _can_use_math_sdpa_jagged(params: SDPAParams, debug: bool = False) -> bool: ...
def _select_sdp_backend(query, key, value, attn_mask, dropout, is_causal, enable_gqa): ...
def _cumulative_and_max_seq_len_nnz(qkv: torch.Tensor) -> tuple[torch.Tensor, int, int]: ...
def _is_safe_to_get_storage_as_tensor(tensor: torch.Tensor): ...
def _view_as_dense(tensor: torch.Tensor, Nnz: int, num_heads: int, head_dim: int) -> torch.Tensor: ...
def _sdpa_nested_preprocessing(query, key, value): ...
def _pad_last_dim(tensor: torch.Tensor, alignment_size: int, slice: bool) -> torch.Tensor: ...
def _calculate_scale(query, scale): ...
def _post_process_flash_output(out: torch.Tensor, og_size): ...
def _is_computing_meta_flops(x): ...
def _autocast(query: torch.Tensor, key: torch.Tensor, value: torch.Tensor, attn_mask: torch.Tensor | None) -> tuple[torch.Tensor, torch.Tensor, torch.Tensor, torch.Tensor | None]:
    """
    [Autocasting SDPA for NJT]

    Normal autocasting doesn't work for NJT+SDPA right now:
    * NJT intercepts the __torch_function__ call for scaled_dot_product_attention, which happens
      before we get to any aten ops or dispatcher logic; then the torch_function logic calls into
      efficient attention or flash attention. So, autocasting on the scaled_dot_product_attention
      op won't work because we never see that aten op.
    * If we put autocasting on `_flash_attention_forward`, then we'll get autocasting to run, but
      the kernel selection logic in torch_function handling (ie. jagged_scaled_dot_product_attention)
      won't work correctly: the kernel selection logic will run before autocasting, and choose
      a kernel based on the un-autocasted dtypes; but then autocasting will run and the actual
      attention computation will happen in a different dtype.

    An alternative is to just change the backend selection logic for SDPA+NJT to be autocast-aware
    and rely on autocasting to do the actual conversions for flash attention / efficient attention.
    However, by manually doing the actual autocast before the backend selection, we ensure that the
    autocast handling for backend selection doesn't diverge from the autocast handling for the
    actual dtype conversions.
    """
def jagged_scaled_dot_product_attention(query: torch.Tensor, key: torch.Tensor, value: torch.Tensor, attn_mask: torch.Tensor | None = None, dropout_p: float = 0.0, is_causal: bool = False, scale=None, enable_gqa: bool = False): ...
