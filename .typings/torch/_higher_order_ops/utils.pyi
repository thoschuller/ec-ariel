import torch
from _typeshed import Incomplete
from collections.abc import Generator
from contextlib import contextmanager
from dataclasses import dataclass
from torch._dispatch.python import suspend_functionalization as suspend_functionalization
from torch._guards import detect_fake_mode as detect_fake_mode
from torch._higher_order_ops.schema import HopSchema as HopSchema
from torch._ops import HigherOrderOperator as HigherOrderOperator, OpOverload as OpOverload, OperatorBase as OperatorBase
from torch._subclasses.fake_tensor import FakeTensor as FakeTensor
from torch._subclasses.functional_tensor import FunctionalTensor as FunctionalTensor, disable_functional_mode as disable_functional_mode
from torch.fx.experimental.proxy_tensor import _temp_remove_metadata_torch_function_mode as _temp_remove_metadata_torch_function_mode, disable_proxy_modes_tracing as disable_proxy_modes_tracing, make_fx as make_fx
from torch.fx.passes.runtime_assert import insert_deferred_runtime_asserts as insert_deferred_runtime_asserts
from torch.fx.passes.shape_prop import TensorMetadata as TensorMetadata, _extract_tensor_metadata as _extract_tensor_metadata
from torch.multiprocessing.reductions import StorageWeakRef as StorageWeakRef
from typing import Any, Callable, TypeVar, overload

@dataclass
class UnsupportedAliasMutationException(RuntimeError):
    reason: str

def autograd_not_implemented_inner(operator: OperatorBase, delayed_error: bool, *args: Any, **kwargs: Any) -> Any:
    """If autograd is enabled and any of the arguments require grad this will either
    raise an error or return a DelayedError depending on the value of delayed.

    Args:
        operator: The Operator to call with the *args and **kwargs with
        op_name: The name of the Operator
        delayed_error: If True, return a DelayedError instead of raising an error
        args: The flattened operands to the Operator
        kwargs: The keyword arguments to the Operator

    Raises:
        RuntimeError: If autograd is enabled and any of the arguments to the Operator
    """
def autograd_not_implemented(op: OperatorBase, deferred_error: bool) -> Callable: ...
def _maybe_run_with_interpreter(fn): ...
def _maybe_compile_and_run_fn(fn, *args): ...
def reenter_make_fx(fn): ...
def _maybe_reenter_make_fx(fn): ...
def check_meta_consistency(lhs_list: list[torch.Tensor | torch.SymInt | int], rhs_list: list[torch.Tensor | torch.SymInt | int], lhs_name: str, rhs_name: str, include_contiguity: bool = True) -> None: ...
@contextmanager
def _set_compilation_env() -> Generator[None]: ...
def _maybe_fake_tracing(fn, inputs: list[Any], pre_dispatch): ...
def potential_input_alias_or_mutation(gm, inputs, pre_dispatch: bool = False): ...
def analyze_potential_input_alias_or_mutation(name, aliases, input_mutations) -> None: ...
def _has_potential_branch_input_mutation(gm, inputs, pre_dispatch: bool = False): ...
def has_potential_input_alias_or_mutation(gm, inputs, pre_dispatch: bool = False): ...
def _collect_fake_inputs(inputs): ...
def _check_alias_and_mutation(graph_module, inputs_fake, name, pre_dispatch) -> None: ...
def unique_graph_id(proxy_mode, prefix):
    """Returns a unique name and id for a graph to be added to a proxy_mode tracer"""
def unique_graph_name_with_root(root: torch.fx.GraphModule, prefix: str) -> tuple[int, str]: ...
def _from_fun(t): ...
def clone_outputs_aliasing_inputs(args): ...
def prepare_fw_with_masks(fn): ...
def prepare_fw_with_masks_all_requires_grad(fn): ...
def unmask_none_gradients(grads, operands): ...
def _maybe_fake_prop_ignore_unbacked(fn, args): ...
def redirect_to_mode(hop: OperatorBase, mode):
    """Utility for redispatching HOP to underlying mode

    Args:
        hop: The HOP to redispatch
        mode: The mode to redispatch to

    Returns:
        A decorated function that implements the HOP for the given mode
    """
def create_fw_bw_graph(fn, use_output_and_grad_bw, fw_inputs, fw_outputs): ...
def _unstack_pytree(xs): ...
def _stack_pytree(pytrees): ...
def save_tensors_and_symints_for_backward(ctx, args) -> None: ...
def saved_tensors_and_symints(ctx): ...
def get_dummy_aot_autograd_config(): ...
def first_slice_copy(t: torch.Tensor, dim: int = 0) -> torch.Tensor: ...
def diff_tensor_meta(meta1: TensorMetadata, meta2: TensorMetadata, check_grad: bool = True) -> list[str]: ...
def validate_subgraph_args_types(lifted_args: tuple[Any, ...] | list[Any]): ...
def check_input_alias_and_mutation(gm: torch.fx.GraphModule, fake_args: list[FakeTensor]) -> tuple[dict[int, int], dict[int, int], dict[int, int], list[int]]: ...
def check_input_alias_and_mutation_return_outputs(gm: torch.fx.GraphModule, fake_args: list[FakeTensor] | tuple[FakeTensor, ...]) -> tuple[dict[int, int], dict[int, int], dict[int, int], list[int], tuple[Any, ...] | list[Any]]: ...

registered_hop_fake_fns: dict[torch._ops.OpOverload, Callable]
F = TypeVar('F', bound=Callable)

@overload
def register_fake(hop, fn: None = None) -> Callable[[F], F]: ...
@overload
def register_fake(hop, fn: F) -> F: ...

class FunctionalizeCtxWrapper:
    """
    This is a dummy wrapper to facilitate fake tensor caching.

    For AOT Dispatcher metadata collection pass, HOPs go from functionalization
    key to fake tensor key. The functionalization key wraps the subgraphs in a
    function, which changes from call to call even though the subgraph might
    still be same.

    To enable fake tensor caching, we just wrap the ctx and subgraph in this
    class and then use the subgraph as the hash.
    """
    ctx: Incomplete
    subgraph: Incomplete
    @torch._disable_dynamo
    def __init__(self, ctx, subgraph) -> None: ...
    def __hash__(self): ...
    def __repr__(self) -> str: ...
    def __call__(self, *args, **kwargs): ...

class HopInstance:
    _op: Incomplete
    _schema: Incomplete
    def __init__(self, op: HigherOrderOperator, schema: HopSchema) -> None: ...
    def __call__(self, *args, **kwargs): ...
    @staticmethod
    def create(hop: HigherOrderOperator, *args, **kwargs): ...

def call_op(op: OpOverload | HopInstance, args, kwargs): ...
def materialize_as_graph(fn: Callable, args: tuple[Any], include_key_set: torch._C.DispatchKeySet | None = None, exclude_key_set: torch._C.DispatchKeySet | None = None, force_enable_grad: bool = False) -> torch.fx.GraphModule: ...
def materialize_callable_in_args(op: HopInstance, args, kwargs): ...
def has_user_subclass(args, allowed_subclasses):
    """Check if any tensor arguments are user subclasses.

    This is used to determine if tensor subclasses should get a chance to run
    their own implementation first before falling back to the default implementation.

    Args:
        args: Arguments to check (will be flattened with pytree)
        allowed_subclasses: Tuple of allowed subclass types

    Returns:
        True if user tensor subclasses are found, False otherwise
    """
def _has_gen_schema(op: HigherOrderOperator): ...
