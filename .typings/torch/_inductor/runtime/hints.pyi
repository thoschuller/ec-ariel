import functools
import typing
from _typeshed import Incomplete
from enum import Enum
from torch.utils._triton import has_triton_package as has_triton_package
from typing import NamedTuple

TRITON_MAX_BLOCK: Incomplete
TRITON_MAX_RSPLIT: int

class ReductionHint(Enum):
    INNER = 0
    OUTER = 1
    OUTER_TINY = 2
    DEFAULT = 3

class TileHint(Enum):
    SQUARE = 0
    DEFAULT = 1

def AttrsDescriptorWrapper(divisible_by_16=None, equal_to_1=None): ...

class AttrsDescriptorWrapper(NamedTuple):
    divisible_by_16: Incomplete
    equal_to_1: Incomplete

_NUM_THREADS_PER_WARP: int

class HeuristicType(Enum):
    PERSISTENT_REDUCTION = ...
    POINTWISE = ...
    REDUCTION = ...
    SPLIT_SCAN = ...
    TEMPLATE = ...
    USER_AUTOTUNE = ...
    FIXED = ...

class AutotuneHint(Enum):
    ONE_ELEMENT_PER_THREAD = 0
    __repr__ = ...

class DeviceProperties(typing.NamedTuple):
    """Copy device properties into a data structure not requiring torch to be imported"""
    type: str
    index: int
    multi_processor_count: int
    cc: int
    major: int | None = ...
    regs_per_multiprocessor: int | None = ...
    max_threads_per_multi_processor: int | None = ...
    warp_size: int | None = ...
    @classmethod
    @functools.cache
    def create(cls, device) -> DeviceProperties: ...

class HalideInputSpec(typing.NamedTuple):
    ctype: str
    name: str
    shape: list[str] | None = ...
    stride: list[str] | None = ...
    offset: str | None = ...
    alias_of: str | None = ...
    def bindings_type(self) -> str: ...
    def halide_type(self) -> str: ...
    def is_scalar(self) -> bool: ...
    def is_buffer(self) -> bool: ...

class HalideMeta(typing.NamedTuple):
    argtypes: list[HalideInputSpec]
    target: str
    scheduler: str | None = ...
    scheduler_flags: dict[str, int | str] | None = ...
    cuda_device: int | None = ...
    def args(self) -> list[str]:
        """Command line args to pass to halide generator"""
    def is_cuda(self) -> bool: ...
