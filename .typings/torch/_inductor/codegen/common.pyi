import abc
import contextlib
import dataclasses
import enum
import functools
import itertools
import sympy
import torch
import torch.fx
from .. import config as config, metrics as metrics
from ..custom_graph_pass import CustomGraphModulePass as CustomGraphModulePass
from ..dtype_propagation import DtypePropagationOpsHandler as DtypePropagationOpsHandler
from ..ir import Buffer as Buffer, ChoiceCaller as ChoiceCaller, FixedLayout as FixedLayout, IRNode as IRNode
from ..loop_body import LoopBody as LoopBody
from ..ops_handler import BasicMathOpsMixin as BasicMathOpsMixin, DefaultHandler as DefaultHandler
from ..scheduler import BaseScheduling as BaseScheduling, Scheduler as Scheduler, SchedulerNode as SchedulerNode
from ..utils import DeferredLineBase as DeferredLineBase, IndentedBuffer as IndentedBuffer, ScopedDict as ScopedDict, boolean_ops as boolean_ops, generate_assert as generate_assert, get_current_backend as get_current_backend, ir_dataclass as ir_dataclass, sympy_dot as sympy_dot, sympy_index_symbol as sympy_index_symbol, sympy_subs as sympy_subs, triton_type as triton_type, unique as unique
from ..virtualized import OpsHandler as OpsHandler, OpsValue as OpsValue, ReductionType as ReductionType, StoreMode as StoreMode, V as V, ops as ops
from .wrapper import PythonWrapperCodegen as PythonWrapperCodegen
from _typeshed import Incomplete
from abc import ABC, abstractmethod
from collections.abc import Iterator, MutableMapping, Sequence
from enum import Enum
from torch._prims_common import ELEMENTWISE_TYPE_PROMOTION_KIND as ELEMENTWISE_TYPE_PROMOTION_KIND
from torch.fx import GraphModule as GraphModule
from torch.utils._ordered_set import OrderedSet as OrderedSet
from torch.utils._sympy.numbers import int_oo as int_oo
from torch.utils._sympy.printers import PythonPrinter as _PythonPrinter
from torch.utils._sympy.symbol import SymT as SymT, free_symbol_is_type as free_symbol_is_type, symbol_is_type as symbol_is_type
from torch.utils._sympy.value_ranges import ValueRanges as ValueRanges, bound_sympy as bound_sympy
from typing import Any, Callable, ClassVar, Generic, NamedTuple
from typing_extensions import Self, TypeVar

_T = TypeVar('_T')
SchedulingConstructor = Callable[[Scheduler | None], BaseScheduling]
WrapperConstructor = type[PythonWrapperCodegen]
SymbolLike: Incomplete
OpVarT = str
schedule_log: Incomplete
log: Incomplete

def data_type_logger(msg: str) -> None: ...

@dataclasses.dataclass
class FileBackedGraphModule:
    """
    Output of FX wrapper codegen. Exposes the same methods as ModuleType, but these
    map back to a GraphModule instead of Python source.
    """
    gm: GraphModule
    compiled_fn: Callable[..., Any]
    tempfile = ...
    def __post_init__(self) -> None: ...
    @property
    def __file__(self) -> str: ...
    def call(self, args: list[Any]) -> Any: ...
    @property
    def value(self) -> str: ...

class WorkspaceZeroMode(enum.Enum):
    UNINITIALIZED = 0
    ZERO_ON_CALL = 1
    ZERO_PER_GRAPH = 2
    @staticmethod
    def combine(a: WorkspaceZeroMode, b: WorkspaceZeroMode) -> WorkspaceZeroMode: ...
    @staticmethod
    def from_bool(zero_fill: bool) -> WorkspaceZeroMode: ...

class CodegenSymbol(ABC, metaclass=abc.ABCMeta):
    """
    An IR object possibly corresponding to a variable in the wrapper code.
    """
    @abstractmethod
    def get_name(self) -> str: ...
    @abstractmethod
    def get_example(self) -> torch.Tensor | sympy.Symbol: ...

@ir_dataclass(frozen=True)
class WorkspaceArg(CodegenSymbol):
    """A temporary buffer used for a single kernel, then discarded.

    Not registered as a traditional buffer since there are no users,
    so it would be dead code eliminated.

    Args:
        nbytes: The size of the buffer in bytes.
        zero_fill: Whether the buffer should be initialized to zero.

    """
    count: sympy.Expr
    zero_mode: WorkspaceZeroMode
    device: torch.device
    outer_name: str
    inner_name: str = ...
    dtype: torch.dtype = ...
    @staticmethod
    def unique_name(prefix: str = 'workspace_') -> str: ...
    @staticmethod
    def can_join(a: WorkspaceArg, b: WorkspaceArg) -> bool: ...
    @staticmethod
    def join(a: WorkspaceArg, b: WorkspaceArg) -> WorkspaceArg: ...
    @staticmethod
    def maximum(a: WorkspaceArg, b: WorkspaceArg) -> WorkspaceArg: ...
    def get_device(self) -> torch.device: ...
    get_device_or_error = get_device
    def get_dtype(self) -> torch.dtype: ...
    def get_example(self) -> torch.Tensor | sympy.Symbol: ...
    def get_layout(self) -> FixedLayout: ...
    @property
    def layout(self) -> FixedLayout: ...
    get_output_spec = get_layout
    maybe_get_output_spec = get_layout
    maybe_get_layout = get_layout
    def get_offset(self) -> sympy.Expr: ...
    def get_size(self) -> list[sympy.Expr]: ...
    def get_stride(self) -> list[sympy.Expr]: ...
    def get_name(self) -> str: ...
    def get_inputs_that_alias_output(self) -> list[str]: ...

class TritonScratchWorkspace:
    size: Incomplete
    _generate_dtype_str: Incomplete
    def __init__(self, size: int, generate_dtype_str: Callable[..., str]) -> None: ...
    def generate_dtype_str(self) -> str: ...

@dataclasses.dataclass
class TensorArg:
    name: str
    buffer: str
    dtype: torch.dtype
    offset: sympy.Expr = ...
    alias_of: str | None = ...

@dataclasses.dataclass
class SizeArg:
    name: str
    expr: sympy.Expr
    @property
    def alias_of(self) -> str | None: ...

@dataclasses.dataclass
class ConstexprArg:
    name: str

@dataclasses.dataclass
class TMADescriptorArg:
    name: str
    api_type: str
    block_shape: list[sympy.Expr] | None
    dtype: torch.dtype | None

@dataclasses.dataclass
class DeviceCodegen:
    scheduling: SchedulingConstructor
    wrapper_codegen: WrapperConstructor
    cpp_wrapper_codegen: WrapperConstructor | None = ...
KernelArgType = WorkspaceArg | TensorArg | SizeArg | TMADescriptorArg | ConstexprArg
device_codegens: dict[str, DeviceCodegen]

class DeviceOpOverrides:
    def import_get_raw_stream_as(self, name: str) -> str: ...
    def set_device(self, device_idx: int) -> str: ...
    def synchronize(self) -> str: ...
    def device_guard(self, device_idx: int) -> str: ...
    def cpp_device_guard(self) -> str: ...
    def cpp_aoti_device_guard(self) -> str: ...
    def cpp_stream_guard(self) -> str: ...
    def cpp_aoti_stream_guard(self) -> str: ...
    def cpp_getStreamFromExternal(self) -> str: ...
    def kernel_header(self) -> str: ...
    def kernel_driver(self) -> str: ...
    def cpp_stream_type(self) -> str: ...
    def aoti_get_stream(self) -> str: ...
    def cpp_kernel_type(self) -> str: ...
    def cpp_device_ptr(self) -> str: ...
    def tma_descriptor_helpers(self) -> str: ...
    def cpp_global_scratch(self, idx: int, workspace: TritonScratchWorkspace) -> tuple[list[str], str] | None: ...

device_op_overrides_dict: dict[str, DeviceOpOverrides]
custom_backend_passes: dict[str, CustomGraphModulePass | None]

def register_backend_for_device(device: str, device_scheduling: SchedulingConstructor, device_wrapper_codegen: WrapperConstructor, device_cpp_wrapper_codegen: WrapperConstructor | None = None, device_custom_pass: CustomGraphModulePass | None = None) -> None: ...

class BackendFeature(Enum):
    FOREACH = ...
    BUCKETIZE = ...
    INPLACE_BUFFERS = ...
    MASKED_SCATTER_WITH_INDEX = ...
    SCAN = ...
    SORT = ...
    TUPLE_REDUCTION = ...
    PREFER_STORE_LOOP_ORDER = ...
    TRITON_TEMPLATES = ...
    REDUCE_TO_SINGLE_ELEMENT = ...

def get_backend_features(device: torch.device | str | None) -> OrderedSet[BackendFeature]: ...
def has_backend_feature(device: torch.device | str | None, feature: BackendFeature) -> bool:
    """See also V.graph.has_feature"""
def get_scheduling_for_device(device: str) -> SchedulingConstructor | None: ...
def get_wrapper_codegen_for_device(device: str, cpp_wrapper: bool = False) -> WrapperConstructor | None: ...
def get_custom_backend_pass_for_device(device: str) -> CustomGraphModulePass | None: ...
@functools.cache
def init_backend_registration() -> None: ...
def index_prevent_reordering(index: Sequence[sympy.Expr], index_vars: Sequence[sympy.Expr], sizes: Sequence[sympy.Expr]) -> list[sympy.Expr]: ...
def register_device_op_overrides(device: str, device_op_overrides: DeviceOpOverrides) -> None: ...
def get_device_op_overrides(device: str) -> DeviceOpOverrides: ...

DTYPE_TO_COMPUTATION_DTYPE: dict[torch.dtype, torch.dtype]

def deduce_output_dtype_by_name(op_name: str, *args: Any, **kwargs: Any) -> torch.dtype | None:
    """
    Given op name and a list of input dtypes, deduce the output dtype
    """
def check_dtype(buffer: IndentedBuffer, var: CSEVariableType, dtype: torch.dtype) -> None: ...

class DataTypePropagation:
    body: Incomplete
    graphs: dict[Callable[..., Any] | str, Any]
    def __init__(self, body: LoopBody) -> None: ...
    def deduce_node_dtype_by_inputs(self, node: torch.fx.Node) -> torch.dtype | None: ...
    def deduce_node_dtype_by_subgraph(self, node: torch.fx.Node) -> torch.dtype: ...
    def deduce_node_dtype(self, node: torch.fx.Node) -> torch.dtype | None: ...
    def propagate_graph(self, graph: torch.fx.Graph) -> torch.dtype | None: ...
    def propagate(self) -> torch.dtype | None: ...
    @classmethod
    def propagate_loopbody(cls, body: LoopBody) -> torch.dtype | None: ...
    @classmethod
    def propagate_scheduler_node(cls, node: SchedulerNode) -> torch.dtype | None: ...

class PythonPrinter(_PythonPrinter):
    def doprint(self, expr: sympy.Expr, *, simplify: bool = True, p: bool = True) -> str: ...

class OpDecompositions:
    """
    Decomposes inductor ops
    """
    @staticmethod
    def identity(value: OpVarT) -> OpVarT: ...
    @staticmethod
    def reciprocal(x: OpVarT) -> OpVarT: ...
    @staticmethod
    def square(x: OpVarT) -> OpVarT: ...
    @staticmethod
    def erfc(x: OpVarT) -> OpVarT: ...
    @staticmethod
    def erfcx(x: OpVarT) -> OpVarT: ...
    @staticmethod
    def expm1(x: OpVarT) -> OpVarT: ...
    @staticmethod
    def log10(x: OpVarT) -> OpVarT: ...
    @staticmethod
    def log2(x: OpVarT) -> OpVarT: ...
    @staticmethod
    def exp2(x: OpVarT) -> OpVarT: ...
    @staticmethod
    def log1p(x: OpVarT) -> OpVarT: ...
    @staticmethod
    def sigmoid(x: OpVarT) -> OpVarT: ...
    @staticmethod
    def relu(x: OpVarT) -> OpVarT: ...
    @staticmethod
    def fma(x: OpVarT, y: OpVarT, z: OpVarT) -> OpVarT: ...
    @staticmethod
    def floor_to_int(a: OpVarT, dtype: torch.dtype) -> OpVarT: ...
    @staticmethod
    def ceil_to_int(a: OpVarT, dtype: torch.dtype) -> OpVarT: ...
    @staticmethod
    def trunc_to_int(a: OpVarT, dtype: torch.dtype) -> OpVarT: ...
    @staticmethod
    def remainder(a: OpVarT, b: OpVarT) -> OpVarT: ...
    @staticmethod
    def round_to_int(a: OpVarT, dtype: torch.dtype) -> OpVarT: ...

_RE_PAREN_NOT_NEEDED: Incomplete

def _all_in_parens(string: str) -> bool: ...

class OpOverrides(BasicMathOpsMixin, OpDecompositions, OpsHandler[Any]):
    @staticmethod
    def paren(string: OpVarT) -> OpVarT: ...
    @staticmethod
    def constant(value: bool | float | int, dtype: torch.dtype) -> OpVarT: ...
    @staticmethod
    def bitwise_not(x: OpVarT) -> OpVarT: ...
    @staticmethod
    def logical_not(a: OpVarT) -> OpVarT: ...
    @staticmethod
    def bitwise_and(x: OpVarT, y: OpVarT) -> OpVarT: ...
    @staticmethod
    def bitwise_or(x: OpVarT, y: OpVarT) -> OpVarT: ...
    @staticmethod
    def bitwise_xor(x: OpVarT, y: OpVarT) -> OpVarT: ...
    @staticmethod
    def bitwise_left_shift(x: OpVarT, y: OpVarT) -> OpVarT: ...
    @staticmethod
    def bitwise_right_shift(x: OpVarT, y: OpVarT) -> OpVarT: ...
    @staticmethod
    def int_truediv(a: OpVarT, b: OpVarT) -> OpVarT: ...
    @staticmethod
    def load_seed(name: str, offset: OpVarT) -> OpVarT: ...
    def indirect_indexing(self, var: OpVarT, size: sympy.Expr | int, check: bool = True, wrap_neg: bool = True) -> sympy.Symbol: ...
    def check_bounds(self, expr: sympy.Expr, size: sympy.Expr, lower: bool, upper: bool) -> None: ...
    def load(self, name: str, index: sympy.Expr) -> OpVarT: ...
    def store(self, name: str, index: sympy.Expr, value: OpVarT, mode: StoreMode = None) -> None: ...
    def store_reduction(self, name: str, index: sympy.Expr, value: OpVarT) -> None: ...
    def reduction(self, dtype: torch.dtype, src_dtype: torch.dtype, reduction_type: ReductionType, value: OpVarT | tuple[OpVarT, ...]) -> OpVarT | tuple[OpVarT, ...]: ...
    def scan(self, dtypes: tuple[torch.dtype, ...], combine_fn: Callable[[tuple[OpVarT, ...], tuple[OpVarT, ...]], tuple[OpVarT, ...]], values: tuple[OpVarT, ...]) -> tuple[OpVarT, ...]: ...
    def sort(self, dtypes: tuple[torch.dtype, ...], values: tuple[OpVarT, ...], stable: bool, descending: bool) -> tuple[OpVarT, ...]: ...
    def bucketize(self, values: OpVarT, boundaries: tuple[str, sympy.Expr, sympy.Expr, sympy.Expr], boundary_indices: OpVarT, indexing_dtype: torch.dtype, right: bool, sorter: tuple[str, sympy.Expr] | None = None, sorter_indices: OpVarT | None = None) -> OpVarT: ...
    def halide_clamp(self, value: OpVarT, size: sympy.Expr, check: bool) -> OpVarT: ...
    def inline_asm_elementwise(self, *inputs: OpVarT, asm: str, constraints: str | None = None, dtype: torch.dtype = ..., is_pure: bool = True, pack: int = 1) -> OpVarT: ...
    def output(self, *args: OpVarT) -> None: ...
    def placeholder(self, index: int) -> OpVarT: ...
    @staticmethod
    def _unimplemented(name: str) -> Callable[..., OpVarT]: ...
    @classmethod
    def _is_unimplemented(cls, name: str) -> bool: ...
    @classmethod
    def _initialize_pointwise_overrides(cls, target: str) -> None: ...

@dataclasses.dataclass
class OverridesData:
    name: str
    cpp: Callable[..., str]
    triton: Callable[..., str] | None = ...
    cppvec: Callable[..., str] | None = ...
    type_promotion_kind: ELEMENTWISE_TYPE_PROMOTION_KIND = ...
    halide: Callable[..., str] | None = ...
    mps: Callable[..., str] | None = ...

pointwise_overrides_data: dict[str, OverridesData]

def is_buffer_removed(name: str) -> bool: ...

class DeferredLine(DeferredLineBase):
    """A line that can be 'unwritten' by adding name to V.graph.removed_buffers"""
    name: Incomplete
    def __init__(self, name: str, line: str) -> None: ...
    def __call__(self) -> str | None: ...
    def _new_line(self, line: str) -> DeferredLine: ...

class BracesBuffer(IndentedBuffer):
    def indent(self, offset: int = 1) -> contextlib.AbstractContextManager[None]: ...

class InplacedBuffer(NamedTuple):
    inner_name: str
    other_names: list[str]

@dataclasses.dataclass
class ArgName:
    name: str
    is_constexpr: bool = ...
    def full_name(self) -> str: ...

class RemovedArg:
    def __str__(self) -> str: ...

REMOVED: Incomplete

class KernelArgs:
    @staticmethod
    def _lookup(prefix: str, odict: dict[_T, str | RemovedArg] | dict[_T, str], name: _T) -> str: ...
    input_buffers: dict[str, str]
    output_buffers: dict[str, str | RemovedArg]
    inplace_buffers: dict[str, InplacedBuffer | RemovedArg]
    sizevars: dict[sympy.Expr, str]
    workspace_args: list[WorkspaceArg]
    def __init__(self) -> None: ...
    def __repr__(self) -> str: ...
    @staticmethod
    def _buffer_is_marked_removed(name: Any) -> bool: ...
    def input(self, name: str) -> str: ...
    def output(self, name: str) -> str: ...
    def make_inplace(self, input_name: str, output_name: str) -> None: ...
    def workspace(self, nbytes: sympy.Expr, zero_fill: bool) -> tuple[str, int]:
        '''
        Allocate or extend a workspace buffer of nbytes bytes.

        This function manages the allocation of a workspace buffer. It either creates
        a new WorkspaceArg or extends an existing one.

        Note:
        - Calling this function will in-place mutate the args by adding or updating
        a WorkspaceArg.
        - The codegen for generating the Python argdefs and call_defs will check
        this field and allocate the buffer accordingly.
        - A new argument "ws_ptr" will be present in the generated code.

        Args:
            nbytes (sympy.Expr): The number of bytes to allocate.
            zero_fill (bool): Whether to initialize the buffer to zero.

        Returns:
            Tuple[str, int]: A tuple containing:
                - "ws_ptr": A string identifier for the workspace pointer.
                - offset: An integer representing the byte offset in the workspace.
        '''
    def semaphores(self, min_size: sympy.Expr) -> str:
        """
        Lazily allocate a graph-wide semaphores buffer with at least min_size.  This is a single buffer shared by
        all kernels and zero initialized once at graph start.  Each kernel must leave the buffer zeroed on exit.

        Warning: multiple calls to this function will return the same buffer.

        Args:
            min_size: the number of int32 semaphores required

        Returns:
            name of the semaphores buffer
        """
    def seed_offset(self, name: str, value: int) -> str: ...
    def size(self, name: sympy.Symbol) -> str: ...
    def call_names(self) -> Iterator[str]: ...
    def arg_name(self, name: str) -> str | None:
        """
        Returns inner name of a given outer name.
        """
    def wrap_ptr_arg(self, buf: str, dtype: torch.dtype) -> str: ...
    def wrap_size_arg(self, size: SymbolLike) -> str: ...
    def cpp_argdefs(self, dtype_to_cpp_type: dict[torch.dtype, str] | None = None) -> tuple[list[str], list[str], list[str]]: ...
    def python_argdefs(self) -> tuple[list[ArgName], list[str], list[KernelArgType], list[Any]]: ...
    def aliases(self) -> Iterator[tuple[str, str]]: ...
    def is_removed(self, name: str) -> bool: ...
    def live_output_buffers(self) -> OrderedSet[str]: ...

class CSEVariable:
    '''A CSEVariable is just a name for an expression but it is useful to be able to annotate them on a backend dependent basis.
    To do so, the backends can simply overload `Kernel.create_cse_var`
    The "CSEVariable.update_on_args" method gives you a hook for annotations
    See example of TritonCSEVariable in triton.py
    '''
    name: Incomplete
    bounds: Incomplete
    use_count: int
    dtype: Incomplete
    def __init__(self, name: str, bounds: ValueRanges[Any], dtype: torch.dtype | None = None) -> None: ...
    def __str__(self) -> str: ...
    def __hash__(self) -> int: ...
    def __eq__(self, other: object) -> bool: ...
    def update_on_args(self, name: str, args: Any, kwargs: Any) -> None: ...
    def __repr__(self) -> str: ...
AugmentedKeyT = TypeVar('AugmentedKeyT', default=str)
CSEVariableType = TypeVar('CSEVariableType', bound=CSEVariable, default=CSEVariable)
ReductionCacheKey = tuple[torch.dtype, ReductionType, CSEVariable | tuple[CSEVariable, ...]]

class CSE(Generic[CSEVariableType, AugmentedKeyT]):
    """Common subexpression elimination"""
    prefix: Incomplete
    suffix: Incomplete
    _cache: MutableMapping[AugmentedKeyT, CSEVariableType]
    name_prefix: Incomplete
    store_cache: MutableMapping[str, CSEVariableType]
    reduction_cache: MutableMapping[ReductionCacheKey, CSEVariableType]
    iter_buffer_ids: itertools.count[int]
    invalidated_stores: OrderedSet[str]
    varname_map: dict[str, CSEVariableType]
    def __init__(self, prefix: str = '', suffix: str = '', name_prefix: str = 'tmp', iter_buffers: itertools.count[int] | None = None, store_cache: MutableMapping[str, CSEVariableType] | None = None, reduction_cache: MutableMapping[ReductionCacheKey, CSEVariableType] | None = None, varname_map: dict[str, CSEVariableType] | None = None) -> None: ...
    def invalidate(self, keep_vars: OrderedSet[CSEVariable]) -> None: ...
    def clone(self) -> Self: ...
    def scoped_copy(self) -> Self:
        """Return a copy of using ScopedDict so changes to *_cache aren't visible in self"""
    def augment_key(self, cache_key: str) -> AugmentedKeyT:
        """Override this method to augment cache key with backend specifics"""
    def put(self, cache_key: str, val: CSEVariableType) -> None: ...
    def contains(self, cache_key: str) -> bool: ...
    def try_get(self, cache_key: str) -> CSEVariableType | None: ...
    def get(self, cache_key: str) -> CSEVariableType: ...
    def generate(self, buffer: IndentedBuffer, expr: str | CSEVariable | OpsValue | IndentedBuffer | DeferredLineBase, *, bounds: ValueRanges[Any] = ..., write: bool = True, assignment: bool = True, dtype: torch.dtype | None = None) -> CSEVariableType: ...
    def newvar(self, bounds: ValueRanges[Any] = ..., dtype: torch.dtype | None = None) -> CSEVariableType: ...
    def namedvar(self, name: str, bounds: ValueRanges[Any] = ..., dtype: torch.dtype | None = None) -> CSEVariableType: ...

class CodeGen:
    exit_stack: Incomplete
    def __init__(self) -> None: ...
    def __enter__(self) -> Self: ...
    def __exit__(self, exc_type: Any, exc_val: Any, exc_tb: Any) -> None: ...

class Kernel(CodeGen, Generic[CSEVariableType]):
    newvar_prefix: str
    suffix: str
    overrides: Callable[[], OpsHandler[Any]] | None
    args: Incomplete
    loads: Incomplete
    compute: Incomplete
    stores: Incomplete
    num_load: int
    num_reduction: int
    cse: CSE[CSEVariableType, Any]
    must_keep_buffers: OrderedSet[str]
    store_buffer_names: OrderedSet[str]
    _load_mask: str | None
    _load_other: None | int | float
    current_node: SchedulerNode | None
    node_to_bounds: dict[torch.fx.Node, ValueRanges[Any]] | None
    removed_buffers: OrderedSet[str]
    inplaced_to_remove: OrderedSet[str]
    inplace_update_buffers: dict[str, str]
    min_elem_per_thread: int
    kernel_name: str | None
    def __init__(self, args: KernelArgs | None = None, increase_kernel_count: bool = True) -> None: ...
    @contextlib.contextmanager
    def set_current_node(self, node: SchedulerNode) -> Iterator[None]: ...
    @contextlib.contextmanager
    def swap_buffers(self, lb: IndentedBuffer, cb: IndentedBuffer | None = None, sb: IndentedBuffer | None = None) -> Iterator[None]: ...
    def load(self, name: str, index: sympy.Expr) -> CSEVariable: ...
    def indirect_load(self, name: str, index: sympy.Expr) -> CSEVariable:
        """A load the depends on an index we have read"""
    def store_reduction(self, name: str, index: sympy.Expr, value: CSEVariable) -> None: ...
    def store(self, name: str, index: sympy.Expr, value: CSEVariable, mode: StoreMode = None) -> None: ...
    def reduction(self, dtype: torch.dtype, src_dtype: torch.dtype, reduction_type: ReductionType, value: CSEVariable | tuple[CSEVariable, ...]) -> CSEVariable | tuple[CSEVariable, ...]: ...
    def scan(self, dtypes: tuple[torch.dtype, ...], combine_fn: Callable[[tuple[CSEVariable, ...], tuple[CSEVariable, ...]], tuple[CSEVariable, ...]], values: tuple[CSEVariable, ...]) -> tuple[CSEVariable, ...]: ...
    def sort(self, dtypes: tuple[torch.dtype, ...], values: tuple[CSEVariable, ...], stable: bool, descending: bool) -> tuple[CSEVariable, ...]: ...
    def var_ranges(self) -> dict[sympy.Symbol, sympy.Expr]: ...
    def bucketize(self, values: CSEVariable, boundaries: tuple[str, sympy.Expr, sympy.Expr, sympy.Expr], boundary_indices: CSEVariable, indexing_dtype: torch.dtype, right: bool, sorter: tuple[str, sympy.Expr] | None = None, sorter_indices: CSEVariable | None = None) -> CSEVariable:
        """
        See [Note: Inductor bucketize op]
        """
    @property
    def assert_function(self) -> str: ...
    def indirect_assert(self, var: CSEVariable | str, lower: str | None, upper: str | None, mask: CSEVariable | str | None = None) -> str: ...
    def check_bounds(self, expr: sympy.Expr, size: sympy.Expr, lower: bool, upper: bool) -> None: ...
    def index_to_str(self, index: sympy.Expr) -> str: ...
    def __enter__(self) -> Self: ...
    def __exit__(self, exc_type: Any, exc_val: Any, exc_tb: Any) -> None: ...
    def remove_kernel_local_buffers(self) -> None:
        """
        Any buffers that are both created and have a last use in the
        same kernel can be removed.

        Note that V.graph.scheduler can be None when codegening triton template
        kernels.
        """
    def remove_buffer(self, name: str) -> None: ...
    def remove_inplace_buffer(self, name: str) -> None: ...
    def rename_indexing(self, index: list[sympy.Expr] | tuple[sympy.Expr, ...] | sympy.Expr) -> sympy.Expr: ...
    def create_cse_var(self, *args: Any, **kwargs: Any) -> CSEVariable: ...
    def arg_name(self, node: IRNode) -> str | None:
        """
        Returns arg name of a given input or output node.
        """

@dataclasses.dataclass
class OptimizationContext:
    key: ClassVar[str] = ...
    dtype: torch.dtype | None = ...
    ops_name: str = ...

@functools.cache
def jinja2_env() -> Any: ...

class KernelTemplate:
    """
    Base class for defining kernel templates.

    Children classes: TritonTemplate, CUDATemplate
    """
    @staticmethod
    def indent_except_first(source: str, num_indents: int, indents_spacing: int = 4) -> str: ...
    original_error: Incomplete
    @staticmethod
    def _template_from_string(source: str) -> Any: ...
    @staticmethod
    def _fake_get_dtype(fake_outs: list[Buffer] | Buffer) -> Callable[[str], torch.dtype]: ...
    name: Incomplete
    def __init__(self, name: str) -> None: ...
    def maybe_append_choice(self, choices: list[Any], **kwargs: Any) -> NotImplementedError | None:
        """
        Maybe generates a new ChoiceCaller and appends it into existing choices.
        Returns None if success, otherwise returns the error.

        choices: A list of ChoiceCallers.
        kwargs: Additional kwargs to be passed to self.generate() to generate a new ChoiceCaller.
        """
    def generate(self, **kwargs: Any) -> ChoiceCaller:
        """
        Generates a ChoiceCaller instance from the given arguments.
        """

class CSEProxy(DefaultHandler):
    name: str
    vr_analysis: Incomplete
    kernel: Incomplete
    parent_handler: Incomplete
    def __init__(self, kernel: Kernel[Any], parent_handler: OpsHandler[Any]) -> None: ...
    def _default(self, name: str, args: tuple[Any, ...], kwargs: dict[str, Any]) -> Any: ...
    def _bound_variable(self, name: str, *args: Any, **kwargs: Any) -> ValueRanges[Any]:
        """
        If the variable comes from an FX node, we forward the bound we have already computed
        Else, if the variable when codegen'ing another op, we try to compute its bounds
        """
    def indirect_indexing(self, var: CSEVariable, size: sympy.Expr | int, check: bool = True, wrap_neg: bool = True) -> sympy.Symbol: ...
    def check_bounds(self, expr: sympy.Expr, size: sympy.Expr, lower: bool, upper: bool) -> None: ...
    def load(self, name: str, index: sympy.Expr) -> CSEVariable: ...
    def _update_store_cache(self, name: str, value: CSEVariable) -> None: ...
    def store(self, name: str, index: sympy.Expr, value: CSEVariable, mode: StoreMode = None) -> None: ...
    def store_reduction(self, name: str, index: sympy.Expr, value: CSEVariable) -> None: ...
    def reduction(self, dtype: torch.dtype, src_dtype: torch.dtype, reduction_type: ReductionType, value: CSEVariable | tuple[CSEVariable, ...]) -> CSEVariable | tuple[CSEVariable, ...]: ...
    def scan(self, dtypes: tuple[torch.dtype, ...], combine_fn: Callable[[tuple[CSEVariable, ...], tuple[CSEVariable, ...]], tuple[CSEVariable, ...]], values: tuple[CSEVariable, ...]) -> tuple[CSEVariable, ...]: ...
    def sort(self, dtypes: tuple[torch.dtype, ...], values: tuple[CSEVariable, ...], stable: bool, descending: bool) -> tuple[CSEVariable, ...]: ...
    def bucketize(self, values: CSEVariable, boundaries: tuple[str, sympy.Expr, sympy.Expr, sympy.Expr], boundary_indices: CSEVariable, indexing_dtype: torch.dtype, right: bool, sorter: tuple[str, sympy.Expr] | None = None, sorter_indices: CSEVariable | None = None) -> CSEVariable:
        '''
        [Note: Inductor bucketize op]

        Inputs:
        -------
        values: the values to be bucketized.
        boundaries: a tuple containing
          (a) the name of the boundaries tensor (which must be sorted, unless
          the sorting tensor is present),
          (b) the length of the tensor in the last dimension (i.e. the length of
          one set of boundaries),
          (c) the number of elements in the underlying storage (i.e. the length
          of the flattened tensor, ignoring striding), and
          (d) the stride of the tensor in the last dimension.
        boundary_indices: indices into a flattened version of the boundaries
        tensor, of the same size and shape as "values".  Each index points to
        the first element in the set of boundaries to be used for the
        corresponding value.
        indexing_dtype: the dtype to use when indexing into the boundaries
        tensor.  This must be int64 or int32.  This additionally specifies the
        dtype of the return value.
        right: see "Details" below.
        sorter: an optional tuple containing
          (a) the name of an optional sorting tensor, used to access unsorted
          boundaries without reordering the boundaries tensor, and
          (b) the stride of the tensor in the last dimension.
        The values in the sorting tensor are used as indices into the *last*
        dimension of the boundaries tensor, with all other indices matching.
        The size of the sorting and boundaries tensors must be equivalent.
        sorter_indices: must be present if the sorting array is present; see
        "boundary_indices" for the equivalent definition for the boundaries
        tensor.

        Output:
        -------
        The buckets each value belongs in, within a given set of boundaries.  0
        indicates a position before the first boundary, and len(boundaries_set)
        represents a position after the last boundary.

        Details:
        --------
        Given a value and a set of boundaries, calculate the bucket that each
        value belongs to.  This works differently in 1-D and N-D cases.

        for values [[-1, 0, 1, 2], [3, 4, 5, 9]], boundaries [0, 4, 4, 8], right=True
        return =   [[ 0, 1, 1, 1], [1, 3, 3, 4]].

        for values [[-1, 0, 1, 2], [3, 4, 5, 9]], boundaries [[0, 4], [4, 8]], right=True
        return =   [[ 0, 1, 1, 1], [0, 1, 1, 2]]

        Note that in the N-D boundaries case, the shape of "values" and
        "boundaries" must match in every dimension _except_ the last.

        When right == False, bucket i refers to range (boundaries[i], boundaries[i+1]].
        When right == True,  bucket i refers to range [boundaries[i], boundaries[i+1]).

        Boundaries must be non-decreasing, or a sorter must be provided which
        would re-index offsets in a non-decreasing order (e.g. the second output
        of torch.sort(offsets)).  Otherwise, the result is undefined.
        '''
