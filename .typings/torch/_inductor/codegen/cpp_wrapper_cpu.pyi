import functools
import sympy
import torch
import torch._ops
from .. import config as config, ir as ir
from ..graph import GraphLowering as GraphLowering
from ..utils import DeferredLineBase as DeferredLineBase, LineContext as LineContext, _align as _align, normalize_name as normalize_name
from ..virtualized import V as V
from .aoti_hipify_utils import maybe_hipify_code_wrapper as maybe_hipify_code_wrapper
from .common import IndentedBuffer as IndentedBuffer, Kernel as Kernel, get_device_op_overrides as get_device_op_overrides
from .cpp_utils import DEVICE_TO_ATEN as DEVICE_TO_ATEN, DEVICE_TO_INT as DEVICE_TO_INT, DTYPE_TO_ATEN as DTYPE_TO_ATEN, DTYPE_TO_CPP as DTYPE_TO_CPP, cexpr as cexpr
from .wrapper import EnterSubgraphLine as EnterSubgraphLine, ExitSubgraphLine as ExitSubgraphLine, PythonWrapperCodegen as PythonWrapperCodegen, SymbolicCallArg as SymbolicCallArg
from _typeshed import Incomplete
from collections.abc import Sequence
from torch._inductor.runtime.runtime_utils import dynamo_timed as dynamo_timed
from torch.fx.experimental.symbolic_shapes import ConvertIntKey as ConvertIntKey, DivideByKey as DivideByKey, SymTypes as SymTypes
from torch.utils._ordered_set import OrderedSet as OrderedSet
from torch.utils._sympy.symbol import SymT as SymT, symbol_is_type as symbol_is_type
from typing import Any, Callable, Protocol

_OUTPUT_ARGS_TYPE = list[str | None | list[str | None]]

class HasWriteLine(Protocol):
    def writeline(self, line: LineContext | DeferredLineBase | str) -> None: ...

class CppWrapperCpu(PythonWrapperCodegen):
    """
    Generates cpp wrapper for running on CPU and calls cpp kernels
    """
    device: str
    included_devices: OrderedSet[str]
    declare: str
    declare_maybe_reference: str
    ending: str
    comment: str
    none_str: str
    supports_intermediate_hooks: bool
    kernel_callsite_id: Incomplete
    int_array_id: Incomplete
    declared_int_array_vars: OrderedSet[str]
    tmp_tensor_id: Incomplete
    arg_var_id: Incomplete
    used_cached_devices: OrderedSet[str]
    used_cached_dtypes: OrderedSet[str]
    used_cached_layouts: OrderedSet[str]
    used_cached_memory_formats: OrderedSet[str]
    used_cond_predicate: OrderedSet[str]
    cached_output_id: Incomplete
    scalar_to_tensor_id: Incomplete
    custom_op_wrapper_loaded: bool
    initialized_kernels: dict[str, Kernel]
    device_codegen: Incomplete
    include_extra_header: Incomplete
    def __init__(self) -> None: ...
    @staticmethod
    def create(is_subgraph: bool, subgraph_name: str | None, parent_wrapper: PythonWrapperCodegen | None, partition_signatures: ir.GraphPartitionSignature | None = None): ...
    @staticmethod
    def _generate_temporary_array_pointer(c_type: str, elements: Sequence[str], *, force_mutable: bool = False) -> str:
        """Get a pointer to an array that only exists for the duration of the C++
        statement it's used in."""
    def _generate_kernel_call_helper(self, kernel_name: str, call_args, *, device=None, triton: bool = True, arg_types=None, raw_keys=None, raw_args=None, triton_meta=None, graph_name: str = '', original_fxnode_name=None):
        """
        Generates kernel call code.

        triton: Defines whether the GPU backend uses Triton for codegen.
                Otherwise it uses the CUDA language for codegen.
                Only valid when cuda == True.
        """
    def write_constant(self, name, hashed) -> None: ...
    @staticmethod
    def get_device_include_path(device: str) -> str: ...
    def add_device_include(self, device: str) -> None: ...
    def write_header(self) -> None: ...
    def _include_extra_header(self, header: str): ...
    output_is_tensor: Incomplete
    def mark_output_type(self) -> None: ...
    def write_prefix(self) -> None: ...
    def write_input_output_info(self, info_kind: str, idx: int, name: str): ...
    def codegen_input_symbol_assignment(self, name: str, value: ir.TensorBox, bound_vars: OrderedSet[sympy.Symbol]): ...
    def generate_input_output_runtime_checks(self) -> None:
        """
        In debug_compile mode, we generate checks to ensure the dtype/shape/stride/device of each
        real input/output tensor match ones provided at compile time via sample
        input/output.
        """
    def write_wrapper_decl(self) -> None: ...
    def codegen_tensor_dtype_var_decl(self, code: IndentedBuffer, name): ...
    def codegen_input_size_var_decl(self, code: IndentedBuffer, name): ...
    def codegen_input_stride_var_decl(self, code: IndentedBuffer, name): ...
    def codegen_input_device_type_var_decl(self, code: IndentedBuffer, name): ...
    def codegen_model_kernels(self) -> None: ...
    def codegen_model_constructor(self):
        '''
        // Generated code example
        AOTInductorModel::AOTInductorModel()
            : AOTInductorModelBase(4, 1) {
        inputs_info_[0].name = "input0";
        inputs_info_[0].dtype = "torch.float16";
        ...
        constants_info_[0].name = "L__self___weight";
        constants_info_[0].dtype = at::kFloat;
        constants_info_[0].offset = 0;
        constants_info_[0].data_size = 8192;
        constants_info_[0].shape = {64, 32};
        constants_info_[0].stride = {32, 1};
        ...
        outputs_info_[0].name = "output0";
        outputs_info_[0].dtype = "torch.float16";
        }
        '''
    def codegen_const_run_driver(self) -> None:
        """
        // Generated code example
        std::unordered_map<std::string, AtenTensorHandle> AOTInductorModel::const_run_impl(
            DeviceStreamType stream,
            AOTIProxyExecutorHandle proxy_executor,
            bool initialization
        ) {
            std::unordered_map<std::string, AtenTensorHandle> folded_constants_map;
            std::vector<AtenTensorHandle> output_handles;
            // build up output_handles over here.
            _const_run_impl(output_handles, stream, proxy_executor);
            // build up folded_constants_map
            return folded_constants_map;
        }
        """
    def generate(self, is_inference): ...
    prefix: Incomplete
    def finalize_prefix(self) -> None: ...
    def _define_kernel_helper(self, kernel_name: str, kernel_body: str, metadata: str | None = None, gpu: bool = False, cpp_definition: str | None = None): ...
    def codegen_scalar_to_tensor(self, output: str): ...
    def codegen_tensor_item(self, dtype: torch.dtype, tensor: str, scalar: str, indented_buffer=None): ...
    def generate_return(self, output_refs: list[str]): ...
    def generate_before_suffix(self, result) -> None: ...
    def generate_end(self, result) -> None:
        """Generates the end of the code block, and any code needed to call it."""
    @staticmethod
    def get_c_shim_func_name(kernel: str, device: str) -> str: ...
    def generate_c_shim_extern_kernel_call(self, kernel: str, args: list[str], device: str, *, debug_args: list[str] | None = None) -> None:
        """debug_args kwarg allows CppWrapperCpuArrayRef to pass in wrapped arguments in
        place of args while preserving debug printer output."""
    def generate_c_shim_extern_kernel_alloc(self, extern_kernel: ir.ExternKernelAlloc, args: list[str]) -> None: ...
    def _generate_extern_kernel_alloc_helper(self, extern_kernel, args) -> None: ...
    def generate_c_shim_fallback_kernel(self, fallback_kernel: ir.FallbackKernel, args: list[str]) -> None: ...
    def _generate_extern_kernel_out_helper(self, kernel: str, out: str, out_view: str | None, args: list[str], device: str) -> None: ...
    def generate_scatter_fallback(self, output, inputs, cpp_kernel_name, python_kernel_name, src_is_tensor, reduce, kwargs) -> None: ...
    def generate_index_put_fallback(self, kernel, x, indices, values, accumulate) -> None: ...
    def add_benchmark_harness(self, output) -> None: ...
    def codegen_cpp_sizevar(self, x: sympy.Expr, *, simplify: bool = True) -> str: ...
    def codegen_sizevar(self, x: sympy.Expr) -> str: ...
    def codegen_tuple_access(self, basename: str, name: str, index: str) -> str: ...
    def codegen_shape_tuple(self, shape: Sequence[sympy.Expr]) -> str: ...
    def ensure_size_computed(self, sym: sympy.Symbol): ...
    def _generate_symbolic_call_arg_helper(self, arg: SymbolicCallArg, graph: GraphLowering) -> None: ...
    def codegen_dynamic_scalar(self, node) -> None: ...
    def make_buffer_free(self, buffer): ...
    def make_free_by_names(self, names_to_del: list[str]): ...
    def codegen_exact_buffer_reuse(self, old_name: str, new_name: str, del_line: str): ...
    def generate_profiler_mark_wrapper_call(self, stack) -> None: ...
    def generate_start_graph(self) -> None: ...
    def generate_end_graph(self) -> None: ...
    def generate_inf_and_nan_checker(self, nodes) -> None: ...
    def codegen_device(self, device): ...
    def codegen_dtype(self, dtype): ...
    def codegen_layout(self, layout): ...
    def codegen_memory_format(self, memory_format): ...
    @functools.cache
    def codegen_int_array_var(self, int_array: str, writeline: Callable[..., None], known_statically: bool = False, graph=None): ...
    def make_buffer_allocation(self, buffer): ...
    def make_allocation(self, name, device, dtype, shape, stride, allocation_shape=None): ...
    def codegen_alloc_from_pool(self, name, offset, dtype, shape, stride) -> str: ...
    def codegen_reinterpret_view(self, data, size, stride, offset, writeline: Callable[..., None], dtype=None) -> str:
        """Returns a newly-created, temporary RAII tensor handle containing the
        reinterpreted tensor data.  Callers of this function are responsible for saving
        the handle if persistent access is needed."""
    def codegen_device_copy(self, src, dst, non_blocking: bool):
        """This function is overridden by cpp_wrapper_cpu_array_ref, so we don't need to
        handle cases where dst is not an AtenTensorHandle."""
    def codegen_multi_output(self, node: ir.MultiOutput): ...
    def codegen_subgraph_prefix(self, subgraph, outer_inputs, outer_outputs) -> None: ...
    def codegen_subgraph_suffix(self, subgraph, outer_inputs, outer_outputs) -> None: ...
    def codegen_invoke_subgraph(self, invoke_subgraph) -> None: ...
    def codegen_conditional(self, conditional) -> None: ...
    def codegen_subgraph(self, subgraph, outer_inputs, outer_outputs) -> None: ...
    def codegen_while_loop(self, while_loop) -> None: ...
    def generate_extern_kernel_args_decl_if_needed(self, op_overload: torch._ops.OpOverload | torch._ops.HigherOrderOperator, raw_args: Sequence[Any], output_args: _OUTPUT_ARGS_TYPE, raw_outputs: Sequence[ir.Buffer]):
        """
        Generates declarations for external kernel arguments if needed, based on the provided
        operator and its arguments. It processes both input and output arguments, categorizing
        them into tensor and integer arguments for further code generation.
        """
    @staticmethod
    def _compatible_with_stableivalue(op: torch._ops.OpOverload) -> bool:
        """Returns true if op_overload._schema only utilizes types supported by the AOT
        C-shim *internal* function to_ivalue.  to_ivalue is an implementation detail, so
        these types are not guaranteed to be supported long-term.  When generating code
        for cpp_wrapper mode, we don't have to be forward-compatible, so changing this
        function's implementation in future is fine."""
    def generate_fallback_kernel_with_runtime_lookup(self, buf_name: str, python_kernel_name: str, get_args: Callable[[], Sequence[str]], op_overload: torch._ops.OpOverload | torch._ops.HigherOrderOperator, raw_args: Sequence[Any], outputs: Sequence[ir.Buffer]) -> None:
        """Generate a call to a kernel not contained in the C-shim.  This results in
        different code paths for AOT Inductor vs cpp_wrapper Inductor mode."""
    def generate_scoped_gil_acquire(self, declarations_before_scope, lines_in_scope): ...
    def load_custom_op_wrapper(self) -> None: ...
    def generate_float_value(self, val): ...
    def generate_py_arg(self, py_args_var, idx, raw_arg, arg_type): ...
    def generate_fallback_kernel_with_runtime_lookup_nopython(self, get_args: Callable[[], Sequence[str]], op_overload: torch._ops.OpOverload, output_args: Sequence[str | None], raw_outputs: Sequence[ir.Buffer]) -> None:
        """Generate fallback kernel calls with runtime (non-AOT) dispatch.  This can
        only be called in cpp_wrapper mode, and assumes that the input is a non-None
        OpOverload.

        In the future, we may switch over to directly calling c10::Dispatcher if we need
        to support more datatypes."""
    def generate_fallback_kernel_with_runtime_lookup_python(self, buf_name: str, python_kernel_name: str, op_overload: torch._ops.OpOverload, raw_args: Sequence[Any], output_args: Sequence[str | None], raw_outputs: Sequence[ir.Buffer]) -> None:
        """Generate fallback kernel calls with runtime (non-AOT) dispatch.  This can
        only be called in cpp_wrapper mode, and assumes that the input is a non-None
        OpOverload.

        This function calls into Python to dispatch, which allows it to handle datatypes
        that cannot be contained in StableIValue, at the cost of some performance."""
    def generate_fallback_kernel_with_runtime_lookup_aot(self, op_overload: torch._ops.OpOverload | torch._ops.HigherOrderOperator, raw_args: Sequence[Any], output_args: _OUTPUT_ARGS_TYPE, raw_outputs: Sequence[ir.Buffer]) -> None: ...
    def generate_reset_kernel_saved_flags(self) -> None: ...
    def generate_save_uncompiled_kernels(self) -> None: ...
    def c_type_for_prim_type(self, val, type_) -> str: ...
    def val_to_arg_str_for_prim_type(self, val, type_) -> str: ...
    def val_to_arg_str(self, val, type_=None) -> str: ...
    def create_tmp_raii_handle_var_if_needed(self, handle: str, writer: HasWriteLine | list[str] | None = None) -> str:
        """If the input handle is an rvalue RAII tensor, creates an lvalue variable for
        it in writer.  Returns a variable name that can be used to access handle."""
