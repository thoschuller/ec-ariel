import contextlib
import dataclasses
import sympy
import torch
import torch._ops
import torch.utils._pytree as pytree
import triton
from .. import async_compile as async_compile, config as config, ir as ir
from ..codecache import output_code_log as output_code_log
from ..graph import GraphLowering as GraphLowering
from ..ir import IRNode as IRNode, ReinterpretView as ReinterpretView
from ..runtime import triton_heuristics as triton_heuristics
from ..runtime.hints import DeviceProperties as DeviceProperties
from ..utils import DelayReplaceLine as DelayReplaceLine, IndentedBuffer as IndentedBuffer, LineContext as LineContext, cache_on_self as cache_on_self, get_benchmark_name as get_benchmark_name, is_codegen_graph_partition_subgraph as is_codegen_graph_partition_subgraph, set_kernel_post_grad_provenance_tracing as set_kernel_post_grad_provenance_tracing, sympy_product as sympy_product, sympy_str as sympy_str, sympy_subs as sympy_subs, triton_version_uses_attrs_dict as triton_version_uses_attrs_dict
from ..virtualized import V as V
from .common import ArgName as ArgName, CodeGen as CodeGen, DeferredLine as DeferredLine, PythonPrinter as PythonPrinter, WorkspaceArg as WorkspaceArg, WorkspaceZeroMode as WorkspaceZeroMode
from .cpp_utils import cexpr as cexpr
from .triton_utils import config_of as config_of, should_unwrap_unspec_arg as should_unwrap_unspec_arg, signature_to_meta as signature_to_meta
from .wrapper_fxir import FxConverter as FxConverter
from _typeshed import Incomplete
from collections.abc import Iterator, Sequence
from sympy import Expr
from torch._dynamo.utils import counters as counters, dynamo_timed as dynamo_timed
from torch._inductor.codegen.debug_utils import DebugPrinterManager as DebugPrinterManager
from torch._inductor.codegen.multi_kernel import MultiKernelState as MultiKernelState
from torch._inductor.runtime.runtime_utils import cache_dir as cache_dir
from torch.fx.experimental.symbolic_shapes import CallMethodKey as CallMethodKey, ConvertIntKey as ConvertIntKey, DivideByKey as DivideByKey, SymTypes as SymTypes, resolve_unbacked_bindings as resolve_unbacked_bindings
from torch.fx.node import _get_qualified_name as _get_qualified_name
from torch.utils._ordered_set import OrderedSet as OrderedSet
from torch.utils._sympy.singleton_int import SingletonInt as SingletonInt
from torch.utils._sympy.symbol import SymT as SymT, symbol_is_type as symbol_is_type
from typing import Any, Callable

log: Incomplete
pexpr: Incomplete
ReuseKey = tuple[torch.device, torch.dtype, str, bool]
BufferLike = ir.Buffer | WorkspaceArg
FxConversionFunc: Incomplete

def buffer_reuse_key(node: BufferLike) -> ReuseKey: ...
def can_match_buffer_size(input_buf: BufferLike, output_buf: BufferLike): ...
TritonMetaParams = dict[str, int]
TritonGrid: Incomplete

def user_defined_kernel_grid_fn_code(name: str, configs: list[triton.Config], grids: list[TritonGrid], wrapper: PythonWrapperCodegen | None = None, original_fxnode_name: str | None = None) -> tuple[str, str]: ...
def user_defined_triton_kernel_transitive_closure_source_code(kernel) -> str:
    """
    Given a triton kernel function pointer collect the transitive closure of
    its dependencies
    """

@dataclasses.dataclass
class SymbolicCallArg:
    inner: str
    inner_expr: sympy.Expr
    def __str__(self) -> str: ...

class MemoryPlanningState:
    reuse_pool: dict[ReuseKey, list[FreeIfNotReusedLine]]
    total_allocated_buffer_size: int
    def __init__(self) -> None: ...
    def __contains__(self, key: ReuseKey) -> bool: ...
    def pop(self, key: ReuseKey) -> FreeIfNotReusedLine: ...
    def push(self, key: ReuseKey, item: FreeIfNotReusedLine) -> None: ...

class WrapperLine:
    def codegen_fx(self, converter: FxConverter) -> FxConversionFunc: ...

@dataclasses.dataclass
class EnterSubgraphLine(WrapperLine):
    wrapper: PythonWrapperCodegen
    graph: GraphLowering
    def __post_init__(self) -> None: ...
    def codegen(self, code: IndentedBuffer) -> None: ...
    def codegen_fx(self, converter: FxConverter) -> FxConversionFunc: ...

@dataclasses.dataclass
class CommentLine(WrapperLine):
    line: LineContext
    def codegen(self, code: IndentedBuffer) -> None: ...
    @staticmethod
    def codegen_fx(converter: FxConverter) -> FxConversionFunc: ...

@dataclasses.dataclass
class ExitSubgraphLine(WrapperLine):
    wrapper: PythonWrapperCodegen
    def __post_init__(self) -> None: ...
    def codegen(self, code: IndentedBuffer) -> None: ...
    def codegen_fx(self, converter: FxConverter) -> FxConversionFunc: ...

@dataclasses.dataclass
class EnterDeviceContextManagerLine(WrapperLine):
    device_idx: int
    last_seen_device_guard_index: int | None
    def codegen(self, code: IndentedBuffer) -> None: ...
    def codegen_fx(self, converter: FxConverter) -> FxConversionFunc: ...

class ExitDeviceContextManagerLine(WrapperLine):
    def codegen(self, code: IndentedBuffer) -> None: ...
    def codegen_fx(self, converter: FxConverter) -> FxConversionFunc: ...

@dataclasses.dataclass
class ExternKernelAllocLine(WrapperLine):
    wrapper: PythonWrapperCodegen
    node: ir.ExternKernelAlloc
    def codegen(self, code: IndentedBuffer) -> None: ...
    def codegen_fx(self, converter: FxConverter) -> FxConversionFunc: ...

@dataclasses.dataclass
class ExternKernelOutLine(WrapperLine):
    wrapper: PythonWrapperCodegen
    node: ir.ExternKernelOut
    def codegen(self, code: IndentedBuffer) -> None: ...
    def codegen_fx(self, converter: FxConverter) -> FxConversionFunc: ...

@dataclasses.dataclass
class FreeLine(WrapperLine):
    wrapper: PythonWrapperCodegen
    node: BufferLike | ir.TorchBindObject
    def codegen(self, code: IndentedBuffer) -> None: ...
    def codegen_fx(self, converter: FxConverter) -> FxConversionFunc: ...

@dataclasses.dataclass
class KernelCallLine(WrapperLine):
    wrapper: PythonWrapperCodegen
    kernel_name: str
    call_args: tuple[Any, ...]
    raw_keys: tuple[Any, ...]
    raw_args: tuple[Any, ...]
    arg_types: list[str]
    triton: bool
    triton_meta: dict[str, Any]
    device: torch.device
    graph_name: str
    original_fxnode_name: str
    def codegen(self, code: IndentedBuffer) -> None: ...
    def codegen_fx(self, converter: FxConverter) -> FxConversionFunc: ...

@dataclasses.dataclass
class KernelDefinitionLine(WrapperLine):
    wrapper: PythonWrapperCodegen
    kernel_name: str
    kernel_body: str
    metadata: str | None = ...
    gpu: bool = ...
    cpp_definition: str | None = ...
    def codegen(self, code: IndentedBuffer) -> None: ...
    def codegen_fx(self, converter: FxConverter) -> FxConversionFunc: ...

@dataclasses.dataclass
class MemoryPlanningLine(WrapperLine):
    wrapper: PythonWrapperCodegen
    def plan(self, state: MemoryPlanningState) -> MemoryPlanningLine:
        """First pass to find reuse"""
    def codegen(self, code: IndentedBuffer) -> None:
        """Second pass to output code"""
    def __str__(self) -> str:
        """
        Emits a string representation that fits on one line.
        """

@dataclasses.dataclass
class AllocateLine(MemoryPlanningLine):
    node: BufferLike
    def plan(self, state: MemoryPlanningState) -> MemoryPlanningLine: ...
    def codegen(self, code: IndentedBuffer) -> None: ...
    def codegen_fx(self, converter: FxConverter) -> FxConversionFunc: ...

@dataclasses.dataclass
class FreeIfNotReusedLine(MemoryPlanningLine):
    node: BufferLike
    is_reused: bool = ...
    def plan(self, state: MemoryPlanningState) -> MemoryPlanningLine: ...
    def codegen(self, code: IndentedBuffer) -> None: ...
    def codegen_fx(self, converter: FxConverter) -> FxConversionFunc: ...

@dataclasses.dataclass
class ReinterpretLine(MemoryPlanningLine):
    node: BufferLike
    reused_as: BufferLike
    layout: ir.Layout
    def plan(self, state: MemoryPlanningState) -> MemoryPlanningLine: ...
    def codegen(self, code: IndentedBuffer) -> None: ...
    def codegen_fx(self, converter: FxConverter) -> FxConversionFunc: ...

@dataclasses.dataclass
class ReuseLine(MemoryPlanningLine):
    node: BufferLike
    reused_as: BufferLike
    delete_old: bool = ...
    def plan(self, state: MemoryPlanningState) -> MemoryPlanningLine: ...
    def codegen(self, code: IndentedBuffer) -> None: ...
    def codegen_fx(self, converter: FxConverter) -> FxConversionFunc: ...

class NullLine(MemoryPlanningLine):
    def codegen_fx(self, converter: FxConverter) -> FxConversionFunc: ...

@dataclasses.dataclass
class CommBufferLine(WrapperLine):
    wrapper: PythonWrapperCodegen
    node: ir.Buffer
    @property
    def size(self) -> int: ...
    @property
    def comm_buffer_type(self) -> ir.CommBufferType: ...
    @property
    def group_name(self) -> str: ...

@dataclasses.dataclass
class CommBufferAllocateLine(CommBufferLine):
    def codegen(self, code: IndentedBuffer) -> None: ...
    @staticmethod
    def make_allocation_line(comm_buffer_type, group_name, wrapper, name, device, dtype, shape, stride): ...
    def codegen_fx(self, converter: FxConverter) -> FxConversionFunc: ...

@dataclasses.dataclass
class CommBufferFreeLine(CommBufferLine):
    def codegen(self, code: IndentedBuffer) -> None: ...
    def codegen_fx(self, converter: FxConverter) -> FxConversionFunc: ...

@dataclasses.dataclass
class MultiOutputLine(WrapperLine):
    """
    Given a MultiOutputLayout buffer, indexes actual buffer(s) from the result.
    """
    wrapper: PythonWrapperCodegen
    result_name: str
    arg_name: str
    indices: Sequence[Any]
    def codegen(self, code: IndentedBuffer) -> None: ...
    def codegen_fx(self, converter: FxConverter) -> FxConversionFunc: ...

@dataclasses.dataclass
class SymbolicCallArgLine(WrapperLine):
    wrapper: PythonWrapperCodegen
    arg: SymbolicCallArg
    graph: GraphLowering
    def codegen(self, code: IndentedBuffer) -> None: ...
    def codegen_fx(self, converter: FxConverter) -> FxConversionFunc: ...
BufferName = str
Line = MemoryPlanningLine | LineContext

class PythonWrapperCodegen(CodeGen):
    """
    Generate outer wrapper in Python that calls the kernels.
    """
    supports_caching: bool
    _names_iter: Iterator[int]
    args_to_buffers: dict[str, None | ir.TensorBox | ir.Buffer | ir.TorchBindObject]
    imports: Incomplete
    header: Incomplete
    prefix: Incomplete
    suffix: Incomplete
    kernel_declarations: Incomplete
    wrapper_call: Incomplete
    kernel_autotune_defs: Incomplete
    kernel_autotune_calls: Incomplete
    subgraph_definitions: Incomplete
    kernel_autotune_names: OrderedSet[str]
    kernel_autotune_example_args: dict[str, tuple[str, str]]
    kernel_autotune_tmp_arg_idx: int
    src_to_kernel: dict[str, str]
    kernel_numel_expr: OrderedSet[tuple[str, GraphLowering]]
    lines: list[Line]
    declare: str
    declare_maybe_reference: str
    ending: str
    comment: str
    none_str: str
    move_begin: Incomplete
    move_end: Incomplete
    last_seen_device_guard_index: int | None
    supports_intermediate_hooks: bool
    user_defined_kernel_cache: dict[tuple[Any, ...], tuple[str, Any]]
    unbacked_symbol_decls: OrderedSet[str]
    computed_sizes: OrderedSet[sympy.Symbol]
    launcher_fn_name: Incomplete
    codegened_graph_stack: Incomplete
    computed_sizes_stack: Incomplete
    allocated: Incomplete
    freed: Incomplete
    reuses: dict[BufferName, BufferName]
    add_import_once: Incomplete
    _metas: dict[str, str]
    _meta_vars: OrderedSet[str]
    multi_kernel_state: Incomplete
    already_codegened_subgraphs: OrderedSet[str]
    allocated_workspaces: dict[str, Any]
    debug_printer: Incomplete
    additional_files: Incomplete
    def __init__(self) -> None: ...
    @staticmethod
    def create(is_subgraph: bool, subgraph_name: str | None, parent_wrapper: PythonWrapperCodegen | None, partition_signatures: ir.GraphPartitionSignature | None = None): ...
    def set_launcher_fn_name(self) -> None: ...
    def write_constant(self, name: str, hashed: str) -> None: ...
    def write_header(self) -> None: ...
    def include_extra_header(self, header: str): ...
    def write_kernel_autotune_defs_header(self) -> None: ...
    @cache_on_self
    def write_triton_header_once(self) -> None: ...
    def write_get_raw_stream_header(self) -> None: ...
    @cache_on_self
    def write_get_raw_stream_header_once(self) -> None: ...
    def add_meta_once(self, meta: TritonMetaParams) -> str: ...
    @cache_on_self
    def get_output_refs(self) -> list[str]: ...
    def mark_output_type(self) -> None: ...
    def get_graph_inputs(self) -> dict[str, ir.TensorBox | ir.TorchBindObject | sympy.Expr]: ...
    def get_graph_outputs(self) -> list[IRNode]: ...
    def codegen_input_size_asserts(self) -> None: ...
    def codegen_input_nan_asserts(self) -> None: ...
    def write_async_compile_wait(self) -> None: ...
    def write_args(self, input_names: list[str]): ...
    def write_launcher_fn_call_get_indent(self) -> int: ...
    def get_graph_input_names(self) -> list[str]: ...
    def write_prefix(self) -> None: ...
    def codegen_input_size_and_nan_asserts(self) -> None: ...
    def write_get_raw_stream(self, device_idx: int, graph_name: str) -> str: ...
    def get_codegened_graph(self): ...
    def push_codegened_graph(self, graph) -> None: ...
    def pop_codegened_graph(self): ...
    def push_computed_sizes(self, computed_sizes): ...
    def pop_computed_sizes(self): ...
    def next_kernel_suffix(self) -> str: ...
    def codegen_device_guard_enter(self, device_idx: int) -> None: ...
    def codegen_device_guard_exit(self) -> None: ...
    def generate_return(self, output_refs: list[str]) -> None: ...
    def generate_before_suffix(self, result: IndentedBuffer) -> None: ...
    def generate_after_suffix(self, result: IndentedBuffer) -> None: ...
    def generate_end(self, result: IndentedBuffer) -> None: ...
    def generate_fallback_kernel(self, node: ir.FallbackKernel) -> None: ...
    def generate_extern_kernel_alloc(self, node: ir.ExternKernelAlloc): ...
    def _generate_extern_kernel_alloc_helper(self, extern_kernel, args) -> None: ...
    def generate_extern_kernel_out(self, node: ir.ExternKernelOut) -> None: ...
    def _generate_extern_kernel_out_helper(self, kernel: str, out: str, out_view: str | None, args: list[str], device: str) -> None: ...
    def _generate_tma_descriptor_call_experimental(self, desc, apply_size_hints: bool = False): ...
    def _generate_tma_descriptor_call_stable(self, desc, apply_size_hints: bool = False): ...
    def _generate_tma_descriptor_call(self, desc, apply_size_hints: bool = False): ...
    def generate_tma_descriptor(self, desc) -> None: ...
    def generate_scatter_fallback(self, output, inputs, cpp_kernel_name, python_kernel_name, src_is_tensor, reduce, kwargs) -> None: ...
    def generate_index_put_fallback(self, kernel, x, indices, values, accumulate) -> None: ...
    def generate_fallback_kernel_with_runtime_lookup(self, buf_name: str, python_kernel_name: str, get_args: Callable[[], Sequence[str]], op_overload: torch._ops.OpOverload | torch._ops.HigherOrderOperator, raw_args: Sequence[Any], outputs: Sequence[ir.Buffer]) -> None: ...
    def generate(self, is_inference): ...
    def get_wrapper_call_indent(self) -> int: ...
    @contextlib.contextmanager
    def set_writeline(self, new: Callable[..., None]) -> Iterator[Callable[..., None]]: ...
    def _write_multi_kernel_defs(self) -> None: ...
    def _generate(self, is_inference): ...
    def generate_and_run_autotune_block(self) -> None:
        """
        Compose self.kernel_autotune_defs and self.kernel_autotune_calls into a single block of
        code and execute it to trigger Triton kernel compilation and auto-tuning
        """
    def memory_plan(self) -> None: ...
    def memory_plan_reuse(self) -> None: ...
    def run_wrapper_ir_passes(self, is_inference: bool): ...
    def codegen_input_symbol_assignment(self, name: str, value: ir.TensorBox, bound_vars: OrderedSet[sympy.Symbol]): ...
    def codegen_inputs(self) -> None:
        """Assign all symbolic shapes to locals"""
    def ensure_size_computed(self, sym: sympy.Symbol): ...
    def finalize_prefix(self) -> None: ...
    def codegen_cpp_sizevar(self, x: Expr, *, simplify: bool = True) -> str: ...
    def codegen_python_sizevar(self, x: Expr, *, simplify: bool = True) -> str: ...
    def codegen_sizevar(self, x: Expr) -> str: ...
    def codegen_tuple_access(self, basename: str, name: str, index: str) -> str: ...
    def codegen_python_shape_tuple(self, shape: Sequence[Expr]) -> str: ...
    def codegen_shape_tuple(self, shape: Sequence[Expr]) -> str: ...
    def codegen_alloc_from_pool(self, name, offset, dtype, shape, stride) -> str: ...
    def codegen_reinterpret_view(self, data, size, stride, offset, writeline: Callable[..., None], dtype=None) -> str: ...
    def codegen_device_copy(self, src, dst, non_blocking: bool): ...
    def codegen_multi_output(self, node: ir.MultiOutput): ...
    def codegen_dynamic_scalar(self, node) -> None: ...
    def benchmark_compiled_module(self, output) -> None: ...
    def add_benchmark_harness(self, output) -> None:
        """
        Append a benchmark harness to generated code for debugging
        """
    def define_kernel(self, kernel_name: str, kernel_body: str, metadata: str | None = None, gpu: bool = True, cpp_definition: str | None = None): ...
    @staticmethod
    def _format_kernel_definition(kernel_name: str, kernel_body: str, metadata: str | None = None): ...
    def _define_kernel_helper(self, kernel_name: str, kernel_body: str, metadata: str | None = None, gpu: bool = True, cpp_definition: str | None = None): ...
    def define_subgraph_launcher_fn(self, fn_code: str): ...
    def define_user_defined_triton_kernel(self, kernel, configs, kwargs, restore_value_args, reset_to_zero_args, grids: list[list[int | sympy.Expr]]): ...
    def generate_numel_expr(self, kernel_name: str, tree, suffix: str | None = None): ...
    def _generate_symbolic_call_arg_helper(self, arg: SymbolicCallArg, graph: GraphLowering) -> None: ...
    def generate_workspace_allocation(self, ws: WorkspaceArg): ...
    def generate_workspace_deallocation(self, ws: WorkspaceArg): ...
    def make_zero_buffer(self, name): ...
    def wrap_kernel_call(self, name, call_args): ...
    def generate_profiler_mark_wrapper_call(self, stack) -> None: ...
    def generate_start_graph(self) -> None: ...
    def generate_end_graph(self) -> None: ...
    def generate_reset_kernel_saved_flags(self) -> None: ...
    def generate_save_uncompiled_kernels(self) -> None:
        """
        Precompile and save the CUBINs of the Triton kernels that haven't
        been precompiled and saved as a side effect of running the generated
        JIT model (Python wrapper). This can happen when the model contains
        control flow: only one pass through the control flow operators covers
        the kernels that are saved, the remaining kernels are not launched,
        hence not saved. The main purpose of this codegen is to compile and
        save the Triton kernels outside the active control flow path for
        subsequent AOTInductor code generation and compilation.
        """
    def prepare_triton_kernel_call(self, call_args): ...
    def generate_example_arg_value(self, arg, arg_type, raw_arg=None): ...
    def _grid_dim_str(self, grid_per_dim): ...
    def generate_kernel_call(self, kernel_name: str, call_args, *, device=None, triton: bool = True, arg_types=None, raw_keys=None, raw_args=None, triton_meta=None, original_fxnode_name=None):
        """
        Generates kernel call code.

        triton: Defines whether the backend uses Triton for codegen. Otherwise it uses the CUDA language when gpu=True,
                and C++ when gpu=False.
        """
    def _generate_kernel_call_helper(self, kernel_name: str, call_args, *, device=None, triton: bool = True, arg_types=None, raw_keys=None, raw_args=None, triton_meta=None, graph_name: str = '', original_fxnode_name=None): ...
    def writeline(self, line) -> None: ...
    def writelines(self, lines) -> None: ...
    def enter_context(self, ctx) -> None: ...
    def val_to_arg_str(self, s, type_=None): ...
    def make_buffer_allocation(self, buffer: BufferLike): ...
    def make_allocation(self, name, device, dtype, shape, stride, allocation_shape=None): ...
    def make_comment(self, line) -> None: ...
    def make_tensor_alias(self, new_name, old_name, comment: str = ''): ...
    def make_buffer_free(self, buffer: BufferLike | ir.TorchBindObject): ...
    def make_free_by_names(self, names_to_del: list[str]): ...
    def codegen_exact_buffer_reuse(self, old_name: str, new_name: str, del_line: str): ...
    def make_buffer_reuse(self, old: BufferLike, new: BufferLike, delete_old: bool): ...
    def codegen_deferred_allocation(self, name: str, view: ir.ReinterpretView) -> None: ...
    def codegen_allocation(self, buffer: ir.Buffer): ...
    def codegen_free(self, buffer) -> None: ...
    def can_reuse(self, input_buffer, output_buffer=None): ...
    def did_reuse(self, buffer, reused_buffer): ...
    def codegen_inplace_reuse(self, input_buffer: ir.Buffer, output_buffer: ir.Buffer): ...
    def codegen_unbacked_symbol_decl(self, symbol): ...
    def codegen_unbacked_symbol_defs_for_outputs(self, output_name: str, outputs: Any, unbacked_bindings: dict[sympy.Symbol, pytree.KeyPath] | None) -> None: ...
    def codegen_subgraph_by_inlining(self, subgraph, outer_inputs, outer_outputs) -> None: ...
    def codegen_partition_call(self, partition_id: int, partition_signatures: ir.GraphPartitionSignature):
        """Generate code to call a graph partition"""
    all_partition_names: Incomplete
    def set_all_partition_names(self, num_partitions: int): ...
    def codegen_subgraph_call_with_flattened_outputs(self, subgraph, outer_inputs, outer_flattened_outputs) -> None: ...
    def codegen_subgraph_call(self, subgraph, outer_inputs, outer_buffer_name) -> None: ...
    def codegen_subgraph_common(self, subgraph) -> None: ...
    def codegen_subgraph_with_flattened_outputs(self, subgraph, outer_inputs, outer_flattened_outputs) -> None: ...
    def codegen_subgraph(self, subgraph, outer_inputs, outer_buffer_name) -> None: ...
    def codegen_invoke_subgraph(self, invoke_subgraph) -> None: ...
    def codegen_conditional(self, conditional) -> None: ...
    def codegen_while_loop(self, while_loop) -> None: ...
    @staticmethod
    def statically_known_int_or_none(x): ...
    @staticmethod
    def statically_known_list_of_ints_or_none(lst): ...
    @staticmethod
    def is_statically_known_list_of_ints(lst): ...
    @staticmethod
    def static_shape_for_buffer_or_none(buffer): ...
    @staticmethod
    def can_prove_buffer_has_static_shape(buffer): ...

class SubgraphPythonWrapperCodegen(PythonWrapperCodegen):
    """
    A wrapper codegen that generates code for a subgraph. For most of the
    methods, we rely on the implementation in the PythonWrapperCodegen. But we
    override a few functions to produce cleaner code (like avoiding writing
    imports twice in the output code)
    """
    subgraph_name: Incomplete
    parent_wrapper: Incomplete
    partition_signatures: Incomplete
    def __init__(self, subgraph_name: str, parent_wrapper: PythonWrapperCodegen, partition_signatures: ir.GraphPartitionSignature | None = None) -> None: ...
    launcher_fn_name: Incomplete
    def set_launcher_fn_name(self) -> None: ...
    def write_header(self) -> None: ...
    def add_benchmark_harness(self, output) -> None: ...
    def benchmark_compiled_module(self, output) -> None: ...
    def write_async_compile_wait(self) -> None: ...
    def next_kernel_suffix(self) -> str: ...
    def generate_after_suffix(self, result: IndentedBuffer) -> None: ...
    def write_launcher_fn_call_get_indent(self) -> int: ...
    def get_wrapper_call_indent(self) -> int: ...
    def get_graph_inputs(self) -> dict[str, ir.TensorBox | ir.TorchBindObject | sympy.Expr]: ...
    def get_graph_input_names(self) -> list[str]: ...
    def get_graph_outputs(self) -> list[IRNode]: ...
    def codegen_allocation(self, buffer: ir.Buffer): ...
    @cache_on_self
    def write_triton_header_once(self) -> None: ...
    @cache_on_self
    def write_get_raw_stream_header_once(self) -> None: ...
