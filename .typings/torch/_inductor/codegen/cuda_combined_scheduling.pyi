import torch
from ..scheduler import BaseSchedulerNode as BaseSchedulerNode, BaseScheduling as BaseScheduling, FusedSchedulerNode as FusedSchedulerNode, Scheduler as Scheduler, SchedulerNode as SchedulerNode
from .common import BackendFeature as BackendFeature
from .cuda.cuda_cpp_scheduling import CUDACPPScheduling as CUDACPPScheduling
from .rocm.rocm_cpp_scheduling import ROCmCPPScheduling as ROCmCPPScheduling
from .triton import TritonScheduling as TritonScheduling
from _typeshed import Incomplete
from collections.abc import Sequence
from sympy import Expr
from torch.utils._ordered_set import OrderedSet as OrderedSet
from typing import Any
from typing_extensions import TypeAlias

_IntLike: TypeAlias = int | Expr

class CUDACombinedScheduling(BaseScheduling):
    """
    Scheduler for CUDA Kernels, which delegates calls as appropriate
    to the CUDA-C++ and Triton Schedulers, which both work for CUDA devices
    and use a unified-wrapper for codegen.

    If Scheduling code needs to be specialized for the case of mixed Triton / CUDA C++ code,
    this would also be the place to do it.
    """
    _triton_scheduling: Incomplete
    _cuda_cpp_scheduling: Incomplete
    _rocm_cpp_scheduling: Incomplete
    def __init__(self, scheduler: Scheduler | None) -> None: ...
    def get_backend_features(self, device: torch.device) -> OrderedSet[BackendFeature]: ...
    def choose_node_backend(self, node: BaseSchedulerNode) -> BaseScheduling: ...
    def can_fuse_vertical(self, node1: BaseSchedulerNode, node2: BaseSchedulerNode) -> bool: ...
    def can_fuse_horizontal(self, node1: BaseSchedulerNode, node2: BaseSchedulerNode) -> bool: ...
    def group_fn(self, sizes: Sequence[Sequence[_IntLike]]) -> tuple[tuple[_IntLike, ...], ...]: ...
    def codegen_template(self, template_node: BaseSchedulerNode, epilogue_nodes: Sequence[BaseSchedulerNode], prologue_nodes: Sequence[BaseSchedulerNode]) -> str | None: ...
    def codegen_node(self, node: FusedSchedulerNode | SchedulerNode) -> None: ...
    def codegen_sync(self) -> None: ...
    def flush(self) -> None: ...
    def codegen_combo_kernel(self, *args: Any, **kwargs: Any) -> None: ...
    def benchmark_fused_nodes(self, nodes: Sequence[BaseSchedulerNode]) -> tuple[float, str]: ...
    def benchmark_codegened_module(self, module): ...
    def generate_kernel_code_from_nodes(self, nodes: Sequence[Any], benchmark_kernel: bool = False) -> str: ...
    def benchmark_combo_kernel(self, node_list: Sequence[BaseSchedulerNode]) -> tuple[float, float, list[str | None]]: ...
