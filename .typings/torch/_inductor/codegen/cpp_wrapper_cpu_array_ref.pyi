import torch
import torch._ops
from .. import config as config, ir as ir
from ..utils import sympy_product as sympy_product
from ..virtualized import V as V
from .cpp_utils import DTYPE_TO_CPP as DTYPE_TO_CPP
from .cpp_wrapper_cpu import CppWrapperCpu as CppWrapperCpu
from .wrapper import BufferLike as BufferLike, EnterSubgraphLine as EnterSubgraphLine, ExitSubgraphLine as ExitSubgraphLine, MemoryPlanningLine as MemoryPlanningLine, MemoryPlanningState as MemoryPlanningState, PythonWrapperCodegen as PythonWrapperCodegen
from _typeshed import Incomplete
from collections.abc import Sequence
from typing import Any, Callable

BufferName = str
MAX_STACK_ALLOCATION_SIZE: Incomplete

class CppWrapperCpuArrayRef(CppWrapperCpu):
    """
    Generates cpp wrapper for running on CPU and calls cpp kernels

    This class is forked from CppWrapperCpu, with a difference that tensors may be
    represented as ArrayRef, see torch/csrc/inductor/aoti_runtime/arrayref_tensor.h
    """
    allow_stack_allocation: Incomplete
    stack_allocated_buffers: dict[BufferName, BufferLike]
    def __init__(self) -> None: ...
    @staticmethod
    def create(is_subgraph: bool, subgraph_name: str | None, parent_wrapper: PythonWrapperCodegen | None, partition_signatures: ir.GraphPartitionSignature | None = None): ...
    @staticmethod
    def get_input_cpp_type(input): ...
    @staticmethod
    def get_device_include_path(device: str) -> str: ...
    def codegen_input_numel_asserts(self) -> None: ...
    def generate_extern_kernel_alloc(self, *args, **kwargs) -> None: ...
    def generate_extern_kernel_out(self, *args, **kwargs) -> None: ...
    def generate_fallback_kernel(self, node: ir.FallbackKernel) -> None: ...
    def _generate_kernel_call_helper(self, kernel_name: str, call_args, *, device=None, triton: bool = True, arg_types=None, raw_keys=None, raw_args=None, triton_meta=None, graph_name: str = '', original_fxnode_name=None):
        """
        Generates kernel call code.

        triton: Defines whether the GPU backend uses Triton for codegen.
                Otherwise it uses the CUDA language for codegen.
                Only valid when cuda == True.
        """
    def write_wrapper_decl(self) -> None: ...
    def generate_return(self, output_refs: list[str]): ...
    lines: Incomplete
    def memory_plan(self) -> None: ...
    def memory_plan_reuse(self) -> None: ...
    def can_stack_allocate_buffer(self, buffer): ...
    def make_buffer_free(self, buffer): ...
    def make_buffer_allocation(self, buffer): ...
    def make_allocation(self, name, device, dtype, shape, stride, buffer_if_can_stack_allocate=None): ...
    def make_buffer_reuse(self, old: BufferLike, new: BufferLike, delete_old: bool): ...
    def _assert_safe_to_use_borrow_arrayref_tensor_as_tensor(self) -> None: ...
    def is_safe_to_use_borrow_arrayref_tensor_as_tensor(self): ...
    def generate_c_shim_extern_kernel_call(self, kernel: str, args: list[str], device: str, **_) -> None: ...
    def generate_scatter_fallback(self, output, inputs, cpp_kernel_name, python_kernel_name, src_is_tensor, reduce, kwargs) -> None: ...
    def generate_index_put_fallback(self, kernel, x, indices, values, accumulate) -> None: ...
    def generate_fallback_kernel_with_runtime_lookup(self, buf_name: str, python_kernel_name: str, get_args: Callable[[], Sequence[str]], op_overload: torch._ops.OpOverload | torch._ops.HigherOrderOperator, raw_args: Sequence[Any], outputs: Sequence[ir.Buffer]) -> None: ...
    def codegen_device_copy(self, src, dst, non_blocking: bool): ...
    def codegen_reinterpret_view(self, data, size, stride, offset, writeline: Callable[..., None], dtype=None) -> str:
        """Returns a newly-created, temporary RAII tensor handle containing the
        reinterpreted tensor data.  Callers of this function are responsible for saving
        the handle if persistent access is needed."""
    def val_to_arg_str(self, val, type_=None) -> str: ...
    def codegen_tensor_item(self, dtype: torch.dtype, tensor: str, scalar: str, indented_buffer=None): ...
