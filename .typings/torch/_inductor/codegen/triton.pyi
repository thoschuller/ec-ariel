import collections
import dataclasses
import functools
import sympy
import torch
from .. import config as config, ir as ir, metrics as metrics
from ...utils._sympy.symbol import SymT as SymT, free_symbol_is_type as free_symbol_is_type, prefix_str as prefix_str, symbol_is_type as symbol_is_type
from ...utils._sympy.value_ranges import ValueRanges as ValueRanges
from ..async_compile import AsyncCompile as AsyncCompile
from ..codecache import PyCodeCache as PyCodeCache, code_hash as code_hash, get_path as get_path, write_atomic as write_atomic
from ..ir import IRNode as IRNode
from ..ops_handler import DefaultHandler as DefaultHandler
from ..runtime import triton_heuristics as triton_heuristics
from ..runtime.benchmarking import benchmarker as benchmarker
from ..runtime.hints import AutotuneHint as AutotuneHint, DeviceProperties as DeviceProperties, TRITON_MAX_BLOCK as TRITON_MAX_BLOCK, TRITON_MAX_RSPLIT as TRITON_MAX_RSPLIT
from ..runtime.runtime_utils import get_max_y_grid as get_max_y_grid, next_power_of_2 as next_power_of_2
from ..scheduler import BaseSchedulerNode as BaseSchedulerNode, FusedSchedulerNode as FusedSchedulerNode, Scheduler as Scheduler, SchedulerNode as SchedulerNode
from ..utils import DelayReplaceLine as DelayReplaceLine, Placeholder as Placeholder, cache_on_self as cache_on_self, get_bounds_index_expr as get_bounds_index_expr, get_fused_kernel_name as get_fused_kernel_name, get_kernel_metadata as get_kernel_metadata, is_welford_reduction as is_welford_reduction, prefix_is_reduction as prefix_is_reduction, sympy_dot as sympy_dot, sympy_product as sympy_product, sympy_subs as sympy_subs, triton_type as triton_type, triton_version_uses_attrs_dict as triton_version_uses_attrs_dict, upcast_compute_type as upcast_compute_type
from ..virtualized import ReductionType as ReductionType, StoreMode as StoreMode, V as V
from ..wrapper_benchmark import get_kernel_category_by_source_code as get_kernel_category_by_source_code
from .block_analysis import BlockPatternMatcher as BlockPatternMatcher
from .common import ArgName as ArgName, BackendFeature as BackendFeature, CSE as CSE, CSEVariable as CSEVariable, ConstexprArg as ConstexprArg, DeferredLine as DeferredLine, IndentedBuffer as IndentedBuffer, InplacedBuffer as InplacedBuffer, OpOverrides as OpOverrides, PythonPrinter as PythonPrinter, RemovedArg as RemovedArg, SizeArg as SizeArg, TensorArg as TensorArg, WorkspaceArg as WorkspaceArg, WorkspaceZeroMode as WorkspaceZeroMode
from .simd import IterationRanges as IterationRanges, IterationRangesEntry as IterationRangesEntry, IterationRangesRoot as IterationRangesRoot, SIMDKernel as SIMDKernel, SIMDScheduling as SIMDScheduling, constant_repr as constant_repr
from .simd_kernel_features import SIMDKernelFeatures as SIMDKernelFeatures
from .triton_utils import config_of as config_of, equal_1_arg_indices as equal_1_arg_indices, non_constexpr_signature as non_constexpr_signature, should_unwrap_unspec_arg as should_unwrap_unspec_arg, signature_to_meta as signature_to_meta
from .wrapper import SymbolicCallArg as SymbolicCallArg
from _typeshed import Incomplete
from collections.abc import Sequence
from torch._dynamo.device_interface import get_interface_for_device as get_interface_for_device
from torch._dynamo.utils import identity as identity, preserve_rng_state as preserve_rng_state
from torch._inductor.dtype_propagation import DtypePropagationOpsHandler as DtypePropagationOpsHandler
from torch._prims_common import is_integer_dtype as is_integer_dtype
from torch.utils._ordered_set import OrderedSet as OrderedSet
from torch.utils._sympy.functions import CeilDiv as CeilDiv, FloorDiv as FloorDiv, ModularIndexing as ModularIndexing
from torch.utils._triton import has_triton_package as has_triton_package
from typing import Any, Callable, TypeVar

_T = TypeVar('_T')
log: Incomplete
perf_hint_log: Incomplete
schedule_log: Incomplete
fusion_log: Incomplete
async_compile: Incomplete

class OpDtypeSupport:
    """
    Some Triton ops such as libdevice and tl.math only support float32 and float64.
    This class records which dtypes are supported by specific IR ops.
    """
    supported_dtypes: dict[str, OrderedSet[torch.dtype]]
    convert_outputs: dict[str, bool]
    @classmethod
    def register_upcast(cls, func: Callable[..., str], convert_output: bool) -> None: ...

def gen_attr_descriptor_import() -> str:
    """
    import AttrsDescriptor if the triton version is new enough to have this
    class defined.
    """
def gen_common_triton_imports() -> str: ...

class TritonSymbols:
    """
    Stores sympy.Symbol instances and constants associated with triton codegen.
    """
    reduction_types: Incomplete
    block_types: Incomplete
    block_offsets: Incomplete
    block_sizes: Incomplete
    @classmethod
    def get_block_size(cls, tree: IterationRanges) -> sympy.Symbol: ...
    @classmethod
    def get_block_offset(cls, tree: IterationRanges) -> sympy.Symbol: ...

@dataclasses.dataclass
class IndexingOptions:
    index_str: str
    mask_vars: OrderedSet[str]
    expand_str: str | None
    _has_rindex: bool
    index: sympy.Expr
    def has_mask(self) -> bool: ...
    def has_indirect(self) -> bool: ...
    def has_rindex(self) -> bool: ...
    def has_tmpmask(self) -> bool: ...
    def has_rmask(self) -> bool: ...
    @property
    def mask_str(self) -> str: ...

@dataclasses.dataclass
class BlockPtrOptions:
    params: BlockParameters
    constant_offset: sympy.Expr
    order: list[int]
    mask_vars: OrderedSet[str]
    broadcast_shape: Sequence[sympy.Expr]
    broadcasting_dims: list[bool]
    final_shape: Sequence[sympy.Expr]
    _boundary_check: list[int] | None = ...
    @property
    def shape(self) -> list[sympy.Expr]: ...
    @property
    def block_shape(self) -> list[sympy.Expr]: ...
    @property
    def strides(self) -> list[sympy.Expr]: ...
    @property
    def offsets(self) -> list[sympy.Expr]: ...
    def codegen_broadcast_and_reshape(self, value: str, initial_shape: Sequence[sympy.Expr], final_shape: Sequence[sympy.Expr], allow_implicit: bool) -> str:
        """
        Generate a broadcast and a reshape for the block pointer.
        This restores stride-0 dimensions which were removed from the block pointer.
        """
    @staticmethod
    def create(*, params: BlockParameters, constant_offset: sympy.Expr, range_trees: list[IterationRangesRoot], mask_vars: OrderedSet[str], get_max_block: Callable[[str], int]) -> BlockPtrOptions:
        """Helper to create a  BlockPtrOptions instance"""
    def replace_offset(self, expr: sympy.Expr, replacement: sympy.Expr, symt: SymT) -> sympy.Expr:
        """
        Replaces instances of {symt}_offset with the new expression.
        """
    def format(self, name: str, roffset: bool = True) -> str:
        '''
        Codegen a call to tl.make_block_ptr()

        Args:
            name: variable name for pointer
            roffset: should rn_offset be included in offsets=..., for use with tl.advance()

        Returns:
            "tl.make_block_ptr(...)"
        '''
    def compute_boundary_check(self, get_max_block: Callable[[str], int], range_trees: list[IterationRangesRoot]) -> None:
        """List of indices to pass to tl.load(boundary_check=...)"""
    def boundary_check(self) -> list[int]: ...
    def advance_roffset(self, symt: SymT) -> sympy.Expr:
        """
        Codegen string to pass to tl.advance(name, ...).

        Advance is the difference between offsets in each loop iteration.
        To compute it, we replace rN_offset with multiples of RN_BLOCK.
        Since we expect rN_offset to vary in range(0, rN_numel, RN_BLOCK), the first
        iteration has rN_offset=0, while the second has rN_offset=RN_BLOCK.
        """
    def has_indirect(self) -> bool: ...
    def has_rindex(self) -> bool: ...
    def has_rmask(self) -> bool: ...
    def has_tmpmask(self) -> bool: ...
    def has_mask(self) -> bool: ...

def triton_reshape(value: str, old_shape: Sequence[sympy.Expr], new_shape: Sequence[sympy.Expr]) -> str:
    """Workaround https://github.com/triton-lang/triton/issues/2836"""

class TritonPrinter(PythonPrinter):
    def _print_TruncToInt(self, expr: sympy.Expr) -> str: ...
    def _print_Float(self, expr: sympy.Expr) -> str: ...
    def _print_ToFloat(self, expr: sympy.Expr) -> str: ...
    def _print_PythonMod(self, expr: sympy.Expr) -> str: ...
    def _print_FloorDiv(self, expr: sympy.Expr) -> str: ...
    def _print_IntTrueDiv(self, expr: sympy.Expr) -> str: ...
    def _print_floor(self, expr: sympy.Expr) -> str: ...
    def _print_FloorToInt(self, expr: sympy.Expr) -> str: ...
    def _print_ceiling(self, expr: sympy.Expr) -> str: ...
    def _print_CeilToInt(self, expr: sympy.Expr) -> str: ...
    def _helper_sqrt(self, expr: sympy.Expr) -> str: ...
    def _print_FloatPow(self, expr: sympy.Expr) -> str: ...
    def _print_PowByNatural(self, expr: sympy.Expr) -> str: ...
    def _print_Where(self, expr: sympy.Expr) -> str: ...
    def _print_min_max_helper(self, expr: sympy.Expr, cmp: str) -> str:
        """
        Helper for max/min code generation.
        cmp: > or <
        """
    def _print_Min(self, expr: sympy.Expr) -> str: ...
    def _print_Max(self, expr: sympy.Expr) -> str: ...
    def _print_Abs(self, expr: sympy.Expr) -> str: ...
    def _print_OpaqueUnaryFn_cos(self, expr: sympy.Expr) -> str: ...
    def _print_OpaqueUnaryFn_cosh(self, expr: sympy.Expr) -> str: ...
    def _print_OpaqueUnaryFn_acos(self, expr: sympy.Expr) -> str: ...
    def _print_OpaqueUnaryFn_sin(self, expr: sympy.Expr) -> str: ...
    def _print_OpaqueUnaryFn_sinh(self, expr: sympy.Expr) -> str: ...
    def _print_OpaqueUnaryFn_asin(self, expr: sympy.Expr) -> str: ...
    def _print_OpaqueUnaryFn_tan(self, expr: sympy.Expr) -> str: ...
    def _print_OpaqueUnaryFn_tanh(self, expr: sympy.Expr) -> str: ...
    def _print_OpaqueUnaryFn_atan(self, expr: sympy.Expr) -> str: ...
    def _print_OpaqueUnaryFn_log2(self, expr: sympy.Expr) -> str: ...
    def _print_RoundToInt(self, expr: sympy.Expr) -> str: ...
    def _print_RoundDecimal(self, expr: sympy.Expr) -> str: ...

texpr: Incomplete

def triton_compute_type(dtype: torch.dtype) -> str:
    """Convert torch.dtype to triton type and upcast [b]float16 to float32"""
def triton_store_type(dtype: torch.dtype) -> str:
    """Convert torch.dtype to triton type, with fix for storing tl.bool"""
def upcast_acc_dtype(dtype: torch.dtype) -> torch.dtype:
    """Implicit upcasts used for Triton reduction types"""
def triton_acc_type(dtype: torch.dtype) -> str:
    """Convert torch.dtype to triton type, with reduction upcasts"""
def low_precision_fp(dtype: torch.dtype) -> bool: ...
def low_precision_fp_var(var: CSEVariable | Any) -> bool: ...

class TritonCSEVariable(CSEVariable):
    mask_vars: OrderedSet[str]
    def __init__(self, name, bounds: ValueRanges[Any], dtype: torch.dtype) -> None: ...
    def update_on_args(self, name, args, kwargs) -> None: ...

def get_dtype_handler() -> DtypePropagationOpsHandler: ...
def maybe_upcast_float32(convert_output: bool = True) -> Callable[[_T], _T]:
    """
    Codegen helper to upcast arguments to float32, depending on the config and dtype.
    This decorates tl.math/libdevice codegen functions.
    """

class TritonOverrides(OpOverrides):
    """Map element-wise ops to Triton"""
    _LOG_2_E: Incomplete
    @staticmethod
    def to_dtype(x, dtype: torch.dtype, src_dtype: torch.dtype | None = None, use_compute_types: bool = True): ...
    @staticmethod
    def to_dtype_bitcast(x, dtype: torch.dtype, src_dtype: torch.dtype): ...
    @staticmethod
    def _shaped_constant(value, dtype, shape): ...
    @classmethod
    def constant(cls, value, dtype): ...
    @staticmethod
    def abs(x): ...
    @staticmethod
    def truediv(x, y): ...
    @staticmethod
    def mod(x, y): ...
    @staticmethod
    def exp(x):
        """
        When use_fast_math, use the ftz (flushing to zero) variant
        of exponent computation.

        Check https://github.com/triton-lang/triton/issues/5735 for
        more details.
        """
    @staticmethod
    def exp2(x): ...
    @staticmethod
    def expm1(x): ...
    @staticmethod
    def sqrt(x): ...
    @staticmethod
    def relu(x): ...
    @staticmethod
    def minimum(a, b): ...
    @staticmethod
    def maximum(a, b): ...
    @staticmethod
    def where(a, b, c): ...
    @staticmethod
    def inline_asm_elementwise(*inputs, asm, constraints=None, dtype=..., is_pure: bool = True, pack: int = 1): ...
    @staticmethod
    def cos(x): ...
    @staticmethod
    def sin(x): ...
    @classmethod
    def index_expr(cls, expr, dtype) -> None: ...
    @staticmethod
    def masked(mask, body, other) -> None: ...
    @staticmethod
    def lgamma(x): ...
    @staticmethod
    def erf(x): ...
    @staticmethod
    def cosh(x): ...
    @staticmethod
    def sinh(x): ...
    @staticmethod
    def acos(x): ...
    @staticmethod
    def acosh(x): ...
    @staticmethod
    def asin(x): ...
    @staticmethod
    def asinh(x): ...
    @staticmethod
    def atan2(x, y): ...
    @staticmethod
    def atan(x): ...
    @staticmethod
    def atanh(x): ...
    @staticmethod
    def copysign(x, y): ...
    @staticmethod
    def erfc(x): ...
    @staticmethod
    def erfinv(x): ...
    @staticmethod
    def hypot(x, y): ...
    @staticmethod
    def log10(x): ...
    @staticmethod
    def log2(x): ...
    @staticmethod
    def nextafter(x, y): ...
    @staticmethod
    def logical_and(a, b): ...
    @staticmethod
    def logical_not(a): ...
    @staticmethod
    def logical_or(a, b): ...
    @staticmethod
    def logical_xor(a, b): ...
    @staticmethod
    def bitwise_and(a, b): ...
    @staticmethod
    def bitwise_not(a): ...
    @staticmethod
    def bitwise_or(a, b): ...
    @staticmethod
    def bitwise_xor(a, b): ...
    @staticmethod
    def bitwise_left_shift(a, b): ...
    @staticmethod
    def bitwise_right_shift(a, b): ...
    @staticmethod
    def rand(seed, offset): ...
    @staticmethod
    def randn(seed, offset): ...
    @staticmethod
    def randint64(seed, offset, low, high): ...
    @staticmethod
    def load_seed(name, offset) -> None: ...
    @staticmethod
    def rsqrt(x): ...
    @staticmethod
    def log1p(x): ...
    @staticmethod
    def tan(x): ...
    @staticmethod
    def tanh(x): ...
    @staticmethod
    def sigmoid(x): ...
    @staticmethod
    def signbit(x): ...
    @staticmethod
    def fmod(a, b): ...
    @staticmethod
    def pow(a, b): ...
    @staticmethod
    def log(x): ...
    @staticmethod
    def isinf(x): ...
    @staticmethod
    def isnan(x): ...
    @staticmethod
    def round(x): ...
    @staticmethod
    def floor(x): ...
    @staticmethod
    def floordiv(a, b): ...
    @staticmethod
    def sign(x): ...
    @staticmethod
    def trunc(x): ...
    @staticmethod
    def truncdiv(a, b): ...
    @staticmethod
    def ceil(x): ...

class TritonKernelOverrides(TritonOverrides):
    """Map element-wise ops to Triton within a TritonKernel

    Unlike TritonOverrides, these assume the code is going to be inserted into
    the body of the main triton kernel and so it may use indexing and mask
    variables which are assumed to already be defined in the current scope.
    """
    def __init__(self, *args, **kwargs) -> None: ...
    @classmethod
    @functools.cache
    def _setup_libdevice_routing(cls):
        """Set up routing to libdevice implementations for fp64 inputs."""
    @classmethod
    def constant(cls, value, dtype): ...
    @classmethod
    def index_expr(cls, expr, dtype): ...
    @staticmethod
    def masked(mask, body, other): ...
    @staticmethod
    def load_seed(name, offset): ...
    @staticmethod
    def frexp(x): ...

class HelperFunctions:
    """An ordered set of helper functions."""
    _templates_seen: dict[str, str]
    finalized_helpers: list[str]
    def __init__(self) -> None: ...
    def add(self, template_code: str, *, base_name: str = '_triton_helper_fn') -> str:
        """This accepts a function definition with the function name
        left as a format specifier e.g.

            @triton.jit
            def {name}(arg0, arg1):
                return arg0 + arg1

        We add the templated code to the function set and return the name
        assigned to that function.

        """
    def __iter__(self): ...
    def __getitem__(self, idx): ...

@dataclasses.dataclass
class BlockParameters:
    """
    Class representing ND block dimensions, for block pointer analysis.
    """
    shape: list[sympy.Expr] = dataclasses.field(default_factory=list)
    block_shape: list[sympy.Expr] = dataclasses.field(default_factory=list)
    strides: list[sympy.Expr] = dataclasses.field(default_factory=list)
    offsets: list[sympy.Expr] = dataclasses.field(default_factory=list)
    def __add__(self, other: BlockParameters) -> BlockParameters:
        """
        Concatenates block parameters.
        """

class CooperativeReductionWorkspaceCache:
    """
    The scratch space used for cooperative reductions can be reused
    after two reduction loops.  This keeps track of what can be reused.
    """
    args: Incomplete
    current_loop: Incomplete
    prior_loop: Incomplete
    ready_for_reuse: Incomplete
    loop_count: int
    store_count: int
    def __init__(self, args) -> None: ...
    def allocate(self, nbytes: sympy.Expr): ...
    def on_loop_end(self) -> None: ...
    def increment_store_count(self): ...

@dataclasses.dataclass
class FixedTritonConfig:
    config: dict[str, int]
    def __getitem__(self, item): ...
    def __contains__(self, item) -> bool: ...

class TritonCSE(CSE[TritonCSEVariable, str | tuple[str, str]]):
    """
    Subclasses CSE to apply the current load mask to the cache key to avoid CSEing
    variables across separate masked blocks.
    """
    def augment_key(self, cache_key: str) -> str | tuple[str, str]: ...

class TritonKernel(SIMDKernel[TritonCSEVariable]):
    overrides = TritonKernelOverrides
    helper_functions: HelperFunctions
    kexpr: Callable[[sympy.Expr], str]
    allow_block_ptr: bool
    optimize_mask: bool
    fixed_config: Incomplete
    cse: Incomplete
    post_loop_combine: IndentedBuffer
    post_loop_store: IndentedBuffer
    outside_loop_vars: Incomplete
    min_elem_per_thread: Incomplete
    block_ptr_id: Incomplete
    block_ptr_to_buffer: Incomplete
    pointer_advancements: dict[SymT, dict[str, list[sympy.Expr]]]
    _load_counts: collections.Counter[str]
    autotune_hints: Incomplete
    triton_meta: dict[str, Any] | None
    def __init__(self, tiling: dict[str, sympy.Expr], min_elem_per_thread: int = 0, optimize_mask: bool = True, fixed_config: FixedTritonConfig | None = None, **kwargs) -> None: ...
    def dtype_to_str(self, dtype: torch.dtype) -> str: ...
    def should_use_cooperative_reduction(self) -> bool: ...
    semaphores_name: Incomplete
    cooperative_reduction_workspace_cache: Incomplete
    def init_cooperative_reduction(self) -> None:
        """One time setup code for cooperative reductions."""
    def init_cooperative_reduction_mask(self) -> None: ...
    def codegen_range_tree(self) -> None: ...
    def need_numel_args(self):
        """
        Indicate whether we need provide numel as arguments for the generated
        kernel calls in the benchmark.

        Should be true for pointwise/reduction kernels but false for triton
        matmul kernels.
        """
    def should_use_persistent_reduction(self) -> bool: ...
    def want_no_x_dim(self): ...
    @property
    def assert_function(self) -> str: ...
    def indexing(self, index: sympy.Expr, *, copy_shape=None, dense_indexing: bool = False, override_mask=None, block_ptr: bool = False):
        """
        Compute the index and mask to pass to tl.load() or tl.store()
        """
    def codegen_block_ptr(self, name: str, var: str, indexing: BlockPtrOptions, other: str = '') -> tuple[str, str]: ...
    def codegen_block_ptr_store_line(self, name, indexing, block_ptr, value, other: str = ''): ...
    def check_bounds(self, expr: sympy.Expr, size: sympy.Expr, lower: bool, upper: bool): ...
    def get_load_buffer(self, indexing): ...
    def load(self, name: str, index: sympy.Expr): ...
    def store(self, name: str, index: sympy.Expr, value: CSEVariable, mode: StoreMode = None) -> None: ...
    def guard_cooperative_store(self, name, buffer):
        """
        For cooperative reductions only one thread block should write out the result.
        We rotate which thread block does each write for better parallelism
        """
    def bucketize(self, values: CSEVariable, boundaries: tuple[str, sympy.Expr, sympy.Expr, sympy.Expr], boundary_indices: CSEVariable, indexing_dtype: torch.dtype, right: bool, sorter: tuple[str, sympy.Expr] | None = None, sorter_indices: CSEVariable | None = None) -> CSEVariable:
        """
        See [Note: Inductor bucketize op]
        """
    def reduction_resize(self, value) -> str: ...
    def reduction_collapse_dims(self, buffer, value: str, dtype: torch.dtype) -> str:
        """
        Reshape to RBLOCK, collapsing all reduction dims.
        """
    def reduction(self, dtype: torch.dtype, src_dtype: torch.dtype, reduction_type: ReductionType, value: CSEVariable | tuple[CSEVariable, ...]) -> CSEVariable | tuple[CSEVariable, ...]: ...
    def _online_softmax_reduce(self, buffer, accumulator_max, accumulator_sum, dim, dtype: torch.dtype): ...
    def _welford(self, buffer, mean, m2, weight, dim, dtype: torch.dtype):
        """
        Helper to codegen triton_helpers.welford.
        """
    def welford_reduce(self, result_var, reduction_type, value, where_cond, acc_type, dtype):
        """Helper to codegen a welford reduction"""
    def welford_reduce_final_reduction(self, buffer, result_mean, result_m2, result_weight, mean, m2, weight, dim, dtype):
        """Helper to codegen call to triton_helpers.welford"""
    def online_softmax_reduce_final_reduction(self, buffer, result_max, result_sum, peer_max, peer_sum, dim, dtype): ...
    def max_rsplit(self): ...
    def codegen_cooperative_reduction_peer_combine(self, result_var, dtype, default_val):
        """
        Generate code to save a [XBLOCK, RSPLIT] temporary workspace, where each thread block writes a different
        column.  After the barrier, every thread block loads the completed value so that it can compute the final
        value independently.
        """
    inside_reduction: bool
    def store_reduction(self, name: str, index: sympy.Expr, value: CSEVariable | tuple[CSEVariable, ...]): ...
    def _lift_helper(self, fn, num_args, dtypes: tuple[torch.dtype, ...]) -> str: ...
    def scan(self, dtypes: tuple[torch.dtype, ...], combine_fn: Callable[[tuple[CSEVariable, ...], tuple[CSEVariable, ...]], tuple[CSEVariable, ...]], values: tuple[CSEVariable, ...]) -> tuple[CSEVariable, ...]: ...
    def sort(self, dtypes: tuple[torch.dtype, ...], values: tuple[CSEVariable, ...], stable: bool, descending: bool) -> tuple[CSEVariable, ...]: ...
    def codegen_body(self) -> None:
        """
        Concat output code from index_code, loads, compute, stores,
        suffix into self.body.

        For pointwise kernels, this is called just once at the end.

        For reduction kernels, this generates a loop over the reduction
        axis.
        """
    def kernel_benchmark_extra_args(self) -> list[str]: ...
    def codegen_kernel_benchmark(self, num_gb): ...
    def imports_for_benchmark_kernel(self): ...
    def _get_heuristic(self): ...
    @staticmethod
    def inductor_meta_common(): ...
    def codegen_kernel(self, name=None): ...
    @staticmethod
    def _get_persistent_RBLOCK(rnumel): ...
    @staticmethod
    def has_persistent_RBLOCK(rnumel): ...
    def codegen_static_numels(self, code):
        """
        We get a small speedup from hard coding numels if they are static.

        This code stomps on the passed-in values by writing an constant to the top of the kernel.

        In a kernel like:
        def KERNEL_NAME(in_ptr0, in_ptr1, out_ptr2, xnumel, r0_numel, XBLOCK : tl.constexpr, R0_BLOCK : tl.constexpr):

        We would add
        xnumel = 4096
        r0_numel = 768

        After the signature, before the kernel code, if we decided to make these static. As its hardcoded, it becomes
        a better signal to triton on how to unroll and do some static indexing. So, it's not so much that downstream
        knows that its a static numel, as that you just plop a constant into the kernel.
        """
    def _get_grid_type(self) -> type[triton_heuristics.GridExpr]: ...
    def add_numel_to_call_args(self, name, call_args, arg_types) -> None: ...
    def call_kernel(self, name: str, node: IRNode | None = None): ...
    def codegen_nan_check(self) -> None: ...
    def create_cse_var(self, *args, **kwargs) -> TritonCSEVariable: ...
    def codegen_iteration_ranges_entry(self, entry: IterationRangesEntry): ...
    def iteration_ranges_ranges_code(self, entry: IterationRangesRoot) -> str: ...
    def iteration_ranges_scalar_code(self, entry: IterationRangesRoot, value: Any) -> str: ...
    def iteration_ranges_get_pid(self, entry: IterationRangesRoot) -> str: ...
    def needs_yz_grid_overflow(self, entry: IterationRangesRoot) -> bool: ...
    def max_block(self, prefix: str) -> int: ...
    def _has_constant_mask(self, tree: IterationRangesRoot) -> bool: ...
    def _has_constant_xmask(self) -> bool: ...
    def filter_masks(self, mask_vars: OrderedSet[str]) -> None: ...
    @cache_on_self
    def get_reduction_prefixes(self) -> list[str]: ...
    def codegen_reduction_numels(self, buffer: IndentedBuffer) -> None:
        """
        Generates code that flattens ND reduction numels, block sizes, etc. into 1D.
        """
    def _get_reduction_symbols(self, suffix: str, **kwargs) -> list[sympy.Symbol]:
        """
        Helper to initialize symbols like rn_numel, rn_base, etc.
        """
    @cache_on_self
    def _get_reduction_index_coeffs(self) -> list[sympy.Expr]:
        """
        Compute coefficients to convert ND reduction indices to linear indices.
        For example:
          rindex = r0_index * r1_numel * ... * rn_numel + ... + rn_index.
        """
    def _flatten_reduction_indices(self, multi_inds: list[sympy.Expr]) -> sympy.Expr:
        """
        Compute linear reduction indices from N dimensional ones.
        """
    def codegen_reduction_indices(self, buffer: IndentedBuffer) -> None:
        """
        Generates code that converts ND reduction indices into linear indices.
        """
    def iteration_ranges_codegen_header(self, entry: IterationRangesRoot, code: IndentedBuffer) -> None: ...

class TritonScheduling(SIMDScheduling):
    kernel_type: type[Any]
    backend_features: Incomplete
    def __init__(self, scheduler: Scheduler | None) -> None: ...
    @classmethod
    def get_backend_features(cls, device: torch.device): ...
    def codegen_comment(self, node_schedule) -> None: ...
    def define_kernel(self, src_code, node_schedule, kernel): ...
    def benchmark_fused_nodes(self, nodes, n_spills_threshold: int = 8) -> tuple[float, str]:
        """
        Benchmark fused list of nodes and return the execution time
        in milliseconds on randomly generated inputs.
        """
    def benchmark_codegened_module(self, mod, n_spills_threshold: int = 8, node_names: OrderedSet[str] | None = None) -> tuple[float, str]:
        """Benchmark an already compiled module"""
    def create_kernel_choices(self, kernel_features: SIMDKernelFeatures, kernel_args: list[Any], kernel_kwargs: dict[str, Any]) -> list[TritonKernel]: ...
    def add_multi_kernel_choices(self, kernel: TritonKernel, kernel_args: list[Any], kernel_kwargs: dict[str, Any]) -> list[TritonKernel]: ...
    def benchmark_combo_kernel(self, node_list): ...

def debug_triton_code(node: BaseSchedulerNode) -> list[str]: ...
