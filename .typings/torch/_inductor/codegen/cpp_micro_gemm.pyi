import dataclasses
import torch
from .. import cpp_builder as cpp_builder, ir as ir
from ..cpu_vec_isa import VecAMX as VecAMX, VecAVX2 as VecAVX2, VecAVX512 as VecAVX512, VecISA as VecISA, VecNEON as VecNEON, VecSVE256 as VecSVE256, pick_vec_isa as pick_vec_isa
from ..utils import IndentedBuffer as IndentedBuffer, parallel_num_threads as parallel_num_threads
from ..virtualized import V as V
from .common import KernelTemplate as KernelTemplate
from .cpp_template_kernel import CppTemplateKernel as CppTemplateKernel
from .cpp_utils import DTYPE_TO_CPP as DTYPE_TO_CPP, GemmBlocking as GemmBlocking, value_to_cpp as value_to_cpp
from _typeshed import Incomplete
from enum import Enum
from typing import Callable

class LayoutType(Enum):
    NORMAL = 0
    VNNI2 = 1
    VNNI4 = 2

_IS_WINDOWS: Incomplete

def get_restrict_keyword() -> str: ...

class CppMicroGemm:
    """
    A class that codegens a kernel that computes small-sized matrix multiplication.

    A micro GEMM kernel is responsible for register blocking, instruction selection,
    and other CPU architecture-specific optimizations.

    The subclasses need to override `codegen_define` to define the kernel function
    that is called by the code generated by `codegen_call`.
    """
    DECLARE_KERNEL: str
    name: Incomplete
    input_dtype: Incomplete
    input2_dtype: Incomplete
    output_dtype: Incomplete
    compute_dtype: Incomplete
    register_blocking: Incomplete
    alpha: Incomplete
    pack_vnni_B_locally: bool
    def __init__(self, name, input_dtype, input2_dtype, output_dtype, compute_dtype, register_blocking, alpha: int = 1) -> None: ...
    def get_common_options(self): ...
    def get_kernel_declaration(self): ...
    def get_kernel_extra_args_declare(self) -> str: ...
    def get_kernel_extra_args(self, **kwargs) -> list[str]: ...
    def codegen_define(self, kernel: CppTemplateKernel) -> str: ...
    def codegen_call(self, kernel: CppTemplateKernel, A: ir.Buffer, B: ir.Buffer, C: ir.Buffer, accum: bool, prefetch: bool = False, **kwargs_for_extra_args) -> str:
        """
        Generate the code for calling the templated kernel that computes
        `C += alpha * A @ B` if `accum` is True, or `C = alpha * A @ B` otherwise.
        """
    def use_local_vnni_blocking(self, should_block_weight: bool): ...
    def codegen_init(self, kernel: CppTemplateKernel) -> str: ...
    def codegen_finalize(self, kernel: CppTemplateKernel) -> str: ...
    def get_b_layout(self) -> LayoutType: ...
    ALLOCATE_WEIGHT_BUFFER: str
    def codegen_allocate_weight_buffer(self, buffer_name: str, buffer_dtype: str, *size_args) -> str: ...
    def is_woq_int4(self): ...

@dataclasses.dataclass
class CppMicroGemmConfig:
    input_dtype: torch.dtype
    input2_dtype: torch.dtype
    output_dtype: torch.dtype
    compute_dtype: torch.dtype
    vec_isa_cls: type[VecISA]
    register_blocking: GemmBlocking
    extra_check: Callable[..., bool] | None = ...

micro_gemm_configs: dict[type[CppMicroGemm], list[CppMicroGemmConfig]]

def register_micro_gemm(*configs): ...
def generate_gemm_config(vec_isa_cls, register_blockings, input_dtype=..., input2_dtype=None, output_dtype=None, compute_dtype=None, extra_check=None): ...

class CppMicroGemmRef(CppMicroGemm):
    """
    A reference implementation of the CppMicroGemm class with naive C++ code.
    It is used for correctness debugging.
    """
    TEMPLATE_ENTRY: str
    def __init__(self, name, input_dtype, input2_dtype, output_dtype, compute_dtype, alpha) -> None: ...
    def codegen_define(self, kernel: CppTemplateKernel) -> str: ...

def is_int8_woq_gemm_small_m_dim_corner_case(config, m, n, k): ...
def check_int8_woq_small_m_dim(config, m, n, k, alpha, num_threads, **kwargs): ...
def do_not_use_with_small_m_for_int8_woq(config, m, n, k, alpha, num_threads, **kwargs): ...

class CppMicroGemmFP32Vec(CppMicroGemm):
    """
    This class generates the code for micro gemm using fp32 vec instructions for compute.
    It supports input types of torch.float, torch.bfloat16, and torch.half with fp32 output.
    The output of the microkernel is in FP32, but it would be converted to BF16/FP16 in the template,
    if the desired output is BF16/FP16.
    """
    TEMPLATE_ENTRY: str
    TEMPLATE_KERNEL: str
    tail_n: Incomplete
    trans_b: Incomplete
    def __init__(self, name, input_dtype, input2_dtype, output_dtype, compute_dtype, register_blocking, alpha: int = 1, tail_n: bool = False, trans_b: bool = False) -> None: ...
    def codegen_define(self, kernel: CppTemplateKernel) -> str: ...

def check_amx_extra(config, m, n, k, alpha, num_threads, **kwargs): ...

class CppMicroGemmAMX(CppMicroGemm):
    """
    This class generates the code for micro gemm using Advanced Matrix extension (AMX)
    instructions available in 4th generation Intel Xeon for compute.
    It supports input types of torch.bfloat16 with fp32 output.
    """
    TEMPLATE_ENTRY: str
    TEMPLATE_KERNEL: str
    def codegen_define(self, kernel: CppTemplateKernel) -> str: ...
    def codegen_init(self, kernel: CppTemplateKernel) -> str: ...
    def codegen_finalize(self, kernel: CppTemplateKernel) -> str: ...
    def get_kernel_extra_args_declare(self) -> str: ...
    def get_kernel_extra_args(self, **kwargs) -> list[str]: ...
    def get_b_layout(self): ...

def check_brgemm_extra(config, m, n, k, alpha, num_threads, **kwargs): ...

class CppMicroBrgemm(CppMicroGemm):
    """
    This class generates the code for micro gemm using oneDNN brgemm.
    It supports input types of torch.half.
    """
    TEMPLATE_ENTRY: str
    def codegen_define(self, kernel: CppTemplateKernel) -> str: ...
    def codegen_finalize(self, kernel: CppTemplateKernel) -> str: ...
    def get_b_layout(self): ...

def check_woq_int4_extra(config, m, n, k, alpha, num_threads, **kwargs): ...

class CppMicroGemmWoQInt4Avx512(CppMicroGemmFP32Vec):
    """
    This class generates the code for WoQ int4 micro gemm using AVX512 intrinsics.
    It is based on the corresponding ATen kernel.
    Shape of packed weight = [N // 64, K, 32], viewed as [N, K // 2]
    Shape of packed ScalesAndZeros = [K // group_size, N, 2]
    """
    TEMPLATE_ENTRY: str
    TEMPLATE_KERNEL: str
    def get_kernel_extra_args_declare(self) -> str: ...
    def get_kernel_extra_args(self, **kwargs) -> list[str]: ...
    def is_woq_int4(self): ...

class CppMicroGemmWoQInt4Amx(CppMicroGemmAMX):
    """
    This class generates the code for WoQ int4 micro gemm using AMX intrinsics,
    which are available on 4th and newer generations of Intel Xeon.
    Shape of packed weight = [N // 32, K, 16], viewed as [N, K // 2]
    Shape of packed ScalesAndZeros = [K // group_size, N, 2]
    Reuse TEMPLATE_KERNEL of CppMicroGemmAMX.
    """
    TEMPLATE_ENTRY: str
    def get_kernel_extra_args_declare(self) -> str: ...
    def get_kernel_extra_args(self, **kwargs) -> list[str]: ...
    def is_woq_int4(self): ...

def create_micro_gemm(name, m, n, k, input_dtype, input2_dtype, output_dtype=None, compute_dtype=None, alpha: int = 1, num_threads: int = -1, use_ref: bool = True, q_group_size=None) -> CppMicroGemm | None:
    """
    Based on the provided info, try to find the config of the micro-kernel that would
    deliver the best performance in terms of lower latency for this case.
    """
