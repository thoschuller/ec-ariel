import contextlib
import dataclasses
import functools
import sympy
import torch
import torch.fx
import types
from .. import config as config, cpp_builder as cpp_builder, cpu_vec_isa as cpu_vec_isa, ir as ir, metrics as metrics
from ..._dynamo.utils import counters as counters
from ..loop_body import LoopBody as LoopBody
from ..scheduler import BaseSchedulerNode as BaseSchedulerNode, BaseScheduling as BaseScheduling, ExternKernelSchedulerNode as ExternKernelSchedulerNode, ForeachKernelSchedulerNode as ForeachKernelSchedulerNode, FusedSchedulerNode as FusedSchedulerNode, Scheduler as Scheduler, SchedulerNode as SchedulerNode
from ..utils import Placeholder as Placeholder, cache_on_self as cache_on_self, get_bounds_index_expr as get_bounds_index_expr, get_fused_kernel_name as get_fused_kernel_name, has_free_symbols as has_free_symbols, is_multi_outputs_template as is_multi_outputs_template, is_welford_reduction as is_welford_reduction, parallel_num_threads as parallel_num_threads, set_kernel_post_grad_provenance_tracing as set_kernel_post_grad_provenance_tracing, sympy_index_symbol as sympy_index_symbol, sympy_index_symbol_with_prefix as sympy_index_symbol_with_prefix, sympy_product as sympy_product, sympy_subs as sympy_subs
from ..virtualized import NullKernelHandler as NullKernelHandler, OpsValue as OpsValue, V as V, ops as ops
from .common import BackendFeature as BackendFeature, BracesBuffer as BracesBuffer, CSE as CSE, CSEVariable as CSEVariable, DTYPE_TO_COMPUTATION_DTYPE as DTYPE_TO_COMPUTATION_DTYPE, DataTypePropagation as DataTypePropagation, DeferredLine as DeferredLine, IndentedBuffer as IndentedBuffer, Kernel as Kernel, KernelArgs as KernelArgs, OpOverrides as OpOverrides, OptimizationContext as OptimizationContext
from .cpp_utils import CppCSEVariable as CppCSEVariable, DTYPE_TO_CPP as DTYPE_TO_CPP, INDEX_TYPE as INDEX_TYPE, LocalBufferContext as LocalBufferContext, _get_dtype_from_loopbodies as _get_dtype_from_loopbodies, _get_loop_body as _get_loop_body, cexpr as cexpr, cexpr_index as cexpr_index, codegen_rand as codegen_rand, get_promote_dtype as get_promote_dtype, may_unify_binary_op_mask_type as may_unify_binary_op_mask_type, promote_args as promote_args, template_fusion_with_epilogues_supported as template_fusion_with_epilogues_supported, unify_mask_base_type as unify_mask_base_type, value_to_cpp as value_to_cpp
from _typeshed import Incomplete
from collections.abc import Generator, Sequence
from enum import Enum
from torch._inductor import dependencies as dependencies
from torch._prims_common import is_float_dtype as is_float_dtype, is_integer_dtype as is_integer_dtype
from torch.utils._ordered_set import OrderedSet as OrderedSet
from torch.utils._sympy.functions import CeilDiv as CeilDiv, FloorDiv as FloorDiv, ModularIndexing as ModularIndexing
from torch.utils._sympy.symbol import SymT as SymT, free_symbol_is_type as free_symbol_is_type, symbol_is_type as symbol_is_type
from typing import Callable

_IS_WINDOWS: Incomplete

@functools.cache
def get_export_declaration(): ...

schedule_log: Incomplete
NATIVE_OMP_RTYPES: Incomplete
RTYPE_TO_CPP: Incomplete
VECTORIZABLE_RTYPES: Incomplete
PYTHON_TO_CPP: Incomplete
CONTAINER_PYTHON_TO_CPP: Incomplete
DTYPE_LOWP_FP: Incomplete
VECTORIZABLE_DTYPES: list[torch.dtype]
MASKED_VECTORIZABLE_DTYPES: list[torch.dtype]

def reduction_init(reduction_type, dtype): ...
def reduction_acc_type(reduction_type, dtype): ...
def reduction_combine(reduction_type, var, next_value, index: sympy.Symbol | None = None, src_dtype=None): ...
def reduction_project(reduction_type, acc): ...
def move_code_under_inner_loop(code: IndentedBuffer, iter_var: sympy.Expr, new_iter_var: str, loop_start: sympy.Expr, loop_end: sympy.Expr) -> BracesBuffer:
    """
    f(iter_var) is transformed to f(new_iter_var) under the inner loop
      \\/
    for (new_iter_var = loop_start; new_iter_var < loop_end; new_iter_var++) {
        f(new_iter_var)
    }
    Please be careful while using this function,
    as the variable defined in f(iter_var) will be invalid outside the for loop.
    For example:
    auto tmp0 = in_ptr[x0]; ->
    for (new_x0 = start; new_x0 < end; new_x0++){
        auto tmp0 = in_ptr[new_x0];
    }
    The tmp0 is invalid outside the loop.
    """
def reduction_prefix_array(acc_var: str | CSEVariable, acc_type: str, reduction_type: str, dtype: torch.dtype, len: str | int, init_fn):
    """
    MSVC don't support dynamic array(VLA). So we use std::unique_ptr here.
    Ref: https://stackoverflow.com/questions/56555406/creating-dynamic-sized-array-using-msvc-c-compiler
    MSVC is the only one compiler without VLA. support. Since MSVC can't get good performance here.
    We just use unique_ptr make it works on MSVC.
    For other compilers, we continue to use VLA to get best performance.
    """
def replace_acc_name(buffer: IndentedBuffer, name: str, new_name: str): ...
@functools.lru_cache
def stride_at(index: sympy.Expr, var: sympy.Symbol): ...
@functools.lru_cache
def simplify_index_in_vec_range(index: sympy.Expr, var: sympy.Expr, vec_length: int):
    """
    Simplifies the index expression within the range of a vectorized loop.
    Given a vectorized loop variable `var` in the range of a loop with `vec_length`,
    this function transforms the `index` into an equivalent form. It handles
    simplifications for cases where `var` can be expressed as `vec_length * a + b`,
    where `b` ranges from 0 to `vec_length - 1`. The function reduces occurrences
    of `FloorDiv` and `ModularIndexing` in the `index` with best-effort optimizations.

    NOTE:
    The simplified index expression is intended for analysis purposes only, not
    for code generation. It replaces `FloorDiv` and `ModularIndexing` with free variables
    which are not dependent on the loop variable `var` in the vectorized range. Check
    https://github.com/pytorch/pytorch/pull/117221#discussion_r1449746217 for more details.

    Examples:
    1. If `var` is `x3` and `vec_length` is 16, and `x3 = 16*a + b`, then
       `FloorDiv(x3, div)` or `ModularIndexing(x3, div, mod)` becomes a free variable
       when `div` is divisible by 16.
    2. `ModularIndexing(x3, 1, mod)` can be simplified to `x3 + c` where `c` is a free
       variable when `mod` is divisible by 16.
    """
@functools.lru_cache
def stride_at_vec_range(index: sympy.Expr, var: sympy.Symbol, vec_length: int | None = None): ...

@dataclasses.dataclass
class ParallelDepth:
    """
    A class representing parallel depth.
    Includes the starting depth of parallelism and the depth of parallelism.
    """
    parallel_depth: int
    start_depth: int

class OuterLoopFusedSchedulerNode(FusedSchedulerNode):
    @classmethod
    def fuse(cls, node1: BaseSchedulerNode, node2: BaseSchedulerNode, outer_loop_fusion_depth): ...
    outer_fused_nodes: list[FusedSchedulerNode | SchedulerNode]
    outer_loop_fusion_depth: Incomplete
    def __init__(self, scheduler: Scheduler, outer_fused_nodes: list[FusedSchedulerNode | SchedulerNode], outer_loop_fusion_depth) -> None: ...
    def get_outer_nodes(self): ...
    def check_outer_fusion_loop_level_attr(self, cpp_kernel_proxy_list, outer_loop_fusion_depth): ...
    def merge_outer_fusion_kernels(self, cpp_kernel_proxy_list): ...

class RecordOptimizationContext:
    func_name: Incomplete
    current_node: torch.fx.Node | None
    opt_ctx: OptimizationContext | None
    def __init__(self, func_name: str = '') -> None: ...
    def __enter__(self): ...
    def __exit__(self, exc_type: type[BaseException] | None, exc_val: BaseException | None, exc_tb: types.TracebackType | None) -> None: ...
    def get_opt_ctx(self): ...
    def get_fx_node(self): ...

def decltype_promoted(*args): ...

class CppOverrides(OpOverrides):
    """Map element-wise ops to C++"""
    @staticmethod
    def add(a, b): ...
    @staticmethod
    def sub(a, b): ...
    @staticmethod
    def mul(a, b): ...
    @staticmethod
    def to_dtype(x, dtype, src_dtype=None, use_compute_types: bool = True): ...
    @staticmethod
    def to_dtype_bitcast(x, dtype, src_dtype): ...
    @staticmethod
    def abs(x): ...
    @staticmethod
    def sin(x): ...
    @staticmethod
    def cos(x): ...
    @staticmethod
    def neg(x): ...
    @staticmethod
    def exp(x): ...
    @staticmethod
    def exp2(x): ...
    @staticmethod
    def expm1(x): ...
    @staticmethod
    def erf(x): ...
    @staticmethod
    def erfc(x): ...
    @staticmethod
    def erfinv(x): ...
    @staticmethod
    def sqrt(x): ...
    @staticmethod
    def rsqrt(x): ...
    @staticmethod
    def log1p(x): ...
    @staticmethod
    def tan(x): ...
    @staticmethod
    def tanh(x): ...
    @staticmethod
    def signbit(x):
        """
        On windows std::signbit only support float type.
        Ref: https://learn.microsoft.com/en-us/cpp/c-runtime-library/reference/signbit?view=msvc-170
        """
    @staticmethod
    def pow(a, b): ...
    @staticmethod
    def log(x): ...
    @staticmethod
    def round(x): ...
    @staticmethod
    def floor(x): ...
    @staticmethod
    def floordiv(a, b): ...
    @staticmethod
    def ceil(x): ...
    @staticmethod
    def trunc(x): ...
    @staticmethod
    def truncdiv(a, b): ...
    @staticmethod
    def fmod(a, b): ...
    @staticmethod
    def isinf(x): ...
    @staticmethod
    def isnan(x): ...
    @staticmethod
    def lgamma(x): ...
    @staticmethod
    def acos(x): ...
    @staticmethod
    def acosh(x): ...
    @staticmethod
    def cosh(x): ...
    @staticmethod
    def sinh(x): ...
    @staticmethod
    def asin(x): ...
    @staticmethod
    def asinh(x): ...
    @staticmethod
    def atan2(x, y): ...
    @staticmethod
    def atan(x): ...
    @staticmethod
    def atanh(x): ...
    @staticmethod
    def copysign(x, y): ...
    @staticmethod
    def frexp(x): ...
    @staticmethod
    def hypot(x, y): ...
    @staticmethod
    def log10(x): ...
    @staticmethod
    def log2(x): ...
    @staticmethod
    def nextafter(x, y): ...
    @staticmethod
    def relu(x): ...
    @staticmethod
    def minimum(a, b): ...
    @staticmethod
    def maximum(a, b): ...
    @staticmethod
    def where(a, b, c): ...
    @staticmethod
    def mod(a, b): ...
    @staticmethod
    def constant(val, dtype): ...
    @staticmethod
    def index_expr(expr, dtype): ...
    @staticmethod
    def masked(mask, body, other): ...
    @staticmethod
    def logical_and(a, b): ...
    @staticmethod
    def logical_not(a): ...
    @staticmethod
    def logical_or(a, b): ...
    @staticmethod
    def logical_xor(a, b): ...
    @staticmethod
    def bitwise_and(a, b): ...
    @staticmethod
    def bitwise_not(a): ...
    @staticmethod
    def bitwise_or(a, b): ...
    @staticmethod
    def bitwise_xor(a, b): ...
    @staticmethod
    def bitwise_left_shift(a, b): ...
    @staticmethod
    def bitwise_right_shift(a, b): ...
    @staticmethod
    def rand(seed: sympy.Expr, offset: sympy.Expr): ...
    @staticmethod
    def randn(seed: sympy.Expr, offset: sympy.Expr): ...
    @staticmethod
    def randint64(seed: sympy.Expr, offset: sympy.Expr, low, high): ...
    @staticmethod
    def sigmoid(x): ...
    @staticmethod
    def sign(x): ...

class CppVecOverrides(CppOverrides):
    """Map element-wise ops to aten vectorization C++"""
    def __new__(cls, *args, **kargs): ...
    @staticmethod
    def add(a, b): ...
    @staticmethod
    def sub(a, b): ...
    @staticmethod
    def mul(a, b): ...
    @staticmethod
    def truediv(a, b): ...
    @staticmethod
    def abs(x): ...
    @staticmethod
    def sin(x): ...
    @staticmethod
    def cos(x): ...
    @staticmethod
    def exp(x): ...
    @staticmethod
    def exp2(x): ...
    @staticmethod
    def expm1(x): ...
    @staticmethod
    def erf(x): ...
    @staticmethod
    def erfc(x): ...
    @staticmethod
    def erfinv(x): ...
    @staticmethod
    def sqrt(x): ...
    @staticmethod
    def eq(x, y): ...
    @staticmethod
    def ne(x, y): ...
    @staticmethod
    def lt(x, y): ...
    @staticmethod
    def gt(x, y): ...
    @staticmethod
    def le(x, y): ...
    @staticmethod
    def ge(x, y): ...
    @staticmethod
    def and_(x, y): ...
    @staticmethod
    def rsqrt(x): ...
    @staticmethod
    def pow(a, b): ...
    @staticmethod
    def log(x): ...
    @staticmethod
    def round(x): ...
    @staticmethod
    def floor(x): ...
    @staticmethod
    def ceil(x): ...
    @staticmethod
    def trunc(x): ...
    @staticmethod
    def fmod(a, b): ...
    @staticmethod
    def lgamma(x): ...
    @staticmethod
    def logical_and(a, b): ...
    @staticmethod
    def logical_not(a): ...
    @staticmethod
    def logical_or(a, b): ...
    @staticmethod
    def logical_xor(a, b): ...
    @staticmethod
    def bitwise_and(a, b): ...
    @staticmethod
    def bitwise_not(a): ...
    @staticmethod
    def bitwise_or(a, b): ...
    @staticmethod
    def bitwise_xor(a, b): ...
    @staticmethod
    def bitwise_left_shift(a, b): ...
    @staticmethod
    def bitwise_right_shift(a, b): ...
    @staticmethod
    def load_seed(name, offset): ...
    @staticmethod
    def rand(seed, offset): ...
    @staticmethod
    def randn(seed, offset): ...
    @staticmethod
    def randint64(seed, offset, low, high): ...
    @staticmethod
    def remainder(a, b): ...
    @staticmethod
    def tan(a): ...
    @staticmethod
    def tanh(a): ...
    @staticmethod
    def reciprocal(a): ...
    @staticmethod
    def atan(x): ...
    @staticmethod
    def acos(x): ...
    @staticmethod
    def asin(x): ...
    @staticmethod
    def cosh(x): ...
    @staticmethod
    def sinh(x): ...
    @staticmethod
    def log10(x): ...
    @staticmethod
    def log2(x): ...
    @staticmethod
    def nextafter(x, y): ...
    @staticmethod
    def copysign(a, b): ...
    @staticmethod
    def atan2(a, b): ...
    @staticmethod
    def hypot(a, b): ...
    @staticmethod
    def atanh(x): ...
    @staticmethod
    def asinh(x): ...
    @staticmethod
    def acosh(x): ...
    @staticmethod
    def relu(x): ...
    @staticmethod
    def sigmoid(x): ...
    @staticmethod
    def neg(x): ...
    @staticmethod
    def floordiv(a, b): ...
    @staticmethod
    def truncdiv(a, b): ...
    @staticmethod
    def minimum(a, b): ...
    @staticmethod
    def maximum(a, b): ...
    @staticmethod
    def square(a): ...
    @staticmethod
    def where(a, b, c): ...
    @staticmethod
    def sign(x): ...
    @staticmethod
    def to_dtype(x, dtype, src_dtype=None, use_compute_dtypes: bool = True): ...
    @staticmethod
    def log1p(x): ...
    @staticmethod
    def masked(mask, body, other): ...
    @staticmethod
    def index_expr(expr, dtype): ...
    @staticmethod
    def frexp(x): ...
    @classmethod
    def _scalarize(cls, scalar_func): ...
    @classmethod
    def _initialize_scalarize(cls) -> None: ...

class CppTile2DOverrides(CppVecOverrides):
    @staticmethod
    def index_expr(expr, dtype): ...

class CppKernel(Kernel):
    overrides = CppOverrides
    sexpr = cexpr
    newvar_prefix: str
    suffix: str
    active_ranges: dict[sympy.Expr, tuple[sympy.Expr, ...]]
    inner_itervars: list[sympy.Symbol]
    call_ranges: tuple[sympy.Expr, ...] | None
    ranges: list[sympy.Expr]
    itervars: list[sympy.Symbol]
    reduction_depth: Incomplete
    reduction_prefix: Incomplete
    reduction_prefix_generators: list[Callable]
    reduction_suffix: Incomplete
    parallel_reduction_prefix: Incomplete
    parallel_reduction_suffix: Incomplete
    local_reduction_init: Incomplete
    local_reduction_stores: Incomplete
    is_reduction: bool
    non_parallel_reduction_prefix: Incomplete
    non_parallel_reduction_suffix: Incomplete
    reduction_cse: Incomplete
    welford_helper_cse: Incomplete
    preloads: Incomplete
    poststores: Incomplete
    num_threads: Incomplete
    reduction_omp_dec: dict[tuple[str, str], str]
    reduction_var_names: list[str]
    def __init__(self, args, num_threads) -> None: ...
    def _gen_parallel_reduction_buffers(self, acc, acc_type, reduction_type, dtype, reduction_combine_fn=..., reduction_init_fn=...) -> None: ...
    def update_stores_with_parallel_reduction(self) -> None: ...
    def gen_body(self, code: BracesBuffer | None = None): ...
    _load_mask: Incomplete
    @contextlib.contextmanager
    def masked(self, mask) -> Generator[Incomplete]:
        """Context manager to add an additional mask to loads and stores."""
    def scale_index_with_offset(self, index: sympy.Expr, scale: int = 1, itervar_idx: int = -1, offset: int = 0): ...
    def index_to_str(self, index: sympy.Expr) -> str:
        '''
        Convert an index expr to a string that can be used in cpp code.
        e.g. a sympy expression "s2" may actually appear as "ks1" in the cpp kernel.
        '''
    def index_indirect_depends_on(self, index: sympy.Expr, itervar: sympy.Symbol):
        """
        Check if an index has free symbol CppCSEVariable that depends on `itervar`.
        """
    def index_depends_on(self, index: sympy.Expr, itervar: sympy.Symbol): ...
    def var_ranges(self): ...
    def check_bounds(self, expr: sympy.Expr, size: sympy.Expr, lower: bool, upper: bool): ...
    def load(self, name: str, index: sympy.Expr): ...
    def store(self, name, index, value, mode=None) -> None: ...
    def _gen_reduction_prefix(self, acc: CSEVariable | str, acc_type: str, rtype: str, dtype: torch.dtype, init_fn): ...
    def finalize_reduction_prefix(self, size: int | None = None): ...
    def reduction(self, dtype, src_dtype, reduction_type, value): ...
    def store_reduction(self, name, index, value) -> None: ...
    def set_ranges(self, lengths, reduction_lengths): ...
    def size_hint(self): ...
    def codegen_loops_impl(self, loop_nest, code, worksharing): ...
    def codegen_loops(self, code, worksharing) -> None: ...
    @property
    def assert_function(self) -> str: ...
    def decide_parallel_depth(self, max_parallel_depth, threads): ...
    loads: Incomplete
    compute: Incomplete
    stores: Incomplete
    cse: Incomplete
    @contextlib.contextmanager
    def write_to_suffix(self) -> Generator[None]: ...
    def create_cse_var(self, *args, **kwargs): ...
    def get_to_dtype_expr(self, src, dtype, src_dtype): ...
    def cache_dtype_convert(self, dst, dst_dtype, src, src_dtype) -> None: ...
    def codegen_conditions(self, code: BracesBuffer, prefix: str | None = None, var: sympy.Symbol | None = None): ...

class CppVecKernel(CppKernel):
    overrides = CppVecOverrides
    vec_isa: Incomplete
    tiling_factor: Incomplete
    tiling_idx: Incomplete
    tail_size: Incomplete
    num_elems: Incomplete
    def __init__(self, args, num_threads, tiling_factor, tiling_idx, tail_size=None) -> None: ...
    def _try_get_const_stride(self, index: sympy.Expr, itervar: sympy.Symbol): ...
    def _get_num_vectors(self, dtype: torch.dtype) -> int: ...
    def _get_raw_num_vectors(self, dtype: torch.dtype) -> float: ...
    def _get_vec_type(self, dtype: torch.dtype) -> str: ...
    def _get_mask_type(self, dtype: torch.dtype = ...) -> str: ...
    def _get_mask_cast(self, mask: CppCSEVariable, dtype: torch.dtype) -> str: ...
    def _get_vec_load_line(self, var: str, index: sympy.Expr, dtype: torch.dtype, load_mask: CppCSEVariable | None = None):
        """
        Get a load line str that loads a vector from `var` at `index` of type `dtype`.
        If `load_mask` is not None, we do a masked load accordingly.
        Notes on the `dtype`:
        1. We always load `self.tiling_factor` number of elements regardless of the `dtype`.
           It means we load half of the vector lanes for 16-bit data types and quarter of the
           vector lanes for 8-bit data types.
        2. `torch.bool` and `torch.uint8` could mean masks and we load them as float mask vectors.
        """
    def _load_or_store_non_contiguous(self, var: str | None, index: sympy.Expr, dtype: torch.dtype, buffer: IndentedBuffer | None = None, store_value: str | CppCSEVariable | None = None, accu_store: bool = False) -> CppCSEVariable | None:
        """
        Load or store a vector in a non-contiguous way. The vector is initialized from an array that is
        filled in an inner loop over the tiling factor.
        :param var: buffer to load from or store to, i.e. `var[transformed(index)]`. If None, we load the index
                    as index expression, i.e. `transformed(index)`.
        :param index: index into the `var` or the index expression by its own if `var` is None.
                      The `index` could contain indirect indexing or the tiling itervar. When used in
                      the inner loop, the index is transformed as follows:
                      1. the index is linearized along the tiling dim.
                      2. the indirect indexing vector variables are transformed into arrays over the tiling dim.
        :param dtype: data type of `var` or `index` if `var` is None.
        :param buffer: the code buffer to write the generated code to. If None, we write to `self.loads`.
        :param store_value: the value to store. If None, we load the vector.
        :param accu_store: whether accumulate the store_value to store_ptr. If True, a store_value should be provided
        :return: a CppCSEVariable that represents the loaded vector or None if it is a store.
        """
    def load(self, name: str, index: sympy.Expr): ...
    def _get_store_line(self, value: str | CppCSEVariable, var: str, index: sympy.Expr, dtype: torch.dtype, accu_store: bool = False):
        """
        Get a store line buffer that stores `value` into `var` at `index` of `dtype`. It handles
        both contiguous and non-contiguous store cases.
        :param value: Vectorized type templaterized on `dtype`.
        :param var: buffer to store into.
        :index: index into the `var`.
        """
    def store(self, name, index, value, mode=None): ...
    is_reduction: bool
    def reduction(self, dtype, src_dtype, reduction_type, value): ...
    def store_reduction(self, name, index, value): ...
    def broadcast(self, scalar_var: CppCSEVariable) -> CppCSEVariable: ...
    def arange(self, index: CppCSEVariable, stride: sympy.Symbol) -> CppCSEVariable: ...
    def reduction_init_vec(self, reduction_type, dtype): ...
    def reduction_acc_type_vec(self, reduction_type, dtype): ...
    def _welford_helper_init(self, welford_helper_val, welford_helper_vec_range, dtype, num_threads=None): ...
    def _use_welford_helper(self, acc_vec, welford_helper_val, welford_helper_vec_range, dtype) -> None: ...
    def reduction_combine_vec(self, reduction_type, var, next_value, welford_helper_val=None, index: sympy.Symbol | None = None, horizontal_reduction: bool | None = None, src_dtype: torch.dtype | None = ...): ...
    def indirect_assert(self, var, lower, upper, mask=None): ...
    def get_to_dtype_expr(self, src, dtype, src_dtype): ...

class CppTile2DKernel(CppVecKernel):
    """
    A vector kernel that handles the 2d tiles with the tile size defined in `tiling_factor` on
    the inner-most loop level and one of the outer loop level (`outer_tiling_idx`). When the data
    tile is accessed in a contiguous way from the outer loop axis, a transposition is applied on the
    tile to make the access contiguous from the inner-most loop axis. Then, the same vectorization
    logic from its parent `CppVecKernel` is leveraged for load/store/compute. The transposed tile load
    and store are generated into kernel.preloads and kernel.poststores buffers.

    The loop structure looks like below:
    for ...
      for i_outer ...
        for ...
          for inner_most ...
            // generated by CppTile2DKernel
            float tmp0[16*16]; at::vec::transpose_mxn<...>(tmp0, in_ptr0 + ..., ...); // into kernel.preloads
            float tmp1[16*16]; // into kernel.preloads
            for i_inner ... { // the kernel inner loop
              vectorized loads/compute/stores (e.g., load tmp0, store tmp1) // into kernel.loads/compute/stores
            }
            at::vec::transpose_mxn(out_ptr0 + ..., tmp1, ...) // into kernel.poststores
          for inner_most ... (tail)
            // generated by CppVecKernel
            ...
      for i_outer ... (tail)
        for ...
          for ...
            // generated by CppKernel
            ...
    """
    overrides = CppTile2DOverrides
    tiling_indices: Incomplete
    inner_tail_size: Incomplete
    outer_tail_size: Incomplete
    inner_num_elems: Incomplete
    outer_num_elems: Incomplete
    inner_is_tiling_idx: bool
    def __init__(self, args, num_threads, tiling_factor, tiling_indices, inner_tail_size=None, outer_tail_size=None) -> None: ...
    def inner_itervar(self): ...
    def need_vec_transpose(self, index): ...
    def gen_transposed_tile_load_store(self, name, var, index, is_store, store_mode=None): ...
    def load(self, name: str, index: sympy.Expr): ...
    def store(self, name, index, value, mode=None) -> None: ...
    def codegen_inner_loops(self, code) -> None: ...
    tail_size: Incomplete
    num_elems: Incomplete
    def set_ranges(self, group, reduction_group): ...
    def transform_indexing(self, index: sympy.Expr) -> sympy.Expr: ...

def get_loop_body_lowp_fp(_body: LoopBody) -> tuple[torch.dtype | None, bool]:
    """
    Returns the low precision data type (torch.float16/torch.bfloat16) contained in the nodes
    and if all the nodes can codegen with this data type without converting to float.
    Otherwise returns None and True.
    """

class TilingSelect:
    """
    Implement the heuristic to select the tiling factors and tiling indices.
    In the future, we can implement advanced heuristic in a subclass.
    """
    def __init__(self) -> None: ...
    def select_tiling(self, fn_list, var_sizes_list) -> tuple[list[int], list[int]]: ...
    def _select_tiling_indices(self, fn_list, var_sizes_list, tiling_factor): ...

class CppKernelProxy(CppKernel):
    kernel_cls: type[CppKernel]
    vec_kernel_cls: type[CppVecKernel]
    tile2d_kernel_cls: type[CppTile2DKernel]
    kernel_group: Incomplete
    loop_nest: Incomplete
    call_ranges: Incomplete
    picked_vec_isa: cpu_vec_isa.VecISA
    kernels: list[CppKernel]
    def __init__(self, kernel_group) -> None: ...
    def data_type_propagation(self, nodes) -> None: ...
    def is_lowp_fp_scheduler(self, scheduler_node: SchedulerNode): ...
    def legalize_lowp_fp_dtype_loopbody(self, loop_body: LoopBody): ...
    def legalize_lowp_fp_dtype(self, nodes) -> None: ...
    def codegen_functions(self, fn_list, var_sizes_list): ...
    def codegen_loop_bodies(self, loop_bodies, var_sizes_list) -> None: ...
    def codegen_nodes(self, nodes: list[SchedulerNode]): ...
    def codegen_loops(self, code, worksharing) -> None: ...
    def update_stores_with_parallel_reduction(self) -> None: ...
    def gen_body(self, code: BracesBuffer | None = None): ...
    reduction_suffix: Incomplete
    def aggregate_reduction_buffers(self, inner_loop_reduction_outer_not: bool, outer_loop: LoopLevel | None): ...

class OuterLoopFusedKernel(CppKernel):
    inner: list[LoopNest]
    def __init__(self, kernel_group) -> None: ...
    def decide_parallel_depth(self, max_parallel_depth, threads): ...

class ReasonFusedNodes(Enum):
    SAME_VARS_REDUCE = 'same_vars_reduce'
    COMPATIBLE_REDUCTION = 'compatible_reduction'
    COMPATIBLE_RANGES_NO_REDUCTION = 'compatible_ranges_no_reduction'

class CppScheduling(BaseScheduling):
    kernel_proxy_cls: type[CppKernelProxy]
    MAX_FUSED_KERNEL_ARGS_NUM: int
    backend_features: Incomplete
    @classmethod
    def get_backend_features(cls, device: torch.device) -> OrderedSet[BackendFeature]: ...
    _ready_to_flush: bool
    def __init__(self, scheduler) -> None: ...
    def _set_flush_status(self, status: bool): ...
    def group_fn(self, sizes): ...
    kernel_group: Incomplete
    def reset_kernel_group(self) -> None: ...
    def fuse(self, node1, node2): ...
    def _why_fuse_nodes(self, node1, node2) -> ReasonFusedNodes | None: ...
    def _can_fuse_nodes_with_compatible_ranges(self, node1, node2): ...
    def _can_fuse_horizontal_impl(self, node1, node2): ...
    def can_fuse_horizontal(self, node1, node2): ...
    def can_fuse_multi_outputs_template(self, node1: BaseSchedulerNode, node2: BaseSchedulerNode) -> bool: ...
    def _get_outer_loop_fusion_depth(self, node1, node2): ...
    def can_fuse_vertical_outer_loop(self, node1, node2): ...
    def get_fusion_pair_priority(self, node1, node2): ...
    def can_fuse_vertical(self, node1, node2): ...
    def try_loop_split(self, nodes: list[SchedulerNode]):
        """
        Apply loop split optimization.
        When one of the indexing_exprs contains a division, we eliminate the division by splitting the loop
        to avoid non-contiguous loads, subject to the following conditions:
            1. No reduction and no mudular index for all nodes.
            2. The indexing_exprs of all nodes contain only one (or more, but all the same) division,
               where the divisor is an integer and not too small (the divisor > 8), the dividend is
               one of the iter_vars, and this var, i.e. the dimension that needs to be split, is
               contiguous in all other indexing_exprs.

        For example, if the node's var_ranges: {z0: 2, z1: 9216, z2: 960} and indexing_exprs:
        {'index0': 8847360*z0 + 960*z1 + z2, 'index1': 32*z0 + (z2//30), 'index2': z2},
        we will split z2 -> 30*z2 + z3, then the node's var_ranges will be changed to
        {z0: 2, z1: 9216, z2: 32, z3: 30} and indexing_exprs will be changed to
        {'index0': 8847360*z0 + 960*z1 + 30*z2 + z3, 'index1': 32*z0 + z2, 'index2': 30*z2 + z3}.
        """
    def codegen_outer_loop_node(self, node: OuterLoopFusedSchedulerNode):
        """
        Generate the code for the outer loop fused scheduler node.
        1. Codegen with fused outer loop: depends on the analysis of
            the outer loop fused scheduler node, with or without the local buffer.
        2. If failed, fallback to standard codegen.
        """
    def codegen_node(self, node: OuterLoopFusedSchedulerNode | FusedSchedulerNode | SchedulerNode):
        """
        Turn an set of pre-fused nodes into a C++ kernel.
        """
    def is_cpp_template(self, node: BaseSchedulerNode) -> bool: ...
    def codegen_template(self, template_node: BaseSchedulerNode, epilogue_nodes: Sequence[BaseSchedulerNode], prologue_nodes: Sequence[BaseSchedulerNode]):
        """
        Codegen a CPP template, possibly with fused epilogues
        """
    def _get_scheduled_num_args(self): ...
    def ready_to_flush(self): ...
    def codegen_sync(self) -> None: ...
    def define_kernel(self, src_code, nodes, kernel_args=None): ...
    def flush(self) -> None: ...

class KernelGroup:
    args: Incomplete
    loops_code: Incomplete
    ws: Incomplete
    stack: Incomplete
    scheduled_nodes: Incomplete
    def __init__(self) -> None: ...
    def new_kernel(self, cls, *args): ...
    def finalize_kernel(self, new_kernel, nodes) -> None: ...
    def get_num_args(self): ...
    def codegen_group(self, name=None) -> str: ...
    def call_kernel(self, wrapper, kernel_name) -> None: ...

class WorkSharing:
    code: Incomplete
    in_parallel: bool
    num_threads: Incomplete
    stack: Incomplete
    def __init__(self, code) -> None: ...
    def parallel(self, threads) -> None: ...
    def single(self): ...
    def close(self) -> None: ...
    def __enter__(self): ...
    def __exit__(self, exc_type: type[BaseException] | None, exc_val: BaseException | None, exc_tb: types.TracebackType | None) -> None: ...

@dataclasses.dataclass
class LoopLevel:
    var: sympy.Expr | None = ...
    size: sympy.Expr | None = ...
    offset: sympy.Expr = ...
    tiled_size: sympy.Expr = ...
    steps: sympy.Expr = ...
    parallel: int = ...
    simd_omp: bool = ...
    simd_vec: bool = ...
    collapsed: bool = ...
    is_reduction: bool = ...
    simd_nelements: int = ...
    def __post_init__(self) -> None: ...
    def tile(self, factor): ...
    def lines(self): ...

@dataclasses.dataclass
class LoopNest:
    """
    A loop-nest-like structure. It is built with the `build` method
    as a loop nest and then will perform loop-tiling at some depth.

    A typical case is for vectorization, where we typically do loop-tiling
    at the innermost loop level. A more complicated case is when we do
    2D tiling at both the innermost and outer levels.
    """
    loops: list[LoopLevel] | None = ...
    kernel: CppKernel | None = ...
    @staticmethod
    def build(kernel: CppKernel):
        """Build a LoopNest with the given `kernel` as the leaf"""
    def __bool__(self) -> bool: ...
    @cache_on_self
    def max_parallel_depth(self):
        """
        Maximal allowed depth for parallelism: All reduction or non-reduction levels.
        When the range of the first inner loop beyond the maximum parallel depth is much
        larger than the range of all outer loops within the maximum parallel depth,
        change the starting depth of parallelism to the first inner loop and recalculate
        the maximum parallel depth.
        """
    def mark_parallel(self, par_depth) -> None: ...
    def tile(self, depth, factor):
        """
        Do loop-tiling at the `depth` level with `factor`.
            for (x0 = 0; x0 < x0_end; x0++)
            ->
            for (x0 = 0; x0 < x0_end; x0 += factor)
        See details in Note [tiled_size].
        """
    def get_kernel(self) -> CppKernel: ...
    def set_kernel(self, kernel) -> None: ...
    def from_loop_level(self, level: int): ...
