import dataclasses
import sympy
import torch
from .. import config as config, ir as ir
from ..._prims_common import is_integer_dtype as is_integer_dtype
from ...utils._ordered_set import OrderedSet as OrderedSet
from ...utils._sympy.functions import FloorDiv as FloorDiv, ModularIndexing as ModularIndexing
from ...utils._sympy.symbol import SymT as SymT, symbol_is_type as symbol_is_type
from ...utils._sympy.value_ranges import ValueRanges as ValueRanges
from ..codecache import HalideCodeCache as HalideCodeCache
from ..ir import get_reduction_combine_fn as get_reduction_combine_fn
from ..metrics import is_metric_table_enabled as is_metric_table_enabled, log_kernel_metadata as log_kernel_metadata
from ..ops_handler import AddParenHandler as AddParenHandler, ReductionType as ReductionType, StoreMode as StoreMode
from ..runtime.hints import HalideInputSpec as HalideInputSpec, HalideMeta as HalideMeta
from ..utils import get_bounds_index_expr as get_bounds_index_expr, get_kernel_metadata as get_kernel_metadata, parallel_num_threads as parallel_num_threads, sympy_index_symbol as sympy_index_symbol, sympy_subs as sympy_subs
from ..virtualized import V as V
from .common import BackendFeature as BackendFeature, CSEVariable as CSEVariable, DeferredLine as DeferredLine, IndentedBuffer as IndentedBuffer, KernelArgType as KernelArgType, OpOverrides as OpOverrides, PythonPrinter as PythonPrinter, SizeArg as SizeArg, TensorArg as TensorArg
from .cpp import DTYPE_TO_CPP as DTYPE_TO_CPP
from .cpp_utils import cexpr as cexpr
from .simd import SIMDKernel as SIMDKernel, SIMDScheduling as SIMDScheduling, constant_repr as constant_repr
from _typeshed import Incomplete
from collections.abc import Sequence
from typing import Any, Callable

log: Incomplete

def halide_constant(val): ...

class Unsupported(RuntimeError):
    def __init__(self, thing) -> None: ...

class HalidePrinter(PythonPrinter):
    @staticmethod
    def cast_index(expr): ...
    @staticmethod
    def cast_float(expr): ...
    def _print_Float(self, expr): ...
    def _print_ToFloat(self, expr): ...
    def _print_floor(self, expr): ...
    _print_FloorToInt = _print_floor
    def _print_Trunc(self, expr): ...
    _print_TruncToInt = _print_Trunc
    def _print_ceiling(self, expr): ...
    def _helper_sqrt(self, expr): ...
    def _print_Where(self, expr): ...
    def _print_Min(self, expr): ...
    def _print_Max(self, expr): ...
    def _print_Abs(self, expr): ...
    def _print_OpaqueUnaryFn_cos(self, expr): ...
    def _print_OpaqueUnaryFn_cosh(self, expr): ...
    def _print_OpaqueUnaryFn_acos(self, expr): ...
    def _print_OpaqueUnaryFn_sin(self, expr): ...
    def _print_OpaqueUnaryFn_sinh(self, expr): ...
    def _print_OpaqueUnaryFn_asin(self, expr): ...
    def _print_OpaqueUnaryFn_tan(self, expr): ...
    def _print_OpaqueUnaryFn_tanh(self, expr): ...
    def _print_OpaqueUnaryFn_atan(self, expr): ...
    def _print_OpaqueUnaryFn_log2(self, expr) -> None: ...
    def _print_FloorDiv(self, expr): ...
    def _print_Round(self, expr): ...
    _print_RoundToInt = _print_Round
    def _print_IntTrueDiv(self, expr): ...
    def _print_RoundDecimal(self, expr): ...

texpr: Incomplete
pexpr: Incomplete
_halide_type: Incomplete

def halide_type(dtype): ...
def halide_acc_type(dtype): ...

class HalideOverrides(OpOverrides):
    @staticmethod
    def to_dtype(x, dtype: torch.dtype, src_dtype: torch.dtype | None = None, use_compute_types: bool = True): ...
    @staticmethod
    def to_dtype_bitcast(x, dtype: torch.dtype, src_dtype: torch.dtype): ...
    @classmethod
    def constant(cls, value, dtype): ...
    @staticmethod
    def abs(x): ...
    @staticmethod
    def exp(x): ...
    @staticmethod
    def sqrt(x): ...
    @staticmethod
    def minimum(a, b): ...
    @staticmethod
    def maximum(a, b): ...
    @staticmethod
    def where(a, b, c): ...
    @staticmethod
    def cos(x): ...
    @staticmethod
    def sin(x): ...
    @staticmethod
    def lgamma(x) -> None: ...
    @staticmethod
    def erf(x): ...
    @staticmethod
    def cosh(x): ...
    @staticmethod
    def sinh(x): ...
    @staticmethod
    def acos(x): ...
    @staticmethod
    def acosh(x): ...
    @staticmethod
    def asin(x): ...
    @staticmethod
    def asinh(x): ...
    @staticmethod
    def atan2(x, y): ...
    @staticmethod
    def atan(x): ...
    @staticmethod
    def atanh(x): ...
    @staticmethod
    def copysign(x, y) -> None: ...
    @staticmethod
    def erfinv(x) -> None: ...
    @staticmethod
    def hypot(x, y): ...
    @staticmethod
    def nextafter(x, y) -> None: ...
    @staticmethod
    def logical_and(a, b): ...
    @staticmethod
    def logical_not(a): ...
    @staticmethod
    def logical_or(a, b): ...
    @staticmethod
    def logical_xor(a, b): ...
    @staticmethod
    def bitwise_and(a, b): ...
    @staticmethod
    def bitwise_not(a): ...
    @staticmethod
    def bitwise_or(a, b): ...
    @staticmethod
    def bitwise_xor(a, b): ...
    @staticmethod
    def bitwise_left_shift(a, b): ...
    @staticmethod
    def bitwise_right_shift(a, b): ...
    @staticmethod
    def rand(seed, offset): ...
    @staticmethod
    def randn(seed, offset): ...
    @staticmethod
    def randint64(seed, offset, low, high): ...
    @staticmethod
    def load_seed(name, offset): ...
    @staticmethod
    def rsqrt(x): ...
    @staticmethod
    def tan(x): ...
    @staticmethod
    def tanh(x): ...
    @staticmethod
    def signbit(x): ...
    @staticmethod
    def fmod(a, b): ...
    @staticmethod
    def pow(a, b): ...
    @staticmethod
    def log(x): ...
    @staticmethod
    def log2(x) -> None: ...
    @staticmethod
    def isinf(x): ...
    @staticmethod
    def isnan(x): ...
    @staticmethod
    def round(x): ...
    @staticmethod
    def floor(x): ...
    @staticmethod
    def int_truediv(a, b): ...
    @staticmethod
    def floordiv(a, b): ...
    @classmethod
    def sign(cls, x): ...
    @staticmethod
    def trunc(x): ...
    @staticmethod
    def truncdiv(a, b): ...
    @staticmethod
    def ceil(x): ...
    @staticmethod
    def relu(x): ...
    @classmethod
    def index_expr(cls, expr, dtype): ...
    @classmethod
    def indirect_indexing(cls, index_var, size, check: bool = True, wrap_neg: bool = True): ...
    @classmethod
    def halide_clamp(cls, value, size, check): ...
    @staticmethod
    def masked(mask, body, other): ...
    @staticmethod
    def frexp(x) -> None: ...

class HalideCSEVariable(CSEVariable):
    undefined_re: Incomplete
    used_dims: list[sympy.Symbol] | None
    def __init__(self, name, bounds: ValueRanges[Any], dtype: torch.dtype | None = None) -> None: ...
    def update_on_args(self, name, args, kwargs) -> None: ...
    def index_str(self, dims): ...
    def __str__(self) -> str: ...
    def subs_str(self, replacements): ...

@dataclasses.dataclass
class DimensionInfo:
    expr: sympy.Expr | None
    size: sympy.Expr
    stride: sympy.Expr
    def __init__(self, expr, size, stride) -> None: ...
    def index_str(self, replacements=None, zero_vars: bool = False): ...

def eq(left, right): ...
def lt(left, right): ...

class HalideKernel(SIMDKernel):
    overrides = HalideOverrides
    kexpr: Callable[[sympy.Expr], str]
    compute: Incomplete
    loads: Incomplete
    stores: Incomplete
    indexing_code_dom: Incomplete
    needs_dom_indexing: Incomplete
    has_reduction: Incomplete
    buffer_dimensions: dict[str, list[DimensionInfo]]
    buffer_offsets: dict[str, sympy.Expr]
    halide_vars: dict[sympy.Symbol, sympy.Expr]
    index_replacements: dict[sympy.Expr, sympy.Expr]
    reduction_renames: dict[sympy.Symbol, sympy.Symbol]
    dom_renames: dict[str, dict[sympy.Symbol, sympy.Symbol]]
    buffer_aliases: dict[str, list[str]]
    has_indirect_indexing: bool
    def __init__(self, tiling: dict[str, sympy.Expr], **kwargs) -> None: ...
    def dtype_to_str(self, dtype: torch.dtype) -> str: ...
    def create_cse_var(self, name, bounds=None, dtype=None): ...
    def finalize_indexing(self, indices: Sequence[sympy.Expr]):
        """
        Hook called right before codegen with every index that will be
        used in the fused kernel.

        This populates self.halide_vars/index_replacements/reduction_renames which is an alternate indexing
        scheme that avoids using divide and modulus.  Instead of xindex/yindex/rindex
        we base indexing on a larger number of vars whose product combines to those.

        This function populates self.halide_vars, self.index_replacements, and self.reduction_renames
        """
    def setup_dom_indexing(self):
        """RDom based indexing uses explicit iteration ranges for Func updates"""
    def codegen_rdom(self, name, vars) -> None: ...
    def prepare_indexing(self, index: sympy.Expr): ...
    def sym_size(self, sym):
        """The size of an index symbol"""
    def indexing_to_dimensions(self, var: str, index: sympy.Expr, is_store: bool):
        """Convert address-based indexing into dimensions using self.halide_vars"""
    def install_dims(self, var, dims, offset, is_store):
        """Try to set self.buffer_dimensions[var], return True on success"""
    def apply_offset_to_dimension(self, dims, offset) -> None: ...
    def used_dims_from_index(self, index: sympy.Expr):
        """Detect which range trees are used to populate HalideCSEVariable.used_dims"""
    def sort_used_dims(self, used_dims): ...
    def make_index_str(self, dims, replacements=None, zero_vars: bool = False): ...
    def load(self, name: str, index: sympy.Expr):
        """Codegen a load from an InputBuffer"""
    def lookup_cse_var(self, name: str): ...
    def store(self, name: str, index: sympy.Expr, value: CSEVariable, mode: StoreMode = None) -> None:
        """Codegen a store to an OutputBuffer"""
    def reduction(self, dtype: torch.dtype, src_dtype: torch.dtype, reduction_type: ReductionType, value: CSEVariable | tuple[CSEVariable, ...]) -> CSEVariable | tuple[CSEVariable, ...]:
        """Codegen a reduction operation"""
    def welford_combine_impl(self, mean, m2, weight): ...
    def scan(self, dtypes: tuple[torch.dtype, ...], combine_fn: Callable[[tuple[CSEVariable, ...], tuple[CSEVariable, ...]], tuple[CSEVariable, ...]], values_orig: tuple[CSEVariable, ...]) -> tuple[CSEVariable, ...]: ...
    def genfunc(self, line, used_dims, *, bounds=...) -> HalideCSEVariable: ...
    def newfunc(self, used_dims) -> HalideCSEVariable: ...
    def halide_buffer_numel(self, name: str):
        """
        We map all tensors to 1D buffers in Halide since Halide has trouble representing some strides that PyTorch
        supports.  If there are gaps in the underlying layout the numel we pass to Halide includes the gaps while
        PyTorch's numel excludes them.
        """
    def halide_argdefs(self):
        """
        Halide requires scalar inputs before outputs, so need to reorder args.
        """
    def halide_kernel_meta(self) -> HalideMeta:
        """Compute metadata required by codecache.py"""
    def codegen_kernel(self, name=None):
        """Called at the end to generate a final kernel string"""
    @staticmethod
    def _autoscheduler_workarounds(n, dims): ...
    def call_kernel(self, name: str, node=None):
        """Codegen a call to this kernel"""
    def generate_assert(self, check): ...
    def check_bounds(self, expr: sympy.Expr, size: sympy.Expr, lower: bool, upper: bool): ...

class HalideScheduling(SIMDScheduling):
    kernel_type = HalideKernel
    @classmethod
    def get_backend_features(cls, device: torch.device) -> OrderedSet[BackendFeature]: ...
    def define_kernel(self, src_code, node_schedule, kernel):
        """Codegen kernel definition to go in output wrapper code"""
