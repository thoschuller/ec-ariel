import contextlib
import functools
import sympy
import torch
import torch.fx
from . import config as config, inductor_prims as inductor_prims, ir as ir, jagged_lowerings as jagged_lowerings, kernel as kernel, mkldnn_lowerings as mkldnn_lowerings, quantized_lowerings as quantized_lowerings, test_operators as test_operators
from .._dynamo.utils import import_submodule as import_submodule
from .codegen.common import BackendFeature as BackendFeature, pointwise_overrides_data as pointwise_overrides_data
from .comm_lowering import register_comm_lowerings as register_comm_lowerings
from .decomposition import decompositions as decompositions, get_decompositions as get_decompositions
from .ir import DtypeView as DtypeView, ExpandView as ExpandView, IRNode as IRNode, IndexingConstant as IndexingConstant, OnlineSoftmaxReduction as OnlineSoftmaxReduction, PermuteView as PermuteView, Pointwise as Pointwise, Reduction as Reduction, SqueezeView as SqueezeView, TensorBox as TensorBox, View as View, is_triton as is_triton, ops_wrapper as ops_wrapper, validate_ir as validate_ir
from .ops_handler import ReductionType as ReductionType
from .utils import ceildiv as ceildiv, decode_device as decode_device, is_dynamic as is_dynamic, is_gpu as is_gpu, is_pointwise_use as is_pointwise_use, is_view as is_view, needs_fallback_due_to_atomic_add_limitations as needs_fallback_due_to_atomic_add_limitations, pad_listlike as pad_listlike, register_op_dtype_propagation_rules as register_op_dtype_propagation_rules, register_op_requires_libdevice_fp64 as register_op_requires_libdevice_fp64, sympy_product as sympy_product, use_scatter_fallback as use_scatter_fallback
from .virtualized import V as V, ops as ops
from _typeshed import Incomplete
from collections.abc import Generator, Iterable, Sequence
from torch._dynamo.utils import counters as counters
from torch._higher_order_ops.associative_scan import associative_scan_op as associative_scan_op
from torch._higher_order_ops.auto_functionalize import auto_functionalized as auto_functionalized
from torch._higher_order_ops.triton_kernel_wrap import triton_kernel_wrapper_mutation as triton_kernel_wrapper_mutation
from torch._prims_common import ELEMENTWISE_TYPE_PROMOTION_KIND as ELEMENTWISE_TYPE_PROMOTION_KIND, Number as Number, canonicalize_dim as canonicalize_dim, canonicalize_dims as canonicalize_dims, check as check, dtype_to_type as dtype_to_type, elementwise_dtypes as elementwise_dtypes, get_computation_dtype as get_computation_dtype, is_boolean_dtype as is_boolean_dtype, is_float_dtype as is_float_dtype, is_integer_dtype as is_integer_dtype
from torch.fx.experimental.sym_node import magic_methods as magic_methods, method_to_operator as method_to_operator
from torch.fx.experimental.symbolic_shapes import free_unbacked_symbols as free_unbacked_symbols
from torch.utils._ordered_set import OrderedSet as OrderedSet
from torch.utils._sympy.functions import CeilDiv as CeilDiv, FloorDiv as FloorDiv, Identity as Identity, ModularIndexing as ModularIndexing
from typing import Any, Callable, TypeVar
from typing_extensions import ParamSpec

_T = TypeVar('_T')
_P = ParamSpec('_P')
FALLBACK_ALLOW_LIST: Incomplete
log: Incomplete
lowerings: dict[Callable[..., Any] | str, Callable[..., Any]]
_maybe_layout_constraints: dict[torch._ops.OpOverload, Callable[..., Any] | None]
fallbacks: Incomplete
aten: Incomplete
tr_c10d: Incomplete
prims: Incomplete
needs_realized_inputs: Incomplete
foreach_ops: Incomplete
inplace_foreach_ops: Incomplete
inplaceable_foreach_ops: dict[torch._ops.OpOverload, torch._ops.OpOverload]
quantized_decomposed: Incomplete

def cur_node_has_non_foreach_users(): ...
def group_foreach_args(arg_pairs: Iterable[tuple[Any, Any] | Any]): ...
def maybe_layout_constraints(fn: Callable[..., Any]) -> Callable[..., Any] | None:
    """Get layout constraints. Returns None if there are no layout constraints."""
def tag_to_layout_constraint(tag): ...
def assert_nyi(cond, msg) -> None: ...
def add_needs_realized_inputs(fn): ...
def add_layout_constraint(fn, constraint) -> None: ...

DTYPE_ID_LOOKUP: Incomplete

def decode_dtype(dtype: int): ...
def is_integer_type(x): ...
def is_boolean_type(x): ...
def get_promoted_dtype(*args, type_promotion_kind: ELEMENTWISE_TYPE_PROMOTION_KIND): ...
def get_overloads(aten_fn): ...
def in_namespace(op, namespace): ...
def transform_args(args: list[Any], kwargs: dict[str, Any], broadcast: bool, type_promotion_kind: ELEMENTWISE_TYPE_PROMOTION_KIND | None, convert_input_to_bool: bool) -> tuple[list[Any], dict[str, Any]]: ...
def _register_foreach_lowering(aten_fn, decomp_fn):
    """
    Add a foreach lowering to lowerings dict.

    Arguments:
        aten_fn: torch.ops.aten.* fn we are lowering
        decomp_fn: alternate implementation on our IR
        broadcast: True to apply broadcasting to tensor inputs
        type_promotion_kind: kind of type promotion applied to tensor inputs, `None` means no type promotion
        convert_input_to_bool: some logical ops require inputs are converted to bool
    """
def _register_lowering(aten_fn, decomp_fn, broadcast, type_promotion_kind: ELEMENTWISE_TYPE_PROMOTION_KIND | None, convert_input_to_bool, lowering_dict):
    """
    Add a lowering to lowerings dict

    Arguments:
        aten_fn: torch.ops.aten.* fn we are lowering
        decomp_fn: alternate implementation on our IR
        broadcast: True to apply broadcasting to tensor inputs
        type_promotion_kind: kind of type promotion applied to tensor inputs, `None` means no type promotion
        convert_input_to_bool: some logical ops require inputs are converted to bool
    """
def register_lowering(aten_fn, broadcast: bool = False, type_promotion_kind: ELEMENTWISE_TYPE_PROMOTION_KIND | None = ..., convert_input_to_bool: bool = False, lowering_dict=...) -> Callable[[Callable[_P, _T]], Callable[_P, _T]]:
    """
    Shim to support decorator syntax.
    """
def broadcast_symbolic_shapes(a, b):
    """
    Broadcasting logic based on symbolic shapes.

    We give the shapes 0 and 1 concrete values, while all other shapes
    are symbolic sympy formulas.
    """
def promote_constants(inputs, override_return_dtype=None, type_promotion_kind=None): ...
def make_pointwise(fn, override_return_dtype=None, override_device=None, override_fn_when_input_bool=None, allow_alpha: bool = False, triton_fallback=None): ...
def make_foreach_pointwise(pw_fn, allow_alpha: bool = False): ...
def to_dtype(x: TensorBox, dtype: torch.dtype, copy: bool = False): ...
def _foreach_map(subgraph, *args, **kwargs):
    """
    This lowers an invocation of foreach_map
    The way this works is that an arbitrary N-arg func is provided by the user, looped over by the
    polyfill with the same semantics as a foreach op (a loop applying an n-ary function to n args)
    and then traced into a subgraph by dynamo.
    This code allows us to inline the subgraph into the main graph lowering using the PontwiseSubgraphLowering.
    The graph outputs represent the vertically fused sequence of ops, and then register_operation_list
    below registers the buffers as horizontally fuseable in the scheduler.
    """
def _convert_element_type(x: TensorBox, dtype: torch.dtype): ...
def to_dtype_bitcast(x: TensorBox, dtype: torch.dtype, *, copy: bool = False): ...
def _view_dtype(x: TensorBox, dtype: torch.dtype): ...
def to_device(x: TensorBox, device: torch.device, *, copy: bool = False, non_blocking: bool = False): ...
def _device_put(x: TensorBox, device: torch.device, non_blocking: bool = False): ...
def register_pointwise(aten_fn, name=None, broadcast: bool = True, type_promotion_kind=..., convert_input_to_bool: bool = False, override_return_dtype=None, override_fn_when_input_bool=None, allow_alpha: bool = False, triton_fallback=None):
    """A pointwise function that maps ops.{name} to inputs"""
def register_frexp():
    """A pointwise function that maps ops.frexp to inputs"""
def register_foreach_pointwise(aten_fn, pointwise_lowering_fn, allow_alpha: bool = False): ...
def where(cond, a, b): ...
def broadcast_tensors(*inputs): ...
def nop(x): ...
def squeeze(x, dim=None): ...
def squeeze_copy(x, dim=None): ...
def squeeze_(x, dim=None): ...
def isinf(x): ...
def isnan(x): ...
def ceil(x): ...
def floor(x): ...
def round(x): ...
def trunc(x): ...
def expand(x, sizes): ...
def broadcast_in_dim(a, shape, broadcast_dimensions): ...
def expand_as(x, y): ...
def repeat(x, repeats): ...
def view(x, sizes): ...
def permute(x, dims): ...
def slice_(x, dim: int = 0, start: int = 0, end=..., step: int = 1, clamp: bool = True): ...
def as_strided(x, size, stride, storage_offset=None): ...
def as_strided_(x, size, stride, storage_offset=None): ...
def as_strided_copy(x, size, stride, storage_offset=None): ...
def pointwise_cat(inputs, dim: int = 0): ...
def quantized_decomposed_quantize_per_channel(input: TensorBox, scales: TensorBox, zero_points: TensorBox, axis: int, quant_min: int, quant_max: int, dtype: torch.dtype) -> TensorBox: ...
def quantized_decomposed_dequantize_per_channel(input: TensorBox, scales: TensorBox, zero_points: TensorBox, axis: int, quant_min: int, quant_max: int, dtype: torch.dtype, *, out_dtype: torch.dtype | None = None) -> TensorBox: ...
def quantized_decomposed_quantize_per_tensor_default(input: TensorBox, scale: float, zero_point: int, quant_min: int, quant_max: int, dtype: torch.dtype) -> TensorBox: ...
def quantized_decomposed_dequantize_per_tensor_default(input: TensorBox, scale: float, zero_point: int, quant_min: int, quant_max: int, dtype: torch.dtype, *, out_dtype: torch.dtype | None = None) -> TensorBox: ...
def quantized_decomposed_quantize_per_tensor_tensor(input: TensorBox, scale: TensorBox, zero_point: TensorBox, quant_min: int, quant_max: int, dtype: torch.dtype) -> TensorBox: ...
def quantized_decomposed_dequantize_per_tensor_tensor(input: TensorBox, scale: TensorBox, zero_point: TensorBox, quant_min: int, quant_max: int, dtype: torch.dtype, *, out_dtype: torch.dtype | None = None) -> TensorBox: ...
def cat(inputs, dim: int = 0): ...
def diagonal(input, offset: int = 0, dim1: int = 0, dim2: int = 1): ...
def diagonal_copy(input, offset: int = 0, dim1: int = 0, dim2: int = 1): ...
def diagonal_scatter(input, src, offset: int = 0, dim1: int = 0, dim2: int = 1): ...
def select(x, dim, idx): ...
def split(x, sizes, dim: int = 0): ...
def split_with_sizes(x, sizes, dim: int = 0): ...
def unbind(x, dim: int = 0): ...
def unfold(x, dimension, size, step): ...
def unsqueeze(x, dim): ...
def unsqueeze_(x, dim): ...
def _validate_dim(x, dim, offset: int = 0): ...
def glu(x, dim: int = -1): ...
def fallback_handler(kernel, add_to_fallback_set: bool = True): ...
@functools.cache
def _warn_complex_not_supported() -> None: ...
def unsupported_input_tensor(t: torch.Tensor, node=None):
    """Do not support reading or writing to this tensor"""
def unsupported_output_tensor(t: torch.Tensor, node=None):
    """Do not support writing tensor but can read from it"""
def fallback_node_due_to_unsupported_type(node: torch.fx.Node, allow_cpu_inputs: bool = True): ...
def make_fallback(op, layout_constraint=None, warn: bool = True, override_decomp: bool = False): ...
def philox_rand_offset(shape):
    """
    TorchInductor offset calculation differs from PyTorch eager offset
    calculation for random ops (tl.rand vs torch.rand). In future, we should
    strive for same impl for tl.rand and torch.rand.
    """
def philox_rand(size, seed, offset, stride, device, dtype): ...
def native_dropout(x, p, train): ...
def bernoulli_(x, *args): ...
def bernoulli_p(x, *args): ...
def _foobar(_) -> None: ...
def _warn_triton_random(salt) -> None: ...
def warn_triton_random() -> None: ...

fallback_rand_default: Incomplete
fallback_rand_generator: Incomplete
fallback_randn_default: Incomplete
fallback_randn_generator: Incomplete

def rand(*args, **kwargs): ...
def randn(*args, **kwargs): ...
def inductor_force_stride_order(input_tensor, stride): ...
def inductor_seed(device: torch.device): ...
def inductor_seeds(count, device): ...
def inductor_lookup_seed(seeds, index): ...
def inductor_random(size: list[int], seed: TensorBox, mode: str, *, offset: int = 0): ...
def inductor_randint(low: int, high: int, size: list[int], seed: TensorBox, *, offset: int = 0): ...
def _boundaries_helper(tb: TensorBox) -> tuple[str, sympy.Expr, sympy.Expr, sympy.Expr]: ...
def _sorter_helper(tb: TensorBox) -> tuple[str, sympy.Expr]: ...
def searchsorted(sorted_sequence: TensorBox, self: TensorBox, *, out_int32: bool = False, right: bool = False, side: str | None = None, sorter: TensorBox | None = None) -> TensorBox: ...
def bucketize(input: TensorBox, boundaries: TensorBox, *, out_int32: bool = False, right: bool = False): ...
def require_dense(_, *args, **kwargs): ...
def require_contiguous(_, *args, **kwargs): ...
def require_contiguous_strides(_, *args, **kwargs): ...
def require_channels_last(_, *args, **kwargs): ...
def constrain_to_fake_tensor(arg, fake_arg): ...
def constrain_to_fake_tensors(args, kwargs, fake_args, fake_kwargs): ...
def constrain_to_fx_strides(fx_node, *args, **kwargs): ...
def sdpa_constraint(fx_node, *args, **kwargs): ...
def copy(self, src, non_blocking: bool = False): ...
def clone(x, *, memory_format=None): ...
def clone_preserve_reinterpret_view(x): ...
def iota(length, *, start, step, dtype, device, requires_grad): ...
def select_scatter(x, src, dim: int, index: int): ...
def slice_scatter(x, src, dim: int = 0, start=None, end=None, step: int = 1): ...
def _unwrap(x): ...
def tensor(data, *, dtype=None, device=None, layout=None, pin_memory: bool = False): ...
def as_tensor(data, dtype=None, device=None): ...
def long_tensor(data): ...
def _local_scalar_dense(data): ...
def _assert_scalar(data, msg) -> None: ...
def _assert_tensor_metadata(a, size=None, stride=None, dtype=None, *, device=None, layout=None) -> None: ...
def _full(fill_value, device, dtype, size): ...
def full_like(x, fill_value, **kwargs): ...
def tensor_constructor(fill_value): ...
def empty(*size, names=None, dtype=None, layout=None, device=None, pin_memory=None, memory_format=None): ...
def create_tensor_like(creation_fn):
    """
    Shim to convert X_like(...) into X(...).  For example zeros_like() into zeros().
    """
def constant_like(fill_value): ...

empty_like: Incomplete
ones_like: Incomplete
zeros_like: Incomplete

def new_constant(fill_value): ...
def new_empty(x, size, *, dtype=None, layout=None, device=None, pin_memory=None): ...
def empty_strided(size, stride, *, dtype=None, layout=None, device=None, pin_memory=None): ...
def new_empty_strided(x, size, stride, *, dtype=None, layout=None, device=None, pin_memory=None): ...
def copy_strided(x, stride): ...
def full(size, fill_value, **kwargs): ...
def gather(x, dim, index, sparse_grad: bool = False): ...
def embedding(weight, indices, padding_idx: int = -1, scale_grad_by_freq: bool = False, sparse: bool = False): ...
def check_and_broadcast_indices(indices, device): ...
def index_output_size_and_inner_fn(x_size, indices, tensor_indices, tensor_size, indices_loaders, indexed_size, x_loader, check, wrap_neg: bool = True): ...
def index_impl(x, indices, check): ...
def index_impl_helper(x, indices, check, wrap_neg: bool = True): ...
def index(x, indices): ...
def _unsafe_index(x, indices): ...
def index_put(x, indices, values, accumulate: bool = False): ...
def _unsafe_index_put(x, indices, values, accumulate: bool = False): ...
def index_put_as_masked_fill(self, indices, value, accumulate): ...
def index_put_fallback(self, indices, values, accumulate): ...
def index_put_(self, indices, values, accumulate: bool = False): ...
def _unsafe_index_put_(self, indices, values, accumulate: bool = False): ...
def index_put_impl_(self, indices, values, accumulate, check, may_realize: bool = False): ...

fallback__unsafe_masked_index: Incomplete
fallback__unsafe_masked_index_put_accumulate: Incomplete

def _unsafe_masked_index(self, mask, indices, fill): ...
def _unsafe_masked_index_put_accumulate(x, mask, indices, values): ...
@make_pointwise
def clamp(a, min, max): ...
def as_strided_scatter(self, src, size, stride, storage_offset=None): ...
def scatter(x, dim: int, index, src, **kwargs): ...
def scatter_fallback(op_overload: torch._ops.OpOverload, self, dim: int, index, src, *, reduce: str | None = None, include_self: bool = True): ...
def scatter_(self, dim: int, index, src, *, reduce: str | None = None): ...
def scatter_add(x, dim: int, index, src): ...
def scatter_add_(x, dim: int, index, src): ...
def scatter_reduce(x, dim: int, index, src, reduction_type, **kwargs): ...
def scatter_reduce_(self, dim: int, index, src, reduce, *, include_self: bool = True): ...
def upsample_nearestnd(x, output_size, scales_x: tuple[float | None, ...], n: int = 2, exact: bool = False): ...
def upsample_nearest1d(x, output_size, scales: float | None = None): ...
def _upsample_nearest_exact1d(x, output_size, scales: float | None = None): ...
def upsample_nearest2d(x, output_size, scales_h: float | None = None, scales_w: float | None = None): ...
def _upsample_nearest_exact2d(x, output_size, scales_h: float | None = None, scales_w: float | None = None): ...
def upsample_nearest3d(x, output_size, scales_d: float | None = None, scales_h: float | None = None, scales_w: float | None = None): ...
def _upsample_nearest_exact3d(x, output_size, scales_d: float | None = None, scales_h: float | None = None, scales_w: float | None = None): ...
def _create_constants(*args, dtype): ...
def rev(x, dims): ...
def inplace_constant_pad_nd(x: TensorBox, padding: Sequence[int], fill_value: float) -> TensorBox | None:
    """
    This optimization changes the semantics of padding from 'clone'
    style to 'view' style.

    Thanks to functionalization, this change can still maintain numerical
    correctness.
    """
def constant_pad_nd(x, padding, fill_value: int = 0): ...
def range_mask_low(i: sympy.Expr, low: sympy.Expr | int): ...
def range_mask_high(i: sympy.Expr, high: sympy.Expr): ...
def range_mask(i: sympy.Expr, high: sympy.Expr, low: sympy.Expr): ...
def constant_boundary_condition(x, fill_value, padding=None, pad_fill_value: float = 1.0, dim=None): ...
def pooling_size(x, i, kernel_size, stride, padding, ceil_mode, *, dilation=None): ...
def should_fallback_max_pool_with_indices(kernel_size, *, n_dim): ...
def max_pool_checks(x, kernel_size, stride, padding, dilation, n_dim, *, assert_fallback=None): ...
def _max_pool_with_offsets(x, kernel_size, stride, padding, dilation, ceil_mode, *, n_dim): ...
def _low_memory_max_pool_with_offsets(x, kernel_size, stride, padding, dilation, ceil_mode: bool = False): ...
def _pool_offsets_to_indices(offsets: TensorBox, kernel_size: Sequence[int | torch.SymInt], input_size: Sequence[int | torch.SymInt], increments_to_index: Callable[[Sequence[int | torch.SymInt], Sequence[int | torch.SymInt]], torch._inductor.virtualized.OpsValue]) -> TensorBox: ...
def _low_memory_max_pool_offsets_to_indices(offsets, kernel_size, input_size, stride, padding, dilation): ...
def _max_pool_with_indices(x, kernel_size, stride, padding, dilation, ceil_mode, n_dim): ...
def max_pool2d_with_indices(x, kernel_size, stride=None, padding: int = 0, dilation: int = 1, ceil_mode: bool = False): ...
def max_pool3d_with_indices(x, kernel_size, stride=None, padding: int = 0, dilation: int = 1, ceil_mode: bool = False): ...

fallback_max_pool2d_with_indices_backward: Incomplete

def max_pool2d_with_indices_backward(grad_output, x, kernel_size, stride, padding, dilation, ceil_mode, indices): ...
def pad_adaptive_loader(x, pad_val: float = 0.0): ...
def compute_indices_adaptive_pooling(start_index, end_index, h_in, w_in, h_out, w_out): ...
def _adaptive_pooling_fn(start_index, end_index, kernel_maxes, in_sizes, out_sizes, pooling_fn): ...
def _adaptive_pooling_fn_with_idx(start_index, end_index, kernel_maxes, in_sizes, out_sizes, pooling_fn): ...

fallback_adaptive_avg_pool2d: Incomplete

def _adaptive_avg_pool2d(x, output_size): ...

fallback_adaptive_max_pool2d: Incomplete

def adaptive_max_pool2d(x, output_size): ...
def _fractional_pooling_offsets(samples, in_sz, out_sz, kernel_sz, dim, ndims): ...
def fractional_max_pool2d(x, kernel_size, output_size, random_samples): ...
def fractional_max_pool3d(x, kernel_size, output_size, random_samples): ...
def _fractional_max_pool(x, kernel_size, output_size, random_samples, n_dim): ...
def upsample_nearest2d_backward(x, output_size=None, input_size=None, scales_h=None, scales_w=None): ...

fallback_avg_pool2d: Incomplete
fallback_avg_pool3d: Incomplete

def avg_pool2d(x, kernel_size, stride=(), padding: int = 0, ceil_mode: bool = False, count_include_pad: bool = True, divisor_override=None): ...
def avg_pool3d(x, kernel_size, stride=(), padding: int = 0, ceil_mode: bool = False, count_include_pad: bool = True, divisor_override=None): ...
def _avg_poolnd(x, kernel_size, stride, padding, ceil_mode, count_include_pad, divisor_override, dim): ...

fallback_avg_pool2d_backward: Incomplete

def avg_pool2d_backward(grad_output, x, kernel_size, stride, padding, ceil_mode, count_include_pad, divisor_override=None): ...

fallback_avg_pool3d_backward: Incomplete

def avg_pool3d_backward(grad_output, x, kernel_size, stride, padding, ceil_mode, count_include_pad, divisor_override=None): ...
def _validate_reduction_axis(x, axis): ...
def _make_reduction_inner(x, *, axis, keepdims, dtype, override_return_dtype): ...
def make_reduction(reduction_type: ReductionType, override_return_dtype=None): ...
def _make_scan_inner(x, *, axis, dtype): ...
def mean(x, axis=None, keepdim: bool = False, *, dtype=None): ...
def var_mean_sum_(x, axis, correction, keepdim, return_mean): ...
def use_two_step_variance(x, axis, keepdim): ...
def var_mean_welford_(x, axis, *, correction, keepdim, return_mean): ...
def var_mean_helper_(x, *, axis, correction, keepdim, return_mean): ...
def var_(x, axis=None, *, correction=None, keepdim: bool = False): ...
def var_mean(x, axis=None, *, correction=None, keepdim: bool = False): ...
def pow_recursive(x, y, dtype): ...
@make_pointwise
def pow_native(a, b): ...

fallback_pow_tensor_tensor: Incomplete
fallback_pow_scalar: Incomplete
fallback_pow_tensor_scalar: Incomplete

def pow(a, b): ...
def mutate_to(changed, val, unsafe_alias: bool = False): ...
def fill_(x, fill_value): ...
def copy_(dst, src, non_blocking: bool = False): ...
@make_pointwise
def floordiv(a, b): ...
@make_pointwise
def truncdiv(a, b): ...
def div_mode(a, b, rounding_mode=None): ...
def mul(a, b): ...
def get_constant_value(x: ir.IRNode) -> ir.Constant | None:
    """Try convert an arbitrary IR node into an ir.Constant value"""
def div_prim(a, b): ...
def div(a, b): ...
def fmod(a, b): ...
def sum_(x, axis=None, keepdims: bool = False, *, dtype=None): ...

fallback_cumsum: Incomplete
fallback_cumprod: Incomplete
fallback_logcumsumexp: Incomplete
fallback_cummax: Incomplete
fallback_cummin: Incomplete

def cumsum(x, axis=None, dtype=None): ...
def cumprod(x, axis=None, dtype=None): ...
def logcumsumexp(x, dim): ...
def cummax(x, axis=None): ...
def cummin(x, axis=None): ...
def prod(x, axis=None, keepdims: bool = False, *, dtype=None): ...
def reduce_any(x, dim=None, keepdim: bool = False): ...
def reduce_max(x, dim=None, keepdim: bool = False): ...
def reduce_min(x, dim=None, keepdim: bool = False): ...

reduce_amax: Incomplete
reduce_amin: Incomplete
reduce_argmax: Incomplete
reduce_argmin: Incomplete
add: Incomplete
sort_fallback: Incomplete

def sort_stable(x, *, stable=None, dim: int = -1, descending: bool = False): ...
def sort(x, dim: int = -1, descending: bool = False): ...
def register_pointwise_numeric(op, name=None, triton_fallback=None): ...
def register_pointwise_numeric_ldf64(op: torch._ops.OpOverloadPacket): ...

rsqrt: Incomplete
exp: Incomplete
exp2: Incomplete
expm1: Incomplete
relu: Incomplete
sigmoid: Incomplete
sqrt: Incomplete
square: Incomplete
sub: Incomplete
abs: Incomplete
bitwise_and: Incomplete
bitwise_left_shift: Incomplete
bitwise_not: Incomplete
bitwise_or: Incomplete
bitwise_right_shift: Incomplete
bitwise_xor: Incomplete
erf: Incomplete
logical_and: Incomplete
logical_not: Incomplete
logical_or: Incomplete
logical_xor: Incomplete
maximum: Incomplete
minimum: Incomplete
neg: Incomplete
reciprocal: Incomplete
sign: Incomplete
gt: Incomplete

def _get_pointwise_overrides(ns, name) -> Generator[Incomplete, None, Incomplete]: ...

foreach_add_list: Incomplete
foreach_add_scalar: Incomplete
foreach_mul_list: Incomplete
foreach_mul_scalar: Incomplete
foreach_div_list: Incomplete
foreach_div_scalar: Incomplete

def register_foreach_inplace(aten_op, outplace_aten_op, outplace_op): ...
def register_inplace(aten_op, outplace_op): ...
def sym_constrain_range(a, min=None, max=None) -> None: ...
def sym_size(a, dim): ...
def sym_stride(a, dim): ...
def sym_numel(a): ...
def sym_sum(args): ...
def foobar(self, *args, **kwargs) -> None: ...
def _realize(x): ...
def resize_storage_bytes_(variable, new_size): ...
def set__source_tensor(self, source_tensor): ...
def fsdp_copy_(dst, src): ...
def resize(x, size, *, memory_format=None): ...
def triton_kernel_wrap_(*, kernel_idx, constant_args_idx, grid, tma_descriptor_metadata, kwargs): ...
def cond(pred, true_fn, false_fn, operands): ...
def while_loop(cond_fn, body_fn, carried_inputs, additional_inputs): ...
def invoke_subgraph(subgraph_fn: ir.Subgraph, identifier: str, *operands): ...
def invoke_quant_tracer(subgraph_fn: ir.Subgraph, *operands, scheme=None): ...
def associative_scan(combine_fn: ir.Subgraph, xs, additional_inputs: tuple[torch.Tensor]): ...
def _sink_tokens(tokens) -> None: ...
def with_effects(token, op, *args, **kwargs): ...
def prepare_softmax_online(x, dim):
    """
    Lowering inductor_prims.prepare_softmax_online to compute max/sum in one pass if no split is needed.
    """
@contextlib.contextmanager
def force_fallback(op: torch._ops.OpOverload):
    """
    A context manager to force fallback an op. Used in unit test
    for FallbackKernel.
    """
