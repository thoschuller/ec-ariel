import torch
from .runtime.runtime_utils import cache_dir as cache_dir
from _typeshed import Incomplete
from pathlib import Path
from torch._inductor.utils import is_cpu_device as is_cpu_device
from typing import Any, Callable

log: Incomplete

def aoti_eager_cache_dir(namespace: str, device: str) -> Path: ...
def aoti_eager_op_conf_lock(op_func_name_with_overload: str) -> Any: ...
def load_aoti_eager_cache(ns: str, op_func_name_with_overload: str, device_type: str) -> list[dict[str, Any] | None]: ...
def supported_builtin_dtype_torch_dtype() -> dict[type, torch.dtype]: ...
def supported_scalar_types() -> tuple[type, ...]: ...
def extract_tensor_metadata(dynamic: bool, input: torch.Tensor) -> dict[str, Any]: ...
def extract_tensor_list_metadata(dynamic: bool, input: list[torch.Tensor]) -> dict[str, Any]: ...
def extract_scalar_metadata(device_type: str, input: Any) -> dict[str, Any]: ...
def extract_string_metadata(input: str) -> dict[str, Any]: ...
def extract_dtype_metadata(input: torch.dtype) -> dict[str, Any]: ...
def extract_device_metadata(input: torch.device) -> dict[str, Any]: ...
def extract_layout_metadata(input: torch.layout) -> dict[str, Any]: ...
def aoti_compile_with_persistent_cache(ns: str, op_func_name_with_overload: str, device_type: str, dynamic: bool, f: Callable[..., Any], args: tuple[Any], kwargs: dict[str, Any], *, dynamic_shapes: dict[str, Any] | None = None, options: dict[str, Any] | None = None, remove_runtime_assertions: bool = False, disable_constraint_solver: bool = False) -> str:
    """
    Compile the given function with persistent cache for AOTI eager mode.
    """
