import abc
import dataclasses
import functools
import torch
import torch.fx
import torch.utils._pytree as pytree
from . import config as config
from .._functorch.aot_autograd import aot_function as aot_function, make_boxed_func as make_boxed_func
from .._functorch.partitioners import default_partition as default_partition
from .._subclasses import FakeTensor as FakeTensor, FakeTensorMode as FakeTensorMode
from ..fx import Transformer as Transformer
from .decomposition import select_decomp_table as select_decomp_table
from .lowering import fallback_node_due_to_unsupported_type as fallback_node_due_to_unsupported_type
from _typeshed import Incomplete
from abc import ABC, abstractmethod
from collections import defaultdict
from collections.abc import Generator, Iterable, Mapping, Sequence
from torch._dispatch.python import enable_python_dispatcher as enable_python_dispatcher
from torch._dynamo.utils import counters as counters
from torch._prims_common import is_integer_dtype as is_integer_dtype
from torch._subclasses.fake_tensor import unset_fake_temporarily as unset_fake_temporarily
from torch.fx.experimental.proxy_tensor import make_fx as make_fx
from torch.fx.experimental.symbolic_shapes import statically_known_true as statically_known_true
from torch.fx.graph_module import _get_attr as _get_attr
from torch.fx.immutable_collections import immutable_dict as immutable_dict, immutable_list as immutable_list
from torch.fx.passes.graph_transform_observer import GraphTransformObserver as GraphTransformObserver
from torch.utils._ordered_set import OrderedSet as OrderedSet
from typing import Any, Callable, NoReturn, Protocol, TypeVar
from typing_extensions import TypeIs

log: Incomplete
aten: Incomplete
prims: Incomplete
Constant = Any
NodeOrConstant = Constant | torch.fx.Node

class SearchFn(Protocol):
    __name__: str
    def __call__(self, *args: Any, **kwargs: Any) -> Any: ...

class ReplaceFn(Protocol):
    def __call__(self, *args: Any, **kwargs: Any) -> Any: ...

class TraceFn(Protocol):
    def __call__(self, fn: SearchFn | ReplaceFn, *args: Any, **kwargs: Any) -> torch.fx.GraphModule: ...
T = TypeVar('T')
FnsType: Incomplete

class Multiple:
    def __init__(self) -> None: ...

MULTIPLE: Incomplete

def _transfer_meta(new_meta: dict[str, Any], old_node: torch.fx.Node, pass_name: str = '') -> None: ...

class Match:
    """
    Represents a successfully matched pattern.

    The `Match` object is returned to represent a successfully matched
    pattern. Included in the Match are the pattern that was matched, the graph
    nodes matched, and any args that were used during the matching.

    The args and kwargs are specific to the type of pattern that was matched and
    provide hints about what was matched.
    """
    pattern: PatternExpr
    args: list[Any]
    kwargs: dict[str, Any]
    nodes: list[torch.fx.Node]
    targets: dict[_TargetExpr, torch.fx.node.Target]
    ctx: MatchContext
    replacement_graph: torch.fx.GraphModule | None
    def __init__(self, ctx: MatchContext, pattern: PatternExpr, args: Sequence[Any] | None = None, kwargs: dict[str, Any] | None = None) -> None: ...
    @property
    def graph(self) -> torch.fx.Graph: ...
    def extend(self, other: Match) -> None: ...
    def bundle(self) -> Match: ...
    def __repr__(self) -> str: ...
    def erase_nodes(self) -> None: ...
    def output_nodes(self) -> list[torch.fx.Node | None]: ...
    def output_node(self) -> torch.fx.Node: ...
    def replace_with_graph(self, replacement_graph: torch.fx.Graph, args: Sequence[Any]) -> None: ...
    def replace_by_example(self, replacement_fn: ReplaceFn, args: Sequence[Any], trace_fn: TraceFn | None = None, run_functional_passes: bool = True) -> None:
        """Replace with a graph generated by tracing the replacement_fn.

        Args:
            run_functional_passes (bool). If we should run passes that
                assume functional IR (like DCE, remove_noop_ops), on the
                replacement graph.

        """

class FailedMatch(RuntimeError):
    """
    Represents a unsuccessful match.

    The `FailedMatch` object is returned to represent a failure to match a
    pattern.
    """
    format_string: str
    args: Incomplete
    kwargs: Incomplete
    def __init__(self, format_string: str, *args: Any, **kwargs: Any) -> None: ...
    def __str__(self) -> str: ...
    def __bool__(self) -> bool: ...
MatchResult = Match | FailedMatch

def is_match(m: MatchResult) -> TypeIs[Match]:
    """
    TypeIs cannot act on `self`. Thus this function exists to let mypy
    recognize FailedMatch.__bool__ as a TypeIs.
    """

class MatchContext:
    """
    Internal state needed while running PatternExpr._match().
    """
    outputs: list[PatternExpr | None]
    pattern_to_node: dict[PatternExpr, torch.fx.Node | None]
    graph: torch.fx.Graph
    exclusive_node_set: list[NodeOrConstant]
    def __init__(self, outputs: list[PatternExpr | None], pattern_to_node: dict[PatternExpr, torch.fx.Node] | None = None, *, graph: torch.fx.Graph) -> None: ...
    def match(self, pattern: PatternExpr, node: NodeOrConstant) -> MatchResult:
        """wrapper to check reused nodes in patterns"""
    def filter_multi_user_patterns(self) -> dict[PatternExpr, torch.fx.Node]: ...

class PatternExpr(ABC, metaclass=abc.ABCMeta):
    """
    Base class for types of patterns.
    """
    @abstractmethod
    def _match(self, node: torch.fx.Node, ctx: MatchContext) -> MatchResult: ...
    def match(self, node: torch.fx.Node) -> MatchResult: ...
    def has_multiple_users(self) -> bool: ...
    def __repr__(self) -> str: ...
    def find_anchor_nodes(self, ctx: MatchContext, searched: OrderedSet[torch.fx.Node]) -> Generator[torch.fx.Node | None, None, None]: ...
    def pattern_eq(self, other: Any) -> bool:
        """
        Compare two `PatternExpr`s and return true if they are the
        same. Note this is NOT matching a pattern - it is comparing the pattern
        structures (for debugging).
        """

class Arg(PatternExpr):
    """
    Capture an arg which will become an input to the handler.  Args are
    passed in depth first order.
    """
    def _match(self, node: NodeOrConstant, ctx: MatchContext) -> MatchResult: ...

class Ignored(PatternExpr):
    """
    Match an arg, but don't pass it to handler
    """
    def _match(self, node: NodeOrConstant, ctx: MatchContext) -> MatchResult: ...
    def __repr__(self) -> str: ...
    def pretty_print(self, pp: PatternPrettyPrinter) -> str: ...

class KeywordArg(PatternExpr):
    """
    Capture a kwarg which will become an input to the handler.
    """
    name: Incomplete
    def __init__(self, name: str) -> None: ...
    def __repr__(self) -> str: ...
    def _match(self, node: NodeOrConstant, ctx: MatchContext) -> MatchResult: ...
    def pattern_eq(self, other: Any) -> bool: ...

class ExclusiveKeywordArg(PatternExpr):
    """
    Capture a kwarg which will become an input to the handler.
    """
    name: str
    def __init__(self, name: str) -> None: ...
    def __repr__(self) -> str: ...
    def _match(self, node: NodeOrConstant, ctx: MatchContext) -> MatchResult: ...
    def pattern_eq(self, other: Any) -> bool: ...

class _TargetExpr(PatternExpr, metaclass=abc.ABCMeta):
    """
    Base class for filtering match by node.target
    """
    fns: list[FnsType]
    fns_set: OrderedSet[FnsType]
    users: Incomplete
    def __init__(self, fns: FnsType | Sequence[FnsType], users: Multiple | int = 1) -> None: ...
    @property
    @abstractmethod
    def op(self) -> str: ...
    def fns_repr(self) -> str: ...
    def __repr__(self) -> str: ...
    def has_multiple_users(self) -> bool: ...
    def find_anchor_nodes(self, ctx: MatchContext, searched: OrderedSet[torch.fx.Node]) -> Generator[torch.fx.Node | None, None, None]: ...
    def _match_fns(self, node: torch.fx.Node) -> bool: ...
    def _match_users(self, node: torch.fx.Node, ctx: MatchContext) -> bool: ...
    def pattern_eq(self, other: Any) -> bool: ...
_SimpleSpec = tuple[Any, ...]

class _TargetArgsExpr(_TargetExpr, metaclass=abc.ABCMeta):
    """
    Base class for filtering match by node.{target,args,kwargs}
    """
    args: Incomplete
    kwargs: Incomplete
    flatten: Incomplete
    flat_args_kwargs: Incomplete
    def __init__(self, fns: torch.fx.node.Target | str | Sequence[Any], *args: Any, _users: int | Multiple = 1, **kwargs: Any) -> None: ...
    @staticmethod
    def simple_flatten(args: Sequence[Any], kwargs: Mapping[Any, Any]) -> tuple[Sequence[Any], _SimpleSpec | pytree.TreeSpec]: ...
    @staticmethod
    def pytree_flatten(args: Sequence[Any], kwargs: Mapping[Any, Any]) -> tuple[Sequence[Any], _SimpleSpec | pytree.TreeSpec]: ...
    def __repr__(self) -> str: ...
    def pretty_print(self, pp: PatternPrettyPrinter) -> str: ...
    def _match(self, node: torch.fx.Node, ctx: MatchContext) -> MatchResult: ...
    def find_anchor_nodes(self, ctx: MatchContext, searched: OrderedSet[torch.fx.Node]) -> Generator[torch.fx.Node | None, None, None]:
        """
        This is used when we are matching a pattern with multiple outputs.
        There is a partial match (stored in ctx) and we want to walk
        this pattern to find a connection to an already-matched node.

        Yields candidate nodes that `self._match` might like.
        """
    def pattern_eq(self, other: Any) -> bool: ...

class CallFunction(_TargetArgsExpr):
    """
    Matches a call_function node in the FX graphs: `fns[i](*args, **kwargs)`
    """
    op: str

class CallMethod(_TargetArgsExpr):
    """
    Matches a call_method node in the FX graphs: `fns[i].method(*args, **kwargs)`
    """
    op: str

class CallModule(_TargetArgsExpr):
    """
    Matches a call_module node in the FX graphs: `module(*args, **kwargs)`
    """
    op: str

class _TargetExprVarArgs(_TargetExpr, metaclass=abc.ABCMeta):
    """
    Matches a call_function node with any arguments which are passed into the pattern
    """
    def _match(self, node: torch.fx.Node, ctx: MatchContext) -> MatchResult: ...

class CallFunctionVarArgs(_TargetExprVarArgs):
    op: str

class CallMethodVarArgs(_TargetExprVarArgs):
    op: str

class CallModuleVarArgs(_TargetExprVarArgs):
    op: str

class ListOf(PatternExpr):
    """
    Matches a repeated pattern
    """
    pattern: Incomplete
    partial: Incomplete
    def __init__(self, pattern: PatternExpr, partial: bool = False) -> None: ...
    def __repr__(self) -> str: ...
    def _match(self, node: list[torch.fx.Node], ctx: MatchContext) -> MatchResult: ...
    def pattern_eq(self, other: Any) -> bool: ...

class MultiOutputPattern(PatternExpr):
    outputs: list[PatternExpr | None]
    op: Incomplete
    def __init__(self, outputs: Sequence[PatternExpr | None]) -> None: ...
    @property
    def fns(self) -> Callable[..., Any] | str | Sequence[Any]: ...
    def __repr__(self) -> str: ...
    def pretty_print(self, pp: PatternPrettyPrinter) -> str: ...
    def _match(self, node: torch.fx.Node, ctx: MatchContext) -> MatchResult: ...
    def _match_from_anchors(self, pattern: PatternExpr, ctx: MatchContext) -> MatchResult: ...
    def match(self, node: torch.fx.Node) -> MatchResult: ...
    def pattern_eq(self, other: Any) -> bool: ...

class RepeatedExpr(PatternExpr):
    """
    Checks for a repeated pattern. Useful for repeated operations after a node such as `split` or `unbind`
    """
    inner_pattern: Incomplete
    op: Incomplete
    def __init__(self, inner_pattern: _TargetExpr) -> None: ...
    @property
    def fns(self) -> Sequence[FnsType]: ...
    def _match(self, node: torch.fx.Node, ctx: MatchContext) -> MatchResult: ...
    def pattern_eq(self, other: Any) -> bool: ...

class PatternPrettyPrinter:
    """
    Serializes Patterns to executable python.
    XXX: currently only used and tested for fuse attention patterns. May not cover
    all patterns.
    """
    namespace: Incomplete
    memoized_objs_names: dict[PatternExpr, str]
    memoized_objs_pp: dict[PatternExpr, str]
    def __init__(self) -> None: ...
    @staticmethod
    @functools.cache
    def run(obj: PatternExpr, output_name: str = 'output') -> str:
        """
        Serializes obj to python code with obj written out to `output_name`
        """
    def pretty_print(self, obj: Any) -> str: ...
    def memoize(self, obj: _TargetArgsExpr) -> str: ...

class _PassDictsType(Protocol):
    def __getitem__(self, k: tuple[str, torch.fx.node.Target]) -> list[PatternEntry]: ...

@dataclasses.dataclass
class PatternEntry:
    pattern: PatternExpr
    extra_check: Callable[[Match], bool]
    def apply(self, match: Match, graph: torch.fx.Graph, node: torch.fx.Node) -> None: ...
    def register(self, pass_dicts: _PassDictsType | Sequence[_PassDictsType], target: torch.fx.node.Target | None = None, prepend: bool = False) -> None: ...

@dataclasses.dataclass
class LoweringPatternEntry(PatternEntry):
    handler: Callable[..., Any]
    def apply(self, match: Match, graph: torch.fx.Graph, node: torch.fx.Node) -> None: ...

@dataclasses.dataclass
class GraphPatternEntry(PatternEntry):
    """
    A pattern that runs a function on the FX graph
    """
    handler: Callable[..., Any]
    def apply(self, match: Match, graph: torch.fx.Graph, node: torch.fx.Node) -> None: ...

@dataclasses.dataclass
class ReplacementPatternEntry(PatternEntry):
    normalize_args: Callable[..., list[Any]]
    @staticmethod
    def replace_with_graph(match: Match, graph: torch.fx.Graph, replacement_graph: torch.fx.Graph | torch.fx.GraphModule, args: Sequence[torch.fx.Node]) -> None: ...
    def apply(self, match: Match, graph: torch.fx.Graph, node: torch.fx.Node) -> None: ...

def _return_true(match: Match) -> bool: ...
def log_trace_failure(search_fn: Callable[..., Any], e: RuntimeError) -> None: ...
def check_and_add_duplicate_pattern(pattern: PatternExpr, graph: torch.fx.Graph | None, seen_patterns: dict[str, list[str | None]], skip_duplicates: bool = False) -> bool:
    """
    Check if a pattern is a duplicate. Because we ignore certain types in searching, but not
    in matching, use the graph to distinguish equivalent search patterns.

    Returns True if a duplicate is found and `skip_duplicates=True` is passed in. Errors if
    `skip_duplicates` is False and a duplicate is found.
    """
def register_replacement(search_fn: SearchFn, replace_fn: ReplaceFn, example_inputs: Iterable[Any], trace_fn: TraceFn, pass_dicts: _PassDictsType | Sequence[_PassDictsType], extra_check: Callable[[Match], bool] = ..., scalar_workaround: dict[str, float | int] | None = None, exclusive_arg_names: Sequence[str] = (), search_fn_pattern: PatternExpr | None = None, skip_duplicates: bool = False) -> bool:
    """
    Create a replacement rule based on example functions that get traced
    to create patterns.  This supports both training and inference when
    run on a joint forward+backward graph.

    Args:
        search_fn: traced to give original pattern
        replace_fn: traced to give replacement graph
        example_inputs: example inputs for initial trace
        trace_fn: fwd_only or joint_fwd_bwd
        pass_dict: dict of passes to register to
        extra_check: additional check to run on match(using real shapes)
    """

_serialized_patterns: OrderedSet[str]

def _serialize_pattern(unique_name: str, search_fn: SearchFn, example_inputs: Sequence[Any], trace_fn: TraceFn, scalar_workaround: dict[str, float | int] | None) -> PatternExpr: ...

SERIALIZED_PATTERN_PATH: Incomplete
_known_precompiled_patterns: list[tuple[Any, Iterable[Any], Callable[[Callable[..., Any], Iterable[Any]], torch.fx.GraphModule], Any, PatternExpr]]

def gen_register_replacement(unique_name: str, search_fn: SearchFn, replace_fn: ReplaceFn, example_inputs: Iterable[Any], trace_fn: TraceFn, pass_dicts: _PassDictsType | Sequence[_PassDictsType], extra_check: Callable[[Match], bool] = ..., scalar_workaround: dict[str, float | int] | None = None, exclusive_arg_names: Sequence[str] = (), skip_duplicates: bool = False) -> None: ...
def gen_pattern_and_search_gm(search_fn: SearchFn, example_inputs: Sequence[Any], trace_fn: TraceFn, scalar_workaround: dict[str, float | int] | None = None, exclusive_arg_names: Sequence[str] = ()) -> tuple[PatternExpr, torch.fx.GraphModule]: ...
def gen_pattern(search_fn: SearchFn, example_inputs: Sequence[Any], trace_fn: TraceFn, scalar_workaround: dict[str, float | int] | None = None, exclusive_arg_names: Sequence[str] = ()) -> PatternExpr: ...
def register_lowering_pattern(pattern: PatternExpr, extra_check: Callable[[Match], bool] = ..., *, pass_dict: _PassDictsType, prepend: bool = False) -> Callable[[Callable[..., Any]], Callable[..., Any]]:
    """
    Register an aten to inductor IR replacement pattern.  The decorated
    function is saved and then called a lowering time allowing direct
    pattern to inductor IR conversion.
    """
def register_graph_pattern(pattern: PatternExpr, extra_check: Callable[[Match], bool] = ..., *, pass_dict: _PassDictsType, prepend: bool = False) -> Callable[[Callable[..., Any]], Callable[..., Any]]:
    """
    Register a pattern that runs a function on the FX graph, allowing
    custom transformation code.
    """
def is_start_of_fx_graph(graph: torch.fx.Graph, node: torch.fx.Node) -> bool: ...

_mutation_op_re: Incomplete

def fixme_incorrect_inductor_schema_op(op: torch._ops.OpOverload) -> bool: ...
def is_mutation_op(node: torch.fx.Node) -> bool: ...
def same_mutation_regions(a: torch.fx.Node, b: torch.fx.Node) -> bool: ...
def get_mutation_region_id(graph: torch.fx.Graph, node: torch.fx.Node) -> int: ...
def should_compute_mutation_region_ids(graph: torch.fx.Graph) -> bool: ...
def compute_mutation_region_ids(graph: torch.fx.Graph) -> None: ...

class PatternMatcherPass:
    patterns: defaultdict[tuple[str, torch.fx.node.Target], list[PatternEntry]]
    pass_name: Incomplete
    seen_patterns: dict[str, list[str | None]]
    def __init__(self, pass_name: str | None = None) -> None: ...
    def __getitem__(self, item: tuple[str, torch.fx.node.Target]) -> list[PatternEntry]: ...
    def apply(self, gm: torch.fx.GraphModule | torch.fx.Graph) -> int: ...
    def clear(self) -> None: ...

def _not_implemented(*args: Any, **kwargs: Any) -> NoReturn: ...
def fx_to_pattern(gm: torch.fx.GraphModule | torch.fx.Graph, ignore_types: Sequence[type[Any]] = (), argnames: Sequence[str] = (), scalar_workaround: dict[str, float | int] | None = None, exclusive_arg_names: Sequence[str] = ()) -> PatternExpr:
    """
    Convert an FX graph into a PatternExpr.  This is useful for simple
    patterns that can only match single functions and fixed-length lists.
    """
def fwd_only(fn: Callable[..., Any], args: Sequence[Any], *, run_functional_passes: bool = True, get_decomp_fn: Callable[..., Any] | None = None) -> torch.fx.GraphModule:
    """Build a normalized inference graph, for use with fx_to_pattern"""
def joint_fwd_bwd(fn: Callable[..., Any], args: Sequence[Any]) -> torch.fx.GraphModule:
    """Build a normalized training graph, for use with fx_to_pattern"""
def _args(n: torch.fx.Node) -> list[torch.fx.node.Argument]: ...
def stable_topological_sort(graph: torch.fx.Graph) -> None: ...
def init_once_fakemode(fn: Callable[..., Any]) -> Callable[[], Any]:
    """Wrapper around lazy init functions in fx_passes/"""
def config_flag(name: str) -> Callable[[Match], Any]:
    """Function for extra_check to put pass behind a flag"""
def clone_graph(input_graph: torch.fx.GraphModule) -> torch.fx.GraphModule: ...

_seen_patterns: OrderedSet[str]

def get_arg_value(node: torch.fx.Node, arg_number: int, kwarg_name: str | None = None) -> Any: ...
def filter_nodes(nodes: Iterable[torch.fx.Node], fn: Any) -> list[torch.fx.Node]: ...
def extract_target(node: torch.fx.Node) -> torch.fx.node.Target:
    """For call_function and call_method, we directly use the target function;
    For call_module, the target is string, and we treat the module class
     as a function.
    """
