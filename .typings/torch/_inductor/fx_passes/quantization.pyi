import functools
import torch
from .. import config as config
from ..lowering import require_channels_last as require_channels_last
from ..pattern_matcher import Arg as Arg, CallFunction as CallFunction, KeywordArg as KeywordArg, ListOf as ListOf, Match as Match, filter_nodes as filter_nodes, stable_topological_sort as stable_topological_sort
from ..utils import pad_listlike as pad_listlike
from .freezing_patterns import register_freezing_graph_pattern as register_freezing_graph_pattern
from .post_grad import register_lowering_pattern as register_lowering_pattern
from _typeshed import Incomplete
from torch._dynamo.utils import counters as counters
from torch.fx.experimental.symbolic_shapes import has_free_symbols as has_free_symbols
from torch.fx.node import map_arg as map_arg

aten: Incomplete
prims: Incomplete
quantized_decomposed: Incomplete
quantized: Incomplete
_PER_TENSOR_QUANTIZE_OPS: Incomplete
_VIEW_OPS: Incomplete

def _get_pattern_output_dtype(match: Match):
    """
    Get the pattern's output dtype from node's meta
    Assume only 1 output node in this matched pattern.
    """
def _may_generate_pattern_with_dtype_convert(pattern, dtype=..., with_dtype_convert: bool = True, users: int = 1): ...
def _may_generate_pattern_with_reshape(pattern, reshape_size=..., with_reshape: bool = True): ...
def _generate_linear_t_pattern(_dequant_per_channel_pattern, dtype): ...
def _unary_fusion_pattern(unary_fusion, call_fn, users, is_bf16): ...
def get_dequantize_per_tensor_activation_pattern(is_tensor_overload: bool = False): ...

dequantize_per_channel_weight_pattern: Incomplete
dequantize_per_channel_to_bf16_weight_pattern: Incomplete
dequantize_per_channel_clone_weight_pattern: Incomplete
dequantize_per_channel_to_bf16_clone_weight_pattern: Incomplete

def get_qconv_pt2e_pattern(users: int = 1): ...
def get_qconv2d_binary_pt2e_pattern(users: int = 1): ...
def get_qlinear_pt2e_pattern(x_scale_zp_are_tensors, users: int = 1): ...
def get_qlinear_binary_pt2e_pattern(x_scale_zp_are_tensors, users: int = 1): ...

dequantize_accum_pattern: Incomplete

def generate_pattern_with_binary(binary_post_op, computation_call, extra_input_pattern, dtype_convert: bool = False, swap_inputs: bool = False): ...
def generate_pattern_with_unary(computation_call, unary_post_op): ...
def generate_pattern_with_output_quant(computation_call, with_dtype_convert: bool = False): ...
def _check_node_kwarg_arg_value(check_node, kwarg_name, args_index, expected_value): ...
def _is_valid_quantized_conv_optimization_pattern(): ...
def _is_valid_qconv_post_op_fusion_pattern(has_binary_post_op: bool = False): ...
def _is_valid_qconv_lowering_pattern(): ...
def _register_quantized_conv_lowering(pattern, pass_number, computation_op): ...
def _is_valid_quantized_linear_optimization_pattern(): ...
def _is_valid_qlinear_post_op_fusion_pattern(has_binary_post_op: bool = False): ...
def _is_valid_qlinear_lowering_pattern(): ...
def _register_quantized_linear_unary_lowering(pattern, pass_number, computation_op): ...
def _register_quantized_linear_binary_lowering(pattern, pass_number, computation_op): ...
def _is_valid_qconv_binary_optimization_pattern(): ...
def _is_valid_qlinear_binary_optimization_pattern(): ...
def _is_valid_quantized_op_binary_optimization_pattern(qop, extra_input_from_dequant: bool = True): ...
def _register_quantized_conv_binary_lowering(pattern, pass_number, computation_op): ...
def _register_quantization_unary_lowering() -> None: ...
def _register_quantization_binary_lowering() -> None: ...
def _is_valid_quantized_maxpool2d_optimization_pattern(): ...
def _register_quantized_maxpool2d_lowering(pattern, computation_op): ...
def _register_quantization_maxpool2d() -> None: ...
def _is_input_output_same_scale_zp(check_node): ...
def _register_quantized_cat_lowering(pattern, computation_op): ...

_raw_dequantize_per_tensor_activation_pattern: Incomplete

def _register_quantization_cat() -> None: ...
def _register_quantized_reshape_lowering(pattern, computation_op): ...
def _register_quantization_reshape() -> None: ...
def _is_valid_concat_linear_int8_woq_optimization_pattern(): ...
def _is_valid_woq_optimization_pattern(): ...
def _register_concat_linear_int8_woq_lowering(pattern, computation_woq, computation_reshape): ...
def _register_woq_lowering(pattern, computation_woq, computation_reshape): ...
def _register_woq_mm_int8_pattern1() -> None: ...
def _register_woq_mm_int8_pattern2() -> None: ...
def _register_woq_mm_int8_pattern3() -> None: ...
def _register_woq_mm_int8_pattern4() -> None: ...
def _register_int8_woq_concat_linear_pattern(): ...
def _register_quantization_lowerings() -> None: ...
def _register_woq_lowerings() -> None: ...
def _is_valid_dequant_promotion_pattern(dtype=...): ...
def _register_dequant_promotion_pass(pattern, pass_number, dtype=...): ...
def _is_valid_dequant_conv_pattern(dtype): ...
def _register_qconv_weight_prepack_pass(pattern, pass_number, dtype=...) -> None: ...
def _generate_dequant_convolution_node_pattern(_dequant_per_channel_pattern, dtype=...): ...
def _generate_qconv_weight_prepack_patterns(dtype=...): ...
def _get_linear_node(match, input_dim_exceeds_two, input_contiguous): ...
def _get_linear_dq_node(linear_node, input_index, dtype, input_dim_exceeds_two, input_contiguous): ...
def _is_valid_dequant_linear_pattern(dtype, input_dim_exceeds_two, input_contiguous): ...
def _register_qlinear_weight_prepack_pass(pattern, pass_number, dtype=..., input_dim_exceeds_two: bool = False, input_contiguous: bool = True) -> None: ...
def _generate_dequant_linear_node_pattern(_dequant_per_channel_pattern, dtype=..., input_dim_exceeds_two: bool = False, is_tensor_overload: bool = False): ...
def _generate_dequant_bmm_node_pattern(_dequant_per_channel_pattern, dtype=..., with_bias: bool = False, is_tensor_overload: bool = False): ...
def _generate_qlinear_weight_prepack_patterns(dtype=..., input_dim_exceeds_two: bool = False, input_contiguous: bool = True, with_bias: bool = False, is_tensor_overload: bool = False): ...
def _generate_linear_dynamic_fp16_pattern(_dequant_weight_pattern, input_dim_exceeds_two: bool = False, input_contiguous: bool = True, relu_fused: bool = False): ...
def _register_dequant_promotion() -> None: ...
def _register_qconv_weight_prepack() -> None: ...
def _register_qlinear_weight_prepack() -> None: ...
def _register_linear_dynamic_fp16_weight_prepack_pass(pattern, pass_number, input_dim_exceeds_two: bool = False, input_contiguous: bool = True, relu_fused: bool = False): ...
def _register_linear_dynamic_fp16_weight_prepack() -> None: ...
def _register_smooth_quant_int_mm_pattern():
    """
    The pattern is:
      (no bias) reshape -> _int_mm -> convert_element_type -> (expand ->) mul -> mul -> reshape
    or
      (with bias) pattern_no_bias -> add (-> reshape -> reshape)
    """

class PostOpAttr:
    binary_op_name: Incomplete
    alpha: Incomplete
    unary_op_name: Incomplete
    scalars_attr: Incomplete
    algorithm_attr: Incomplete
    def __init__(self, binary_op_name: str = 'none', alpha=None, unary_op_name: str = 'none', scalars_attr=None, algorithm_attr=None) -> None: ...

def _register_qconv_post_op_fusion_pass(pattern, pass_number, computation_op, post_op_attr): ...
def _register_qconv_unary_fusion() -> None: ...
def _register_qconv_binary_fusion() -> None: ...
def _register_qlinear_post_op_fusion_pass(pattern, pass_number, computation_op, post_op_attr) -> None: ...
def _register_qlinear_unary_fusion() -> None: ...
def _register_qlinear_binary_fusion() -> None:
    """
    Supported linear-binary(-unary) patterns

        linear(X)   extra input
               \\   /
                Add
                 |
            Optional(relu)
                 |
                 Y

    1. int8-mixed-fp32
    +---+---------------+-----------+------------------------------+---------+
    | # | Add type      | Quant out | Pattern                      | Post op |
    +---+---------------+-----------+------------------------------+---------+
    | 1 | In-/out-place | Yes       | linear + fp32 -> (relu) -> q | add     |
    +---+---------------+-----------+------------------------------+---------+
    | 2 | In-/out-place | No        | linear + fp32 -> (relu)      | sum     |
    +---+---------------+-----------+------------------------------+---------+

    2. int8-mixed-bf16
    +---+----------+---------------+-----------+-----------------------------------------+---------+
    | # | X2 dtype | Add type      | Quant out | Pattern                                 | Post op |
    +---+----------+---------------+-----------+-----------------------------------------+---------+
    | 1 | BF16     | In-/out-place | Yes       | linear + bf16 -> (relu) -> q            | add     |
    +---+----------+---------------+-----------+-----------------------------------------+---------+
    | 2 | BF16     | In-/out-place | No        | linear + bf16 -> (relu)                 | sum     |
    +---+----------+---------------+-----------+-----------------------------------------+---------+
    | 3 | FP32     | Out-place     | Yes       | linear + fp32 -> (relu) -> q            | add     |
    |   |          | In-place right|           |                                         |         |
    +---+----------+---------------+-----------+-----------------------------------------+---------+
    | 4 | FP32     | Out-place     | No        | linear + fp32 -> (relu)                 | sum     |
    |   |          | In-place right|           |                                         |         |
    +---+----------+---------------+-----------+-----------------------------------------+---------+
    | 5 | FP32     | In-place left | Yes       | linear + fp32 -> to_bf16 -> (relu) -> q | add     |
    +---+----------+---------------+-----------+-----------------------------------------+---------+
    | 6 | FP32     | In-place left | No        | linear + fp32 -> to_bf16 -> (relu)      | add     |
    +---+----------+---------------+-----------+-----------------------------------------+---------+

    Note
    (1) The positions of linear and the extra input can be swapped.
    (2) we don't insert q-dq before the extra input of linear-add by recipe. But if q-dq is found at the
    extra input, we don't match that pattern because we cannot match all these patterns in 3 passes.
    """
@functools.cache
def _register_quantization_weight_pack_pass() -> None: ...
def _is_valid_concat_linear_woq_int4_fusion(computation_nodes): ...
def concat_linear_woq_int4(gm: torch.fx.GraphModule):
    """
    Concat Linear optimization pass for WOQ int4
    This pass fuses the original pattern:
    def ...
        return (woq_int4(x, w1, group_size, scale_zp1), woq_int4(x, w2, group_size, scale_zp1) ...)
    into a single operation:
    def ...
        concat_res = woq_int4(x, concat_w, group_size, concat_scale_zp)
        return split(concat_res, split_size_list)
    """
def quant_lift_up(graph_module: torch.fx.GraphModule):
    """
    Lift up the quant node before view like nodes. It can benefit performance
    of Attention like block. For example, we have the pattern as:

             DQ
    DQ       LINEAR
    LINEAR   VIEW
    VIEW     PERMUTE
    PERMUTE  TRANSPOSE
    Q        Q
    DQ       DQ
       Matmul
        DIV
        ADD
      SOFTMAX

    We want to lift up the the quant nodes from matmul before view like nodes
    as the output of Linear node.

             DQ
    DQ       LINEAR
    LINEAR   Q
    Q        VIEW
    VIEW     PERMUTE
    PERMUTE  TRANSPOSE
    DQ       DQ
       Matmul
        DIV
        ADD
      SOFTMAX

    It produces a DQ->LINEAR->Q pattern which can be fused by backend.
    """
