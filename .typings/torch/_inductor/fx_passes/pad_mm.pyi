import functools
import torch
import torch._inductor
from ...utils._triton import has_triton as has_triton
from ..pattern_matcher import Match as Match, ReplaceFn as ReplaceFn, SearchFn as SearchFn, fwd_only as fwd_only, gen_register_replacement as gen_register_replacement, joint_fwd_bwd as joint_fwd_bwd
from _typeshed import Incomplete
from collections.abc import Sequence
from torch import Tensor as Tensor
from torch._dynamo.utils import counters as counters, dynamo_timed as dynamo_timed
from torch._inductor import utils as utils
from torch._inductor.autoheuristic.autoheuristic import AHContext as AHContext, AutoHeuristic as AutoHeuristic, LocalFeedback as LocalFeedback
from torch._inductor.autoheuristic.autoheuristic_utils import context_add_strides as context_add_strides, context_add_using_tf32 as context_add_using_tf32, pad_mm_operations as pad_mm_operations, pad_mm_precondition as pad_mm_precondition
from torch._subclasses.fake_tensor import FakeTensor as FakeTensor
from torch.utils._mode_utils import no_dispatch as no_dispatch
from typing import Any, Callable

aten: Incomplete
_skip_do_bench_times: bool

def fetch_fake_tensors(match: Match, kwarg_names: Sequence[str]) -> list[Tensor]: ...
def unwrap_fake_args(*arg_names: str) -> Callable[[Callable[..., Any]], Callable[[Match], Any]]: ...
def get_alignment_size(x: Tensor) -> int: ...
def get_alignment_size_dtype(dtype: torch.dtype) -> int: ...
def check_device(a: Tensor, b: Tensor) -> bool: ...
def check_dtype(a: Tensor, b: Tensor) -> bool: ...
def should_pad_common(mat1: Tensor, mat2: Tensor, input: Tensor | None = None) -> bool: ...
def get_padded_length(x: int | torch.SymInt, alignment_size: int) -> int: ...
def pad_dim(x: Tensor, padded_length: int, dim: int) -> Tensor: ...
def addmm_pattern(input: Tensor, mat1: Tensor, mat2: Tensor, beta: float, alpha: float) -> Tensor: ...
def should_pad_addmm(match: Match) -> bool: ...
def pad_addmm(input: Tensor | None, mat1: Tensor, mat2: Tensor, m_padded_length: int, k_padded_length: int, n_padded_length: int, beta: float = 1.0, alpha: float = 1.0, mat1_pre_padded: bool = False, mat2_pre_padded: bool = False) -> Tensor: ...
def addmm_replace(input: Tensor | None, mat1: Tensor, mat2: Tensor, beta: float = 1.0, alpha: float = 1.0) -> Tensor: ...
def is_mm_compute_bound(M: int, K: int, N: int, dtype: torch.dtype) -> bool: ...
@functools.cache
def get_pad_cache() -> torch._inductor.codecache.LocalCache: ...
def get_cached_should_pad(key: str) -> bool: ...
def set_cached_should_pad(key: str, value: bool) -> None: ...
def get_cached_base_mm_benchmark_time(key: str) -> float: ...
def set_cached_base_mm_benchmark_time(key: str, value: float) -> None: ...
def should_pad_bench_key(match: Match, mat1: Tensor, mat2: Tensor, op: torch._ops.OpOverloadPacket, input: Tensor | None = None, is_base_time_key: bool = False) -> str: ...
def get_non_view_def(node: torch.fx.Node) -> torch.fx.Node: ...
def should_exclude_padding_time(match: Match, arg_name: str) -> bool: ...
def should_pad(key: str, ori_time: float, pad_time: float) -> bool: ...
def should_pad_mm_bf16(dtype: torch.dtype, M: int, N: int, K: int) -> bool: ...
def should_pad_bench(*args: Any, **kwargs: Any) -> bool: ...
def get_do_bench() -> Callable[[Callable[[], Any]], float]: ...
def _should_pad_bench(match: Match, mat1: Tensor, mat2: Tensor, op: torch._ops.OpOverloadPacket, input: Tensor | None = None) -> bool: ...
def get_context(mat1: Tensor, mat2: Tensor, mat1_pre_padded: bool, mat2_pre_padded: bool, m_padded_length: int, k_padded_length: int, n_padded_length: int) -> AHContext: ...
def run_autoheuristic(mat1: Tensor, mat2: Tensor, orig_bench_fn: Callable[[], None], pad_bench_fn: Callable[[], None], m_padded_length: int, k_padded_length: int, n_padded_length: int, do_bench: Callable[[Callable[[], Any]], float], mat1_pre_padded: bool, mat2_pre_padded: bool, ori_time: float, ori_time_key: str, key: str) -> bool | None: ...
def mm_pattern(mat1: Tensor, mat2: Tensor) -> Tensor: ...
def should_pad_mm(match: Match) -> bool: ...
def pad_mat1(mat1: Tensor, *, m_padded_length: int, k_padded_length: int, is_bmm: bool = False) -> Tensor: ...
def pad_mat2(mat2: Tensor, *, k_padded_length: int, n_padded_length: int, is_bmm: bool = False) -> Tensor: ...
def pad_mm(mat1: Tensor, mat2: Tensor, m_padded_length: int, k_padded_length: int, n_padded_length: int, mat1_pre_padded: bool = False, mat2_pre_padded: bool = False) -> Tensor: ...
def mm_replace(mat1: Tensor, mat2: Tensor) -> Tensor: ...
def bmm_pattern(mat1: Tensor, mat2: Tensor) -> Tensor: ...
def should_pad_bmm(match: Match) -> bool: ...
def pad_bmm(mat1: Tensor, mat2: Tensor, m_padded_length: int, k_padded_length: int, n_padded_length: int, mat1_pre_padded: bool = False, mat2_pre_padded: bool = False) -> Tensor: ...
def bmm_replace(mat1: Tensor, mat2: Tensor) -> Tensor: ...
@functools.cache
def _pad_mm_init() -> None: ...
