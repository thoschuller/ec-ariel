import functools
import torch
from .. import ir as ir
from ..pattern_matcher import Arg as Arg, CallFunction as CallFunction, KeywordArg as KeywordArg, MULTIPLE as MULTIPLE, filter_nodes as filter_nodes, get_arg_value as get_arg_value
from ..utils import SUPPORTED_MKLDNN_DEVICES as SUPPORTED_MKLDNN_DEVICES, is_mkldnn_bf16_supported as is_mkldnn_bf16_supported, is_mkldnn_fp16_supported as is_mkldnn_fp16_supported
from ..virtualized import V as V, ops as ops
from .freezing_patterns import register_freezing_graph_pattern as register_freezing_graph_pattern
from .post_grad import register_lowering_pattern as register_lowering_pattern
from .quantization import _register_int8_woq_concat_linear_pattern as _register_int8_woq_concat_linear_pattern, _register_quantization_lowerings as _register_quantization_lowerings, _register_quantization_weight_pack_pass as _register_quantization_weight_pack_pass, _register_woq_lowerings as _register_woq_lowerings
from _typeshed import Incomplete
from torch._dynamo.utils import counters as counters
from torch.fx.experimental.symbolic_shapes import has_free_symbols as has_free_symbols
from torch.utils._ordered_set import OrderedSet as OrderedSet

aten: Incomplete
mkldnn: Incomplete
prims: Incomplete
_conv_args: Incomplete
_linear_args: Incomplete
_conv_transpose_args: Incomplete

class MkldnnDeviceOpBase:
    def get_linear_transpose_weight(self, weight_node) -> None: ...
    def pack_conv_weight(self, graph, is_transposed, weight, constant_args, input_size) -> None: ...
    def pack_linear_weight(self, graph, is_lp_weight, transpose_weight_node, batch_size) -> None: ...
    def pack_linear(self, graph, is_lp_weight, batch_size, input, packed_weight_node, bias) -> None: ...

class CpuMkldnnDeviceOp(MkldnnDeviceOpBase):
    def get_linear_transpose_weight(self, weight_node): ...
    def pack_conv_weight(self, graph, is_transposed, weight, constant_args, input_size): ...
    def pack_linear_weight(self, graph, is_lp_weight, transpose_weight_node, batch_size): ...
    def pack_linear(self, graph, is_lp_weight, batch_size, input, packed_weight_node, bias): ...

class XpuMkldnnDeviceOp(MkldnnDeviceOpBase):
    def pack_conv_weight(self, graph, is_transposed, weight, constant_args, input_size): ...

def _get_mkldnn_device_op(device_type: str) -> MkldnnDeviceOpBase:
    """
        Returns the MKLDNN device operation class based on the current device type.
        """
def _is_valid_grouped_gemm_fusion(computation_nodes):
    """
        Here we check:
        1. More than 1 GEMM nodes has been found.
        2. All the GEMM nodes share the same activation.
        3. All the GEMM nodes have same weight size but different wgt node.
        """
def grouped_gemm_pass(graph: torch.fx.Graph):
    """
        Group GEMM has multi output nodes which is complicated to define a Pattern.
        Use below way to connect the pattern to the lowering.
        TODO: Use MultiOutputPattern, current limitation is the pattern requires
        fixed number of output nodes. Extend to support Group GEMM for pattern matcher.
        """
def _conv_call(users: int = 1): ...
def _linear_call(users: int = 1): ...
def _conv_transpose_call(users: int = 1): ...
def _to_float(input_call, users: int = 1): ...
def _to_bf16(input_call): ...
def _to_fp16(input_call): ...
def _unary_fusion_pattern(unary_fusion, call_fn, users, lowp_dtype): ...
def _gelu_fusion_1(computation_call): ...
def _gelu_fusion_2(computation_call): ...
def _hardswish_fusion(computation_call): ...
def _silu_fusion(computation_call): ...
def _hardsigmoid_fusion(computation_call): ...
def _leaky_relu_fusion(computation_call): ...
def _hardtanh_fusion(computation_call): ...
def _combined_fusion(computation_call, elementwise_op): ...
def _binary_fusion_v1(computation_call, binary_fn): ...
def _binary_fusion_v2(computation_call, binary_fn): ...
def _is_single_computation_op(computation_op, lowp_dtype=None): ...
def _is_valid_computation_unary_fusion(computation_op, lowp_dtype=None): ...
def _register_unary_fusion_lowering(pattern, unary_attr, computation_op, lowp_dtype=None): ...
def _register_leaky_relu_fusion_lowering(pattern, computation_op, lowp_dtype=None): ...
def _register_hardtanh_fusion_lowering(pattern, computation_op, lowp_dtype=None): ...

_binary_attr: Incomplete

def _is_valid_binary(match, computation_op, binary_op): ...
def _is_valid_computation_binary(computation_op, binary_op, other_index=None): ...
def _get_remaining_users(extra_input_node, compute_node): ...
def _is_valid_computation_binary_inplace(computation_op, binary_op, other_index): ...
def _register_binary_unary_fusion_lowering(pattern, computation_op, binary_op, fusion_op, unary_attr=None): ...
def _can_be_inplace(_other): ...
def _register_binary_unary_maybe_inplace_fusion_lowering(pattern, computation_op, binary_op, inplace_fusion_op, outplace_fusion_op, unary_attr=None, other_index=None): ...

computation_ops: Incomplete

class UnaryAttr:
    op_name: Incomplete
    scalars_attr: Incomplete
    algorithm_attr: Incomplete
    def __init__(self, op_name: str, scalars_attr=None, algorithm_attr=None) -> None: ...

def _register_unary_fusion(): ...
def _register_inplace_fusion() -> None: ...
def _register_binary_fusion() -> None: ...
def _register_binary_unary_fusion() -> None: ...
def _recover_linear(): ...
def _is_packable_mkldnn_rnn_layer(match): ...
def _is_packable_convolution(match):
    """
        Check if the node is supported for MKLDNN convolution.
        """
def _is_packable_linear(match):
    """
        Check if the node is supported for MKLDNN linear.
        """

_aten_conv_args: Incomplete
_aten_mkldnn_rnn_layer_args: Incomplete

def _register_weight_pack_pass(): ...
def _eliminate_duplicate_packed_nodes(gm):
    """
        Combine packed weight nodes with the same inputs to reduce memory usage.
        for example:
        class Model(nn.Module):
            def __init__(self) -> None:
                super().__init__()
                self.linear = nn.Linear(32, 32, bias=True)

            def forward(self, x):
                return self.linear(self.linear(x))

        the above's packed weight nodes are duplicate if two linear calls have same input size.
        """
@functools.cache
def _mkldnn_fusion_init() -> None: ...
@functools.cache
def _mkldnn_weight_pack_init() -> None: ...
