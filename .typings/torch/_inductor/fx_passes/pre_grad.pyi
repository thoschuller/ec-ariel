import torch
from .. import config as config
from ..fx_utils import matches_module_function_pattern as matches_module_function_pattern
from ..pattern_matcher import PatternMatcherPass as PatternMatcherPass, init_once_fakemode as init_once_fakemode, stable_topological_sort as stable_topological_sort
from ..utils import is_cpu_device as is_cpu_device, pass_execution_and_save as pass_execution_and_save
from .group_batch_fusion import PRE_GRAD_FUSIONS as PRE_GRAD_FUSIONS, group_batch_fusion_passes as group_batch_fusion_passes
from .misc_patterns import numpy_compat_normalization as numpy_compat_normalization
from .split_cat import PRE_GRAD_PATTERNS as PRE_GRAD_PATTERNS
from _typeshed import Incomplete
from collections.abc import Sequence
from torch._dynamo.utils import counters as counters, detect_fake_mode as detect_fake_mode
from torch._logging import trace_structured as trace_structured
from torch.fx.experimental.optimization import matches_module_pattern as matches_module_pattern, replace_node_module as replace_node_module
from torch.fx.passes.graph_transform_observer import GraphTransformObserver as GraphTransformObserver
from torch.fx.passes.shape_prop import ShapeProp as ShapeProp
from torch.nn.utils.fusion import fuse_conv_bn_eval as fuse_conv_bn_eval, fuse_conv_bn_weights as fuse_conv_bn_weights

log: Incomplete
efficient_conv_bn_eval_pass: Incomplete
fuse_split_linear_add_pass: Incomplete
fuse_chunk_squeeze_cat_pass: Incomplete
remove_reshape_pass: Incomplete
normalization_pass_aten: Incomplete
merge_splits_pass_aten: Incomplete
split_cat_pass_aten: Incomplete
unbind_stack_pass_aten: Incomplete
merge_getitem_cat_pass_aten: Incomplete
merge_stack_tahn_unbind_pass_aten: Incomplete
mutate_cat_pass_aten: Incomplete
remove_split_with_size_one_pass_aten: Incomplete

def save_inductor_dict(pass_to_compare=None): ...
def is_same_dict(inductor_dict, optimus_dict): ...
def shape_prop(mod) -> None: ...
def normalize_node_kwargs_pass(graph) -> None: ...
def fuse_parallel_linear_pass(graph) -> None: ...
def remove_split_ops(graph, shape_prop) -> None: ...
def remove_split_ops_pass(graph) -> None: ...
def fuse_chunk_reshape_unsqueeze_concat_pass(graph) -> None: ...
def fuse_chunk_reshape_concat_pass(graph) -> None: ...
def remove_noop_pass(graph) -> None: ...
def stack_to_unsqueeze_pass(graph) -> None: ...
def merge_concats_pass(graph) -> None: ...
def relu_nan_to_num(graph) -> None: ...
def fuse_split_getitem_squeeze_cat(graph) -> None: ...
def use_triton_dot_compress(graph) -> None: ...
def use_triton_lce_replace_simple_LCE_helper(gm, shape_prop) -> None: ...
def use_triton_lce_replace_simple_LCE(graph): ...
def use_triton_lce_replace_normal_LCE_helper(gm, shape_prop) -> None: ...
def use_triton_lce_replace_normal_LCE(graph): ...
def use_matmul_lce_replace_normal_LCE(graph) -> None: ...
def use_matmul_fuse_lce_replace_first_LCE(graph) -> None: ...
@init_once_fakemode
def lazy_init() -> None: ...
def _get_pass_name_func(p): ...
def _run_pre_dispatch_passes(gm: torch.fx.GraphModule, example_inputs: Sequence[object] = (), add_passes: str | None = None, remove_passes: str | None = None) -> None: ...
def pre_grad_passes(gm: torch.fx.GraphModule, example_inputs: Sequence[object] = (), add_passes: str | None = None, remove_passes: str | None = None) -> torch.fx.GraphModule:
    """
    Apply passes on the input FX graph using Torch IR.

    WARNING:
    The IR before grad is not functional or normalized, so it is harder
    to write passes on this IR.  Passes must be safe with respect to
    aliasing and mutation and need to handle all possible arg schemas.

    Consider adding a new pass to post_grad.py or joint_graph.py which
    are after functionalization and normalization.
    """
def fuse_fx(gm: torch.fx.GraphModule, example_inputs) -> torch.fx.GraphModule: ...
def fetch_attr(target: str, mod): ...
def remove_identity(gm: torch.fx.GraphModule) -> torch.fx.GraphModule:
    """
    Removes all identity layers from the module.
    """
def fuse_conv_bn(gm: torch.fx.GraphModule, inplace: bool = False) -> torch.fx.GraphModule:
    """
    Fuses Convolution/BN layers for inference purposes.
    """

class NormalizedLinearNode:
    node: torch.fx.Node
    def __init__(self, node: torch.fx.Node) -> None: ...
    def get_input(self) -> torch.fx.Node: ...
    def get_weight(self) -> torch.fx.Node: ...
    def get_bias(self) -> torch.fx.Node: ...

class NormalizedMatmulNode:
    node: torch.fx.Node
    def __init__(self, node: torch.fx.Node) -> None: ...
    def get_input(self) -> torch.fx.Node: ...
    def get_other(self) -> torch.fx.Node: ...

def check_permute(node: torch.fx.Node) -> bool: ...
def sink_cat_after_pointwise(module: torch.fx.GraphModule) -> torch.fx.GraphModule: ...
def linear_permute_fusion(module: torch.fx.GraphModule) -> torch.fx.GraphModule: ...
def linear_transpose(input: torch.Tensor, weight: torch.Tensor, bias: torch.Tensor | None) -> torch.Tensor: ...
def permute_linear_fusion(module: torch.fx.GraphModule) -> torch.fx.GraphModule: ...
def permute_matmul_fusion(module: torch.fx.GraphModule) -> torch.fx.GraphModule: ...
def transpose_linear(input: torch.Tensor, weight: torch.Tensor, bias: torch.Tensor | None) -> torch.Tensor: ...
def transpose_matmul(A: torch.Tensor, B: torch.Tensor, Atrans: bool, Btrans: bool) -> torch.Tensor: ...
