import functools
import torch
from . import config as config, inductor_prims as inductor_prims
from .utils import is_gpu as is_gpu, needs_fallback_due_to_atomic_add_limitations as needs_fallback_due_to_atomic_add_limitations, use_scatter_fallback as use_scatter_fallback
from _typeshed import Incomplete
from torch._decomp import core_aten_decompositions as core_aten_decompositions, get_decompositions as get_decompositions, remove_decompositions as remove_decompositions
from torch._decomp.decompositions import _index_add as _index_add, pw_cast_for_opmath as pw_cast_for_opmath, pw_cast_for_opmath_non_tensor_args as pw_cast_for_opmath_non_tensor_args
from torch._decomp.decompositions_for_rng import extra_random_decomps as extra_random_decomps
from torch._dynamo.utils import counters as counters
from torch._environment import is_fbcode as is_fbcode
from torch._higher_order_ops.out_dtype import out_dtype as out_dtype
from torch._inductor.utils import pad_listlike as pad_listlike
from torch._prims_common import ELEMENTWISE_TYPE_PROMOTION_KIND as ELEMENTWISE_TYPE_PROMOTION_KIND, elementwise_dtypes as elementwise_dtypes, type_to_dtype as type_to_dtype
from torch.fx.experimental.symbolic_shapes import guard_or_false as guard_or_false, guard_size_oblivious as guard_size_oblivious, statically_known_true as statically_known_true
from typing import Any, Callable, TypeVar
from typing_extensions import ParamSpec, TypeAlias

_T = TypeVar('_T')
_P = ParamSpec('_P')
_GenericOperator: TypeAlias = torch._ops.OperatorBase | torch._ops.OpOverloadPacket
log: Incomplete
aten: Incomplete
prims: Incomplete
quantized: Incomplete
_quantized: Incomplete
quantized_decomposed: Incomplete
inductor_decompositions: Incomplete
decompositions: Incomplete
decomps_to_exclude: list[torch._ops.OpOverload | torch._ops.OpOverloadPacket]

def register_decomposition(ops: _GenericOperator | list[_GenericOperator]) -> Callable[[Callable[_P, _T]], Callable[_P, _T]]: ...
def _embedding_dense_backward(grad_output: torch.Tensor, indices: torch.Tensor, num_weights: int, padding_idx: int, scale_grad_by_freq: bool) -> torch.Tensor: ...
def assert_async_msg_decomp(tensor: torch.Tensor, msg: str) -> None: ...
def functional_assert_async_msg_decomp(tensor: torch.Tensor, msg: str) -> None: ...
def sym_constrain_range_for_size(symbol: torch.SymInt, *, min: torch.types.Number | None = None, max: torch.types.Number | None = None) -> None: ...
@pw_cast_for_opmath_non_tensor_args
def clamp(x: torch.Tensor, min: torch.types.Number | None = None, max: torch.types.Number | None = None) -> torch.Tensor: ...
def full(size: list[int | torch.SymInt], fill_value: torch.types.Number, **kwargs: Any) -> torch.Tensor: ...
def index_add(x: torch.Tensor, dim: int, index: torch.Tensor, tensor: torch.Tensor, *, alpha: torch.types.Number = 1) -> torch.Tensor: ...
def empty_permuted(size: list[int | torch.SymInt], physical_layout: list[int], **kwargs: Any) -> torch.Tensor: ...
def convolution_backward(grad_output: torch.Tensor, input: torch.Tensor, weight: torch.Tensor, bias_sizes: list[int], stride: int | list[int], padding: int | list[int], dilation: int | list[int], transposed: bool, output_padding: list[int], groups: int, output_mask: list[bool]) -> tuple[torch.Tensor, torch.Tensor, torch.Tensor]: ...
def round_dec(x: torch.Tensor, decimals: int = 0) -> torch.Tensor: ...
@pw_cast_for_opmath
def bmm(self, batch2: torch.Tensor, out_dtype: torch.dtype | None = None) -> torch.Tensor: ...
@pw_cast_for_opmath
def addmm(self, mat1: torch.Tensor, mat2: torch.Tensor, out_dtype: torch.dtype | None = None, beta: torch.types.Number = 1, alpha: torch.types.Number = 1) -> torch.Tensor: ...
@pw_cast_for_opmath
def mm(self, input2: torch.Tensor, out_dtype: torch.dtype | None = None) -> torch.Tensor: ...
def cat(tensors: list[torch.Tensor], dim: int = 0) -> torch.Tensor: ...
def angle(x: torch.Tensor) -> torch.Tensor: ...
def add(x: torch.Tensor, y: torch.Tensor, *, alpha: torch.types.Number | None = None) -> torch.Tensor: ...
def conj_physical(self) -> torch.Tensor: ...
def lift(self) -> torch.Tensor: ...
def fmin(self, other: torch.Tensor) -> torch.Tensor: ...
def fmax(self, other: torch.Tensor) -> torch.Tensor: ...
def amax(self, dim: int | None = None, keepdim: bool = False) -> torch.Tensor: ...
def amin(self, dim: int | None = None, keepdim: bool = False) -> torch.Tensor: ...
def narrow_copy(self, dim: int, start: int, length: int) -> torch.Tensor: ...
def view_copy_default(self, size: list[int | torch.SymInt]) -> torch.Tensor: ...
def view_copy_dtype(self, dtype: torch.dtype) -> torch.Tensor: ...
def get_like_layout(tensor: torch.Tensor, memory_format: torch.memory_format | None = None) -> torch.memory_format: ...
def rand_like(self, *, dtype: torch.dtype | None = None, device: torch.device | None = None, memory_format: torch.memory_format | None = None, **kwargs: Any) -> torch.Tensor: ...
def randn_like(self, *, dtype: torch.dtype | None = None, device: torch.device | None = None, memory_format: torch.memory_format | None = None, **kwargs: Any) -> torch.Tensor: ...
def full_like(self, fill_value: int | float, *, dtype: torch.dtype | None = None, layout: torch.layout | None = None, device: torch.device | None = None, pin_memory: bool = False, requires_grad: bool = False, memory_format: torch.memory_format = ...) -> torch.Tensor: ...
def randint_like(self, high: int, *, dtype: torch.dtype | None = None, device: torch.device | None = None, memory_format: torch.memory_format | None = None, **kwargs: Any) -> torch.Tensor: ...
def randint_like_low(self, low: int, high: int, *, dtype: torch.dtype | None = None, device: torch.device | None = None, memory_format: torch.memory_format | None = None, **kwargs: Any) -> torch.Tensor: ...
def randint(high: int, size: list[int | torch.SymInt], **kwargs: Any) -> torch.Tensor: ...
def linear_dynamic_fp16_unpacked_weight(input: torch.Tensor, weight: torch.Tensor, bias: torch.Tensor) -> torch.Tensor: ...
def wrapped_quantized_linear(input: torch.Tensor, input_scale: torch.Tensor, input_zero_point: torch.Tensor, weight: torch.Tensor, weight_scale: torch.Tensor, weight_zero_point: torch.Tensor, bias: torch.Tensor, out_scale: torch.Tensor, out_zero_point: torch.Tensor, out_channel: int) -> torch.Tensor: ...
def q_embedding_bag_byte_unpack_decomp(packed: torch.Tensor) -> torch.Tensor: ...
@pw_cast_for_opmath
def grid_sampler_2d(a: torch.Tensor, grid: torch.Tensor, interpolation_mode: int = 0, padding_mode: int = 0, align_corners: bool = False) -> torch.Tensor: ...
def _foreach_addcmul_scalar(self, left_tensors: list[torch.Tensor], right_tensors: list[torch.Tensor], scalar: float = 1) -> list[torch.Tensor]: ...
def _foreach_addcdiv_scalar(self, left_tensors: list[torch.Tensor], right_tensors: list[torch.Tensor], scalar: float = 1) -> list[torch.Tensor]: ...
def _foreach_lerp_scalar(start_tensors: list[torch.Tensor], end_tensors: list[torch.Tensor], weight: torch.types.Number) -> list[torch.Tensor]: ...
def _foreach_lerp_scalarlist(start_tensors: list[torch.Tensor], end_tensors: list[torch.Tensor], scalars: list[torch.types.Number]) -> list[torch.Tensor]: ...
def miopen_batch_norm(input: torch.Tensor, weight: torch.Tensor, bias: torch.Tensor | None, running_mean: torch.Tensor | None, running_var: torch.Tensor | None, training: bool, exponential_average_factor: float, epsilon: float) -> tuple[torch.Tensor, torch.Tensor, torch.Tensor]: ...
@functools.cache
def fast_random_decomps() -> dict[Any, Callable[..., Any]]: ...
def select_decomp_table() -> dict[Any, Callable[..., Any]]:
    """decomps can change based on config"""
def masked_scatter(self, mask: torch.Tensor, source: torch.Tensor) -> torch.Tensor: ...
def choose_qparams_tensor(input: torch.Tensor, quant_min: int, quant_max: int, eps: float, dtype: torch.dtype) -> tuple[torch.Tensor, torch.Tensor]: ...
def put(self, index: torch.Tensor, source: torch.Tensor, accumulate: bool = False) -> torch.Tensor: ...
def put_(self, index: torch.Tensor, source: torch.Tensor, accumulate: bool = False) -> torch.Tensor: ...
@pw_cast_for_opmath
def _softmax_backward_data(grad_output: torch.Tensor, output: torch.Tensor, dim: int, input_dtype: torch.dtype) -> torch.Tensor: ...
def index_reduce(self, dim: int, index: torch.Tensor, src: torch.Tensor, reduction_type: str, *, include_self: bool = True) -> torch.Tensor: ...
def _max_pool_with_indices(x: torch.Tensor, kernel_size: list[int], stride: int | list[int] | None, padding: int | list[int], dilation: int | list[int], ceil_mode: bool, dim: int) -> tuple[torch.Tensor, torch.Tensor]: ...
def max_pool2d_with_indices(x: torch.Tensor, kernel_size: list[int], stride: int | list[int] | None = None, padding: int | list[int] = 0, dilation: int | list[int] = 1, ceil_mode: bool = False) -> tuple[torch.Tensor, torch.Tensor]: ...
def max_pool3d_with_indices(x: torch.Tensor, kernel_size: list[int], stride: int | list[int] | None = None, padding: int | list[int] = 0, dilation: int | list[int] = 1, ceil_mode: bool = False) -> tuple[torch.Tensor, torch.Tensor]: ...
def adaptive_max_pool2d(x: torch.Tensor, output_size: list[int]) -> tuple[torch.Tensor, torch.Tensor]: ...
def searchsorted_scalar(sorted_sequence: torch.Tensor, self: torch.types.Number, *, out_int32: bool = False, right: bool = False, side: str | None = None, sorter: torch.Tensor | None = None) -> torch.Tensor: ...
def rrelu_with_noise_functional(self, noise: torch.Tensor, lower: float = 0.125, upper: float = 0.3333333333333333, training: bool = False, generator: torch.Generator | None = None) -> tuple[torch.Tensor, torch.Tensor]: ...
