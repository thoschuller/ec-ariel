import contextlib
import dataclasses
import sympy
import torch
import torch._export.serde.schema as export_schema
import torch.fx
import torch.utils._pytree as pytree
from . import config as config, dependencies as dependencies
from .codegen.common import BackendFeature as BackendFeature, CodegenSymbol as CodegenSymbol, get_scheduling_for_device as get_scheduling_for_device, index_prevent_reordering as index_prevent_reordering
from .codegen.cuda.cuda_template import CUDATemplate as CUDATemplate
from .dependencies import Dep as Dep, extract_free_symbols as extract_free_symbols, extract_input_node_reduction_ranges as extract_input_node_reduction_ranges, extract_read_writes as extract_read_writes, var_builder as var_builder
from .graph import GraphLowering as GraphLowering
from .loop_body import LoopBody as LoopBody
from .ops_handler import OpCountResult as OpCountResult, OpCounterCSE as OpCounterCSE, ReductionType as ReductionType, StoreMode as StoreMode
from .runtime.benchmarking import benchmarker as benchmarker
from .runtime.hints import DeviceProperties as DeviceProperties, ReductionHint as ReductionHint
from .utils import GPU_ALIGN_BYTES as GPU_ALIGN_BYTES, IndentedBuffer as IndentedBuffer, argsort as argsort, argsort_sym as argsort_sym, cache_on_self as cache_on_self, ceildiv as ceildiv, convert_shape_to_inductor as convert_shape_to_inductor, convert_shape_to_symint as convert_shape_to_symint, developer_warning as developer_warning, do_bench_using_profiling as do_bench_using_profiling, dtype_from_size as dtype_from_size, get_dtype_size as get_dtype_size, get_kernel_metadata as get_kernel_metadata, ir_dataclass as ir_dataclass, is_dynamic as is_dynamic, is_gpu as is_gpu, sympy_dot as sympy_dot, sympy_index_symbol as sympy_index_symbol, sympy_index_symbol_with_prefix as sympy_index_symbol_with_prefix, sympy_product as sympy_product, sympy_subs as sympy_subs, tensor_is_aligned as tensor_is_aligned
from .virtualized import OpsValue as OpsValue, V as V, ops as ops
from _typeshed import Incomplete
from collections.abc import Generator, Iterable, Sequence
from enum import Enum
from sympy import Expr, Integer, Symbol as Symbol
from torch._dynamo.utils import identity as identity
from torch._export.serde.serialize import GraphModuleSerializer as GraphModuleSerializer
from torch._higher_order_ops.auto_functionalize import can_auto_functionalize as can_auto_functionalize
from torch._inductor import metrics as metrics
from torch._library.fake_class_registry import FakeScriptObject as FakeScriptObject
from torch._prims_common import StrideType as StrideType, compute_required_storage_length as compute_required_storage_length, is_boolean_dtype as is_boolean_dtype, is_float_dtype as is_float_dtype, make_channels_last_strides_for as make_channels_last_strides_for
from torch._subclasses.fake_tensor import get_schema_info as get_schema_info
from torch.fx.experimental.symbolic_shapes import IterateExprs as IterateExprs, ShapeEnv as ShapeEnv, SymTypes as SymTypes, _remove_effect_token_unbacked_bindings as _remove_effect_token_unbacked_bindings, compute_unbacked_bindings as compute_unbacked_bindings, free_symbols as free_symbols, free_unbacked_symbols as free_unbacked_symbols, rebind_unbacked as rebind_unbacked, resolve_unbacked_bindings as resolve_unbacked_bindings, statically_known_true as statically_known_true
from torch.fx.node import Node as Node
from torch.utils._ordered_set import OrderedSet as OrderedSet
from torch.utils._sympy.functions import CleanDiv as CleanDiv, FloorDiv as FloorDiv, ModularIndexing as ModularIndexing
from torch.utils._sympy.symbol import SymT as SymT
from typing import Any, Callable, ClassVar, Literal, TypeVar, overload
from typing_extensions import Never, TypeAlias

triton_version: Incomplete
has_triton: bool
_T = TypeVar('_T')
_U = TypeVar('_U')
_V = TypeVar('_V')
_IntLike: TypeAlias = int | Expr
_NumLike: TypeAlias = int | float | Expr
log: Incomplete
indent: Incomplete
aten: Incomplete
_NodeOrNodes: TypeAlias

def _is_static(x: object) -> bool: ...

@dataclasses.dataclass(frozen=True)
class GraphPartitionSignature:
    symbol_inputs: OrderedSet[sympy.Symbol]
    input_nodes: dict[str, IRNode | sympy.Expr | TorchBindObject]
    output_nodes: list[IRNode]
    input_deallocation: dict[str, bool]
    skip_cudagraph: bool
    constant_names: list[str]

def validate_ir(node_or_nodes: _NodeOrNodes | None) -> None: ...
def ops_wrapper(name: str) -> Callable[..., OpsValue]: ...
def inverse_reorder(order: Sequence[int]) -> Callable[[Sequence[_T]], Sequence[_T]]: ...
def same_reorder(order: Sequence[int]) -> Callable[[Sequence[_T]], Sequence[_T]]: ...
def fuse_reindexing(reindex1: Callable[[Sequence[_U]], Sequence[_V]], reindex2: Callable[[Sequence[_T]], Sequence[_U]]) -> Callable[[Sequence[_T]], Sequence[_V]]: ...
def get_free_symbols(x: IterateExprs, unbacked_only: bool) -> OrderedSet[sympy.Symbol]: ...

NHWC_STRIDE_ORDER: Incomplete
NHWDC_STRIDE_ORDER: Incomplete

def get_fill_order(seq: Sequence[int | torch.SymInt | Expr], shape_env: ShapeEnv | None = None) -> Sequence[int]:
    """
    Convert strides to fill order (argsort)
    """
def stride_order2fill_order(order: Sequence[int | Integer]) -> Sequence[int]:
    """
    Convert stride order to fill order
    For channel last format,

    stride order = [3, 0, 2, 1] and fill order = [1, 3, 2, 0]
    """
def get_stride_order(seq: Sequence[int | torch.SymInt | Expr], shape_env: ShapeEnv | None = None) -> Sequence[int]:
    """
    Convert strides to stride order
    """
@overload
def ir_node_to_tensor(x: Literal[None], guard_shape: bool = True) -> None: ...
@overload
def ir_node_to_tensor(x: IRNode, guard_shape: bool = True) -> torch.Tensor: ...
def may_convert_to_optional(value: Sequence[_T] | None) -> Sequence[_T | None] | None: ...
def get_device_type(x: IRNode | OutputSpec | torch.device | None | str) -> str | None: ...
def is_triton(x: IRNode | torch.device | None | str) -> bool: ...
def is_cpu(x: IRNode | torch.device | None | str) -> bool: ...
def is_aligned_realized_tensor(x: Buffer | TensorBox, alignment: int) -> bool: ...
def significant_strides_equal(strides1: Sequence[_IntLike], strides2: Sequence[_IntLike], shape: Sequence[_IntLike]) -> bool:
    """
    Returns true if the strides are equal, ignoring dimensions of size 1 .
    """
def try_match_insignificant_strides(tensor: TensorBox | BaseView, strides: Sequence[int | torch.SymInt]) -> TensorBox | BaseView:
    """
    Tries to match the strides of the tensor to those in the meta_strides. Strides of insignificant
    dimensions - size 0 or 1 - will be updated.

    If there are real stride differences (NHWC vs NCHW), or the tensor is not realized, then the input will be returned
    """
def gm_original_output_strides(gm: torch.fx.GraphModule) -> None: ...
def get_symbolic_inputs(inputs: list[Buffer]) -> list[Expr]: ...

class IRNode:
    _current_origins: ClassVar[OrderedSet[Any]]
    origins: OrderedSet[Any]
    traceback: list[str] | None
    origin_node: torch.fx.Node | None
    @staticmethod
    @contextlib.contextmanager
    def current_origins(origins: OrderedSet[Node]) -> Generator[None, None, None]: ...
    def _post_init_setattr(self, attr: str, value: Any) -> None: ...
    def __post_init__(self) -> None: ...
    def get_read_names(self) -> OrderedSet[str]: ...
    def get_traceback(self) -> list[str] | None: ...
    def get_origin_node(self) -> torch.fx.Node | None: ...
    def get_defining_op(self) -> Operation | None: ...
    def common_repr(self, shorten: bool = True) -> Sequence[str]: ...
    def str_helper(self, lines: Sequence[object], shorten: bool = True, multiline: bool = True) -> str: ...
    def get_dtype(self) -> torch.dtype: ...
    def maybe_get_dtype(self) -> torch.dtype | None: ...
    def get_layout(self) -> Layout: ...
    def maybe_get_layout(self) -> Layout | None: ...
    def get_output_spec(self) -> OutputSpec: ...
    def maybe_get_output_spec(self) -> OutputSpec | None: ...
    def has_tensor_output(self) -> bool:
        """True for single tensor output (excludes MultiOutput)"""
    def get_size(self) -> Sequence[Expr]: ...
    def maybe_get_size(self) -> Sequence[_IntLike] | None: ...
    @property
    def shape(self) -> _IntLike | sympy.Rel | Sequence[_IntLike]: ...
    def get_numel(self) -> Expr: ...
    def is_zero_elements(self) -> bool: ...
    def realize(self) -> str | None:
        """
        If the IRNode refers to data which has not been materialized (e.g.,
        it is a Pointwise/Reduction that could potentially have more
        compute fused into it), realize the IRNode into physical memory,
        ending the possibility of fusing into it, but allowing, e.g., multiple
        users to access the data without having to recompute.

        Check StorageBox.realize for a particularly notable implementation.

        TODO(ezyang): I think, in principle, every IRNode should have an
        implementation of this, and most of the time no-op is OK, but you
        really do have to audit each IRNode for this, so for now, raise
        an error if it's not implemented.  Note that some code in graph.py
        will catch this thrown error and suppress it with a warning.
        """
    def codegen_reference(self, writer: IndentedBuffer | None = None) -> str: ...
    def get_device(self) -> torch.device | None: ...
    def get_device_or_error(self) -> torch.device: ...
    def has_exceeded_max_reads(self) -> bool: ...
    def make_loader(self) -> Callable[[Sequence[Expr]], OpsValue]: ...
    def make_indexer(self) -> Callable[[Sequence[Expr]], Expr]: ...
    def get_stride(self) -> Sequence[_IntLike]: ...
    def maybe_get_stride(self) -> Sequence[_IntLike] | None: ...
    def get_name(self) -> str: ...
    def maybe_get_name(self) -> str | None: ...
    def is_input_buffer(self) -> bool: ...
    def has_large_inner_fn(self, threshold: int | None = None) -> bool: ...
    def mark_reuse(self, users: int) -> None: ...
    def realize_hint(self) -> None: ...
    def unwrap_view(self) -> IRNode: ...
    def freeze_layout(self) -> None: ...
    def freeze_layout_with_stride_order(self, order: list[int], allow_padding: bool = False) -> None: ...
    def freeze_layout_with_fill_order(self, order: list[int]) -> None: ...
    def freeze_layout_with_same_order(self, stride: list[_IntLike]) -> None: ...
    def freeze_layout_with_exact_strides(self, exact_strides: list[_IntLike], allow_padding: bool = False) -> None: ...
    def get_read_writes(self) -> dependencies.ReadWrites: ...
    def get_reads(self) -> OrderedSet[Dep]: ...
    def num_reads(self) -> int: ...
    def get_storage_numel(self) -> _IntLike: ...
    def get_free_symbol_uses(self, unbacked_only: bool = False) -> OrderedSet[sympy.Symbol]: ...
    def get_reduction_type(self) -> str | None: ...
    def get_reduction_size(self) -> Sequence[sympy.Expr]: ...
    def is_extern(self) -> bool: ...
    def is_no_op(self) -> bool: ...
    def constant_to_device(self, device: torch.device) -> IRNode: ...
    def get_mutation_names(self) -> Sequence[str]: ...
    def get_operation_name(self) -> str: ...
    def get_inputs_that_alias_output(self) -> Sequence[str]: ...
    @property
    def dtype(self) -> torch.dtype: ...

@ir_dataclass(frozen=False)
class Operation:
    operation_name: str | None = ...
    def __post_init__(self) -> None: ...
    def get_device(self) -> torch.device | None: ...
    def get_origin_node(self) -> torch.fx.Node | None: ...
    def get_origins(self) -> OrderedSet[Any]: ...
    def get_operation_name(self) -> str: ...
    def is_extern(self) -> bool: ...
    def is_no_op(self) -> bool: ...
    def get_read_writes(self) -> dependencies.ReadWrites: ...
    def is_user_of(self, name: str) -> bool: ...
    def get_read_names(self) -> OrderedSet[str]: ...
    def get_reads(self) -> OrderedSet[Dep]: ...
    def get_outputs(self) -> list[Buffer]: ...
    def get_unbacked_symbol_defs(self) -> OrderedSet[sympy.Symbol]: ...
    def get_free_symbol_uses(self, unbacked_only: bool = False) -> OrderedSet[sympy.Symbol]:
        """
        When unbacked_only=True:
        Returns the unbacked symbols which are required to be in scope in
        order to successfully perform codegen for this buffer.  For example,
        a buffer that corresponds to an extern kernel call that takes i0 as
        an argument would return {i0} here.  This is used to generate necessary
        dependencies that ensure we actually bind i0 in codegen before you
        try to use it.

        Note that this is NOT transitive; in particular, if this buffer takes
        in as input another buffer with dynamic shape (e.g., (i0,)), we will
        not report it here, because you will already have a dependency
        on that buffer, which will eventually have a dependency on i0 if
        necessary.

        When unbacked_only=False:
        Similar to `unbacked_only=True` but including all free symbols
        instead of only free unbacked symbols.
        """
    def get_workspace_size(self) -> int:
        """
        Gets extra global memory size needed by this buffer.
        Some algorithms (e.g. group gemm) may require extra global memory in the generated code.
        """

@ir_dataclass
class Loops(IRNode):
    device: torch.device
    dtype: torch.dtype
    inner_fn: Callable[..., Any]
    ranges: Sequence[_IntLike]
    def get_free_symbol_uses(self, unbacked_only: bool = False) -> OrderedSet[sympy.Symbol]: ...
    def _to_str(self, names: Sequence[str]) -> str: ...
    def __post_init__(self) -> None: ...
    def __str__(self) -> str: ...
    __repr__ = __str__
    def get_device(self) -> torch.device | None: ...
    def get_origin_node(self) -> torch.fx.Node | None: ...
    def get_size(self) -> Sequence[Expr]: ...
    def get_pointwise_size(self) -> Sequence[Expr]: ...
    @classmethod
    def create(cls, *args: Any, **kwargs: Any) -> TensorBox: ...
    @staticmethod
    def _index(ranges: Sequence[_IntLike], prefix: SymT = ...) -> Sequence[Expr]: ...
    @cache_on_self
    def inner_fn_opcount(self) -> OpCountResult: ...
    def inner_fn_args(self) -> Sequence[Sequence[_IntLike]]: ...
    @cache_on_self
    def inner_fn_str(self) -> str: ...
    def has_large_inner_fn(self, threshold: int | None = None) -> bool: ...
    def inner_fn_free_symbols(self, unbacked_only: bool = False) -> OrderedSet[Symbol]: ...
    def get_reads(self) -> OrderedSet[Dep]: ...
    def get_read_names(self) -> OrderedSet[str]: ...
    def num_reads(self) -> int: ...
    def get_reduction_size(self) -> Sequence[sympy.Expr]: ...
    def get_reduction_type(self) -> str | None: ...
    def constant_to_device(self, device: torch.device) -> IRNode: ...

def nop_loader_fn(idx: Expr | Sequence[Expr], *, dtype: torch.dtype) -> OpsValue: ...

@ir_dataclass
class Pointwise(Loops):
    def make_loader(self) -> Callable[[Sequence[Expr]], OpsValue]: ...
    def get_reduction_size(self) -> Sequence[sympy.Expr]: ...
    def get_reduction_type(self) -> str | None: ...
    def store_output(self, output_name: str | None, indexer: Callable[[Sequence[Expr]], Never], vars: Sequence[Expr]) -> None: ...
    def constant_to_device(self, device: torch.device) -> IRNode:
        """Move this to a given device. Requires that all reads are to constants."""

@ir_dataclass
class Scatter(Pointwise):
    output_indexer: Callable[[Sequence[Expr]], Expr]
    scatter_mode: StoreMode = ...
    def constant_to_device(self, device: torch.device) -> IRNode:
        """Move this to a given device. Requires that all reads are to constants."""
    def store_output(self, output_name: str | None, indexer: Callable[[Sequence[Expr]], Never], vars: Sequence[Expr]) -> None: ...

REDUCTION_COMBINE_FN: dict[str, Callable[..., OpsValue]]

def get_reduction_combine_fn(reduction_type: str, dtype: torch.dtype, arg_break_ties_left: bool = True) -> Callable[..., object]: ...

@ir_dataclass
class Reduction(Loops):
    reduction_ranges: Sequence[_IntLike]
    reduction_type: ReductionType
    src_dtype: torch.dtype
    reduction_hint: ReductionHint
    def __str__(self) -> str: ...
    __repr__ = __str__
    def get_free_symbol_uses(self, unbacked_only: bool = False) -> OrderedSet[Symbol]: ...
    def get_reduction_size(self) -> Sequence[sympy.Expr]: ...
    def get_reduction_type(self) -> str | None: ...
    def store_reduction(self, output_name: str | None, indexer: Callable[[Sequence[Expr]], Never], vars: Sequence[Expr], reduction_vars: Sequence[Symbol]) -> None: ...
    def index_length(self) -> int: ...
    def inner_fn_args(self) -> Sequence[Sequence[Expr]]: ...
    def inner_fn_free_symbols(self, unbacked_only: bool = False) -> OrderedSet[Symbol]: ...
    def constant_to_device(self, device: torch.device) -> IRNode:
        """Move this to a given device. Requires that all reads are to constants."""
    @staticmethod
    def num_splits(device: torch.device, dst_dtype: torch.dtype, src_dtype: torch.dtype, inner_fn: Callable[..., OpsValue], ranges: Sequence[_IntLike], reduction_ranges: Sequence[_IntLike], reduction_type: ReductionType | Literal['scan'], reduction_numel: Expr, input_node: IRNode | None = None) -> tuple[ReductionHint, _IntLike]: ...
    @staticmethod
    def _unroll_reduction_fn(inner_fn: Callable[[Sequence[_IntLike], Sequence[_IntLike]], OpsValue], reduction_ranges: Sequence[_IntLike], reduction_type: str, src_dtype: torch.dtype) -> Callable[[Sequence[_IntLike]], OpsValue]:
        """Convert inner_fn from a reduction to an pointwise"""
    @classmethod
    def create(cls, device: torch.device, dst_dtype: torch.dtype, src_dtype: torch.dtype, inner_fn: Callable[..., Any], ranges: Sequence[Expr], reduction_ranges: Sequence[Expr], reduction_type: ReductionType, reduction_hint: ReductionHint = ..., input_node: IRNode | None = None) -> TensorBox: ...
    @staticmethod
    def default_accumulator(reduction_type: str, dtype: torch.dtype) -> _NumLike | Sequence[_NumLike]: ...
    @staticmethod
    def default_value(reduction_type: str, dtype: torch.dtype) -> _NumLike | Sequence[_NumLike]: ...
    @staticmethod
    def _multilayer_second_step_hint(split: _IntLike, numel_hint: int, reduction_hint: ReductionHint) -> ReductionHint: ...
    @classmethod
    def check_for_split_dense_dim_reindexing(cls, reduction_numel: _IntLike, input_node: IRNode | None) -> int | None:
        """
        If we are reducing over the full tensor, and it is non-dense in the last dimension,
        reindex so we reduce over the dense dimension. initially just handle complete
        reduction case
        """
    @classmethod
    def _multilayer_wrap_loader(cls, loader: Callable[..., OpsValue], reduction_ranges: Sequence[_IntLike], reduction_numel: _IntLike, split: _IntLike, block_size: _IntLike, default: _NumLike | Sequence[_NumLike], input_node: IRNode | None = None) -> Callable[..., object]: ...
    @classmethod
    def _multilayer_wrap_loader_existing_ranges(cls, loader: Callable[[Sequence[sympy.Expr], Sequence[sympy.Expr]], OpsValue], original_ranges: Sequence[Expr], original_reduction_ranges: Sequence[Expr], new_ranges: Sequence[Integer], new_reduction_ranges: Sequence[Integer]) -> Callable[[Sequence[sympy.Expr], Sequence[sympy.Expr]], OpsValue]: ...
    @classmethod
    def create_multilayer_helper(cls, device: torch.device, dst_dtype: torch.dtype, src_dtype: torch.dtype, wrapper_fn: Callable[..., Any], original_ranges: Sequence[Expr], original_reduction_ranges: Sequence[Expr], new_ranges: list[Expr], new_reduction_ranges: list[Integer], reduction_type: ReductionType, split: _IntLike, reduction_hint: ReductionHint) -> TensorBox:
        """
        Break a large reduction up into multiple smaller reductions
        recursively
        """
    @classmethod
    def create_multilayer(cls, device: torch.device, dst_dtype: torch.dtype, src_dtype: torch.dtype, inner_fn: Callable[..., Any], ranges: Sequence[Expr], reduction_ranges: Sequence[Expr], reduction_type: ReductionType, split: _IntLike, reduction_hint: ReductionHint, input_node: IRNode | None = None) -> TensorBox:
        """
        Break a large reduction up into multiple smaller reductions
        recursively
        """
    @classmethod
    def create_multilayer_existing_ranges(cls, device: torch.device, dst_dtype: torch.dtype, src_dtype: torch.dtype, inner_fn: Callable[..., Any], original_ranges: Sequence[Expr], original_reduction_ranges: Sequence[Expr], new_ranges: list[Integer], new_reduction_ranges: list[Integer], reduction_type: ReductionType, reduction_hint: ReductionHint) -> TensorBox:
        """
        Break a large reduction up into multiple smaller reductions
        recursively
        """
INNER_FN_TY = Callable[[Sequence[Expr], Sequence[Expr]], OpsValue]

class MultiOutputReduction(Reduction):
    output_index: int
    def __init__(self, device: torch.device, dst_dtype: torch.dtype, inner_fns: INNER_FN_TY | Sequence[INNER_FN_TY], ranges: Sequence[Integer], reduction_ranges: Sequence[Integer], reduction_type: ReductionType, src_dtype: torch.dtype, reduction_hint: ReductionHint, output_index: int) -> None: ...
    def store_reduction(self, output_name: str | None, indexer: Callable[[Sequence[Expr]], Never], vars: Sequence[Expr], reduction_vars: Sequence[Symbol]) -> None: ...

class OnlineSoftmaxReduction(MultiOutputReduction):
    @classmethod
    def create(cls, device: torch.device, dst_dtype: torch.dtype, src_dtype: torch.dtype, inner_fn: Callable[..., Any], ranges: Sequence[Expr], reduction_ranges: Sequence[Expr], num_output: int, reduction_hint: ReductionHint = ..., input_node: IRNode | None = None) -> Sequence[TensorBox]:
        """
        Create the reduction disregarding splitting.
        """

class WelfordReduction(MultiOutputReduction):
    @classmethod
    def create(cls, device: torch.device, dtype: torch.dtype, inner_fns: Sequence[Callable[..., Any]], ranges: list[Integer], reduction_ranges: list[Integer], reduction_type: ReductionType, reduction_hint: ReductionHint = ...) -> Sequence[TensorBox]: ...
    @staticmethod
    def default_value(reduction_type: str, dtype: torch.dtype) -> _NumLike | Sequence[_NumLike]: ...
    @classmethod
    def create_multilayer(cls, device: torch.device, dtype: torch.dtype, inner_fns: Sequence[Callable[..., Any]], ranges: list[Integer], reduction_ranges: list[Integer], reduction_type: ReductionType, split: _IntLike, reduction_hint: ReductionHint) -> Sequence[TensorBox]:
        """
        Break a large reduction up into multiple smaller reductions
        recursively
        """

@ir_dataclass
class Scan(Loops):
    scan_ranges: list[Integer]
    size: list[Integer]
    combine_fn: Callable[[tuple[Any, ...], tuple[Any, ...]], tuple[Any, ...]]
    reindex: Callable[[Sequence[_IntLike], Sequence[_IntLike]], Sequence[_IntLike]]
    reduction_hint: ReductionHint
    output_index: int
    dtypes: tuple[torch.dtype, ...]
    inner_fns: tuple[Callable[..., Any], ...]
    def get_free_symbol_uses(self, unbacked_only: bool = False) -> OrderedSet[Symbol]: ...
    def __post_init__(self) -> None: ...
    def store_reduction(self, output_name: str | None, indexer: Callable[[Sequence[_IntLike]], Never], vars: Sequence[Expr], scan_vars: Sequence[Symbol]) -> None: ...
    def get_reduction_type(self) -> str | None: ...
    def get_reduction_size(self) -> Sequence[sympy.Expr]: ...
    def get_size(self) -> Sequence[Expr]: ...
    def get_pointwise_size(self) -> Sequence[Expr]: ...
    def index_length(self) -> int: ...
    def inner_fn_args(self) -> Sequence[Sequence[_IntLike]]: ...
    def inner_fn_free_symbols(self, unbacked_only: bool = False) -> OrderedSet[Symbol]: ...
    @classmethod
    def create(cls, device: torch.device, dtypes: tuple[torch.dtype, ...], inner_fns: tuple[Callable[[Sequence[Expr]], Any], ...], size: list[Integer], axis: int, combine_fn: Callable[[tuple[Any, ...], tuple[Any, ...]], tuple[Any, ...]], reduction_hint: ReductionHint = ..., *, can_fallback_to_aten: bool = True, **kwargs: Any) -> Sequence[TensorBox | None]: ...
    @classmethod
    def num_splits(cls, device: torch.device, dtype: torch.dtype, inner_fn: Callable[[Sequence[Expr]], OpsValue], axis: int, pointwise_ranges: list[Integer], scan_ranges: list[Integer], combine_fn: Callable[[tuple[Any, ...], tuple[Any, ...]], tuple[Any, ...]], scan_numel: Expr) -> tuple[ReductionHint, _IntLike]: ...

@ir_dataclass
class SplitScan(Scan): ...

@ir_dataclass
class Sort(Loops):
    sort_ranges: list[Integer]
    size: list[Integer]
    reindex: Callable[[Sequence[Expr], Sequence[Expr]], Sequence[Expr]]
    reduction_hint: ReductionHint
    output_index: int
    dtypes: tuple[torch.dtype, ...]
    inner_fns: tuple[Callable[..., Any], ...]
    stable: bool
    descending: bool
    def get_free_symbol_uses(self, unbacked_only: bool = False) -> OrderedSet[Symbol]: ...
    def __post_init__(self) -> None: ...
    def store_reduction(self, output_name: str | None, indexer: Callable[[Sequence[Expr]], Expr], vars: Sequence[Expr], reduction_vars: Sequence[Expr]) -> None: ...
    def get_reduction_type(self) -> str | None: ...
    def get_reduction_size(self) -> Sequence[Expr]: ...
    def get_size(self) -> Sequence[Expr]: ...
    def get_pointwise_size(self) -> Sequence[Expr]: ...
    def index_length(self) -> int: ...
    def inner_fn_args(self) -> Sequence[Sequence[Expr]]: ...
    def inner_fn_free_symbols(self, unbacked_only: bool = False) -> OrderedSet[Symbol]: ...
    @classmethod
    def create(cls, device: torch.device, dtypes: tuple[torch.dtype, ...], inner_fns: tuple[Callable[[list[Expr]], Any], ...], size: list[Integer], axis: int, stable: bool, descending: bool, reduction_hint: ReductionHint = ..., **kwargs: Any) -> Sequence[TensorBox | None]: ...

def is_storage_and_layout(x: IRNode) -> bool: ...
def is_contiguous_storage_and_layout(x: IRNode) -> bool: ...
def as_storage_and_layout(x: IRNode, freeze: bool = True, want_contiguous: bool = False, stride_order: Sequence[int | Integer] | None = None, allow_padding: bool = False, exact_strides: Sequence[int | Integer] | None = None) -> tuple[StorageBox, Layout]:
    """
    Try to simplify x into a StorageBox and a Layout.

    allow_padding only affect how we apply stride_order. When allow_padding
    is True, we have the freedom to add padding when applying the stride_order.
    """
def is_stride_order_storage_and_layout(x: IRNode, stride_order: Sequence[int | Integer]) -> bool: ...
def is_unaligned(node: IRNode) -> bool: ...

@ir_dataclass
class BaseView(IRNode):
    data: IRNode
    def get_free_symbol_uses(self, unbacked_only: bool = False) -> OrderedSet[Symbol]: ...
    def make_reindexer(self) -> Callable[[Sequence[Expr]], Sequence[Expr]]: ...
    def make_indexer(self) -> Callable[[Sequence[Expr]], Expr]: ...
    def make_loader(self) -> Callable[[Sequence[Expr]], OpsValue]: ...
    @property
    def dtype(self) -> torch.dtype: ...
    def get_layout(self) -> Layout: ...
    def get_device(self) -> torch.device | None: ...
    def get_origin_node(self) -> torch.fx.Node | None: ...
    def get_name(self) -> str: ...
    def get_pointwise_size(self) -> Sequence[Expr]: ...
    def mark_reuse(self, users: int) -> None: ...
    def has_exceeded_max_reads(self) -> bool: ...
    def realize(self) -> str | None: ...
    def realize_hint(self): ...
    def get_storage_numel(self): ...
    def is_extern(self) -> bool: ...
    def is_module_buffer(self) -> bool: ...
    def get_read_names(self) -> OrderedSet[str]: ...
    def get_reads(self) -> OrderedSet[Dep]: ...
    def unwrap_view(self): ...
    def constant_to_device(self, device: torch.device) -> IRNode:
        """Move this to a given device. Requires that all reads are to constants."""

@ir_dataclass
class ExpandView(BaseView):
    size: list[Expr]
    @staticmethod
    def _normalize_size(x, new_size):
        """Replace `-1` with correct sizes"""
    @classmethod
    def create(cls, x, new_size): ...
    def get_size(self) -> Sequence[Expr]: ...
    def make_reindexer(self): ...

@ir_dataclass
class PermuteView(BaseView):
    dims: list[Expr]
    @classmethod
    def create(cls, x, dims): ...
    @classmethod
    def _map_neg_dims(cls, dims): ...
    def get_size(self) -> Sequence[Expr]: ...
    def make_reindexer(self): ...

@ir_dataclass
class SqueezeView(BaseView):
    @classmethod
    def create(cls, x, *, dim=None): ...
    @staticmethod
    def squeezer(size: Sequence[sympy.Expr]): ...
    def __init__(self, data) -> None: ...

@ir_dataclass
class GenericView(BaseView):
    size: list[Expr]
    reindex: Callable[..., Any]
    def make_reindexer(self): ...
    def reindex_str(self) -> str: ...
    def __str__(self) -> str: ...
    __repr__ = __str__
    @classmethod
    def create(cls, x, new_size, reindex): ...
    def get_size(self) -> Sequence[Expr]: ...

@ir_dataclass
class View(GenericView):
    @staticmethod
    def handle_negative_index(idx, size): ...
    @classmethod
    def create(cls, x, new_size): ...
    @staticmethod
    def resolve_negative_size(old_size, new_size): ...
    @classmethod
    def dynamic_reshape_indexer(cls, old_size: Sequence[_IntLike], new_size: Sequence[_IntLike], dense_dim: int | None = None) -> Callable[[Sequence[_T]], Sequence[_V]]: ...
    @staticmethod
    def _dynamic_reshape_indexer(old_size, new_size, dense_dim: int | None = None):
        """
        Perform a reshape entirely by modifying indexing math
        """

@ir_dataclass
class ReinterpretView(BaseView):
    """Pretend our storage has a different layout"""
    layout: Layout
    def __post_init__(self) -> None: ...
    def __str__(self) -> str: ...
    __repr__ = __str__
    def get_name(self) -> str: ...
    def get_device(self) -> torch.device | None: ...
    def get_origin_node(self) -> torch.fx.Node | None: ...
    @property
    def dtype(self): ...
    def get_size(self) -> Sequence[Expr]: ...
    def get_stride(self): ...
    def make_loader(self) -> Callable[[Sequence[Expr]], OpsValue]: ...
    def make_indexer(self) -> Callable[[Sequence[Expr]], Expr]: ...
    def get_layout(self) -> Layout: ...
    def freeze_layout(self) -> None: ...
    def get_free_symbol_uses(self, unbacked_only: bool = False) -> OrderedSet[sympy.Symbol]: ...
    def codegen_reference(self, writer: IndentedBuffer | None = None) -> str: ...
    def num_reads(self) -> int: ...

@ir_dataclass
class DtypeView(BaseView):
    """Pretend our storage has a different type"""
    target_dtype: torch.dtype
    @classmethod
    def create(cls, x, new_dtype): ...
    def __str__(self) -> str: ...
    __repr__ = __str__
    @property
    def dtype(self): ...
    def get_size(self) -> Sequence[Expr]: ...
    def make_loader(self) -> Callable[[Sequence[Expr]], OpsValue]: ...

class SliceView(View):
    @classmethod
    def normalize_start_end(cls, x, dim, start, end):
        """
        Normalize start and end such that both are in the range
        [0, x.get_size()[dim]] and start <= end.
        """
    @classmethod
    def create(cls, x, dim, start, end, step: int = 1, clamp: bool = True): ...

@ir_dataclass
class BaseConstant(IRNode):
    dtype: torch.dtype
    device: torch.device
    def get_size(self) -> Sequence[Expr]: ...
    def get_device(self) -> torch.device | None: ...
    def get_origin_node(self) -> torch.fx.Node | None: ...
    def get_reads(self) -> OrderedSet[Dep]: ...

@ir_dataclass
class Constant(BaseConstant):
    value: Any
    dtype: torch.dtype
    device: torch.device
    def make_loader(self) -> Callable[[Sequence[Expr]], OpsValue]: ...
    def realize(self) -> str | None: ...
    def constant_to_device(self, device: torch.device) -> IRNode: ...

@ir_dataclass
class IndexingConstant(BaseConstant):
    index: Any
    dtype: torch.dtype
    device: torch.device
    def make_loader(self) -> Callable[[Sequence[Expr]], OpsValue]: ...
    def constant_to_device(self, device: torch.device) -> IRNode: ...

def is_contiguous_strides_for_shape(stride: Sequence[_IntLike], shape: Sequence[_IntLike]) -> bool: ...
def get_align_for_dtype(dtype: torch.dtype) -> int: ...

class OutputSpec:
    """Abstract base for Layout, MultiOutputLayout, NoneLayout.
    Represents the memory layout of the output of an Operation."""
    def get_device(self) -> torch.device | None: ...
    def storage_size(self) -> int: ...

@ir_dataclass
class Layout(OutputSpec):
    device = ...
    dtype = ...
    size: list[Expr] = ...
    stride: list[Expr] = ...
    offset: Expr = ...
    def __init__(self, device: torch.device, dtype: torch.dtype, size: list[Expr], stride: list[Expr] | None = None, offset: Expr = ...) -> None: ...
    def __str__(self) -> str: ...
    __repr__ = __str__
    def get_device(self) -> torch.device: ...
    def get_example(self) -> torch.Tensor: ...
    def is_contiguous(self) -> bool: ...
    @staticmethod
    def is_channels_last_contiguous(shape: Sequence[_IntLike], strides: Sequence[_IntLike]) -> bool: ...
    def is_transposed(self) -> bool: ...
    def is_stride_ordered(self, order) -> bool: ...
    def is_channels_last_stride_ordered(self): ...
    @staticmethod
    def _pad_strides(in_strides, size, dtype):
        """
        The padding does not change stride order but makes sure all strides larger
        than the threshold are multiple of align.
        """
    def pad_strides(self) -> None: ...
    def should_pad_strides(self): ...
    def as_fixed(self): ...
    def make_indexer(self) -> Callable[[Sequence[Expr]], Expr]: ...
    def __eq__(self, other) -> bool: ...
    def storage_size(self) -> sympy.Expr: ...

class FixedLayout(Layout):
    """A Tensor layout we cannot change"""
    def make_indexer(self) -> Callable[[Sequence[Expr]], Expr]:
        """A closure containing math to read a given element"""

class FlexibleLayout(Layout):
    """A Tensor layout we are allowed to change"""
    allow_indexing: bool
    @staticmethod
    def contiguous_strides(sizes): ...
    @staticmethod
    def fill_ordered(sizes, order):
        """
        Create a stride based on the order the dimensions should be filled in.

        In this format, channels last would be:
            [1, 3, 2, 0]
        """
    @staticmethod
    def stride_ordered(sizes, order):
        """
        Create a stride based on the sorted order of a permuted range.

        In this format, channels last would be:
            [3, 0, 2, 1]
        """
    @staticmethod
    def stride_ordered_for_memory_format(sizes, memory_format):
        """
        Create a stride based on a memory format.

        Memory format is translasted into a stride order,
        so channels_last is the same as:
            FlexibleLayout.stride_ordered(sizes, [3, 0, 2, 1])

        This interface does not support memory_format `torch.preserve_format`
        which should be used to deduce a format from another source
        """
    @staticmethod
    def same_ordered(sizes, stride):
        """
        Create a stride that has the same stride order as given stride

        For example, if given stride is [1000, 1, 100, 10],
        the fill order should be [1, 3, 2, 0]
        """
    def as_stride_order(self, order, allow_padding: bool = False): ...
    def as_exact_strides(self, exact_strides, allow_padding: bool = False): ...
    def as_fill_order(self, order): ...
    def as_same_order(self, stride): ...
    def __init__(self, device, dtype, size, stride_order=None) -> None: ...

class NonOwningLayout(Layout):
    """Is a view into the storage of another tensor"""
    view: Incomplete
    def __init__(self, view: BaseView | TensorBox) -> None: ...
    def make_indexer(self) -> Callable[[Sequence[Expr]], Expr]: ...
    def maybe_guard_aligned(self): ...

class CommBufferType(Enum):
    SYMM_MEM = 'symm_mem'

class CommBufferLayout(FixedLayout):
    """
    A layout that signifies the buffer is a comm buffer.
    In terms of striding, the layout is identical to `FixedLayout`.

    Buffers with this layout do not participate in in-place reuse - it can be
    neither the source nor the target for in-place reuse.

    For detailed motivation and usage of this layout, see
    NOTE [lowering-time collective optimization].
    """
    comm_buffer_type: CommBufferType
    group_name: str
    def __init__(self, layout: FlexibleLayout, comm_buffer_type: CommBufferType, group_name: str) -> None: ...

@ir_dataclass
class NoneLayout(OutputSpec):
    device: torch.device | None
    size: list[int] = dataclasses.field(default_factory=Incomplete)
    stride: list[int] = dataclasses.field(default_factory=Incomplete)
    def storage_size(self) -> int: ...
    def as_fixed(self): ...
    def get_device(self) -> torch.device | None: ...

class MutationLayoutSHOULDREMOVE(Layout):
    target: Incomplete
    def __init__(self, target: IRNode) -> None: ...
    @property
    def stride(self) -> list[Expr]: ...
    @stride.setter
    def stride(self, value: Never) -> None: ...
    def storage_size(self) -> sympy.Expr: ...
    def get_buffer(self) -> Buffer: ...
    def real_layout(self): ...
    @classmethod
    def realize_into(cls, src, dst, unsafe_alias: bool = False): ...
    def as_fixed(self): ...
    def make_indexer(self) -> Callable[[Sequence[Expr]], Expr]: ...

@ir_dataclass(frozen=False)
class Buffer(IRNode, CodegenSymbol):
    name: str | None
    layout: OutputSpec
    def __post_init__(self) -> None: ...
    def make_indexer(self) -> Callable[[Sequence[Expr]], Expr]: ...
    def get_name(self) -> str: ...
    def get_example(self) -> torch.Tensor | sympy.Symbol: ...
    def get_device(self) -> torch.device | None: ...
    def get_defining_op(self) -> Operation | None: ...
    @property
    def dtype(self) -> torch.dtype: ...
    def get_size(self) -> Sequence[Expr]: ...
    def get_stride(self) -> list[Expr]: ...
    def get_offset(self) -> Expr: ...
    def get_layout(self) -> Layout: ...
    def get_output_spec(self) -> OutputSpec: ...
    def get_storage_numel(self): ...
    def freeze_layout(self) -> None: ...
    def freeze_layout_with_stride_order(self, order, allow_padding: bool = False) -> None: ...
    def freeze_layout_with_fill_order(self, order) -> None: ...
    def freeze_layout_with_same_order(self, stride) -> None: ...
    def freeze_layout_with_exact_strides(self, exact_strides, allow_padding: bool = False) -> None: ...
    def is_zero_elements(self): ...
    def make_loader(self) -> Callable[[Sequence[Expr]], OpsValue]: ...
    def codegen_reference(self, writer: IndentedBuffer | None = None) -> str: ...
    def decide_layout(self) -> None: ...
    def get_inputs_that_alias_output(self) -> Sequence[str]: ...
    def get_mutation_names(self) -> Sequence[str]: ...
    def get_read_names(self) -> OrderedSet[str]: ...
    def get_free_symbol_uses(self, unbacked_only: bool = False) -> OrderedSet[sympy.Symbol]: ...
    def get_unbacked_symbol_defs(self) -> OrderedSet[sympy.Symbol]: ...
    def realize(self) -> str | None: ...
    def should_allocate(self) -> bool: ...

@ir_dataclass(frozen=False)
class OperationBuffer(Buffer, Operation):
    def get_outputs(self) -> list[Buffer]: ...
    def get_defining_op(self) -> Operation: ...
    get_operation_name = ...
    def __post_init__(self) -> None: ...

class InputBuffer(Buffer):
    def num_reads(self) -> int: ...

class DonatedBuffer(InputBuffer):
    """
    Represents a donated buffer which is a saved tensor that is not alias to any
    fwd inputs, fwd user outputs, and bwd outputs. We generally cannot inplace
    reuse the input tensor memory during backward since it might be used in another
    function. However, donated buffer can be inplace reused during backward
    to save memory.
    """

class ConstantBuffer(InputBuffer):
    override_device: torch.device | None
    def make_loader(self) -> Callable[[Sequence[Expr]], OpsValue]: ...
    def constant_to_device(self, device: torch.device) -> IRNode: ...

@ir_dataclass
class NoneAsConstantBuffer(IRNode):
    def get_reads(self) -> OrderedSet[Dep]: ...
    def get_free_symbol_uses(self, unbacked_only: bool = False) -> OrderedSet[sympy.Symbol]: ...
    def codegen_reference(self, writer: IndentedBuffer | None = None) -> str: ...
    def get_output_spec(self) -> OutputSpec: ...
    def has_tensor_output(self) -> bool: ...

@ir_dataclass
class ShapeAsConstantBuffer(IRNode):
    expr: Expr
    def get_free_symbol_uses(self, unbacked_only: bool = False) -> OrderedSet[sympy.Symbol]: ...
    def codegen_reference(self, writer: IndentedBuffer | None = None) -> str: ...
    def has_tensor_output(self) -> bool: ...

@ir_dataclass(frozen=False)
class ComputedBuffer(OperationBuffer):
    data: Loops
    def get_computed_buffer_name(self) -> str | None:
        """
        Returns self.name if it exists, otherwise returns the name of the data node if that exists.
        If neither exist, returns None.
        """
    def num_reads(self) -> int: ...
    def get_reads(self) -> OrderedSet[Dep]: ...
    def get_read_names(self) -> OrderedSet[str]: ...
    def get_read_writes(self) -> dependencies.ReadWrites: ...
    def get_free_symbol_uses(self, unbacked_only: bool = False) -> OrderedSet[sympy.Symbol]: ...
    def make_loader(self) -> Callable[[Sequence[Expr]], OpsValue]: ...
    def get_store_function(self) -> Callable[..., None]: ...
    def get_fill_order(self) -> list[int] | None:
        """
        If our layout is still flexible, try to determine the stride order based on stride orders of reads.

        TODO(jansel): A better algorithm here would look at downstream consumers of this
                      value and try to do global graph-level layout optimization.
                      This is also something just begging to be autotuned.
        """
    def decide_layout(self) -> None: ...
    @cache_on_self
    def get_default_sizes_body(self) -> tuple[tuple[list[sympy.Expr], list[sympy.Expr]], LoopBody, tuple[list[sympy.Expr], list[sympy.Expr]]]: ...
    def simplify_and_reorder(self, extra_indexing_constraints: tuple[dict[Any, Any], list[Any]] | None = None, recompute_sizes_body_func: Callable[..., Any] | None = None) -> tuple[tuple[list[sympy.Expr], list[sympy.Expr]], LoopBody]:
        """
        This is a main place where we do loop transformations in a
        backend-agnostic way.

        Here we:
            1) Remove any 1 dimensions
            2) Fuse contiguous dimensions together
            3) Reorder dimensions based on stride orders

        Optional argument extra_indexing_constraints can be used to append additional
        indexing expressions to existing ones derived from buffer's body. This can be useful
        to fuse scheduler nodes with compatible ranges, e.g. (s0*s1*...,) and (s0, s1, s2, ...)
        on CPU by preventing indexing simplifications and obtaining index/reduce ranges for
        the scheduler node compatible with other nodes.
        Optional argument recompute_sizes_body_func can be used to recompute sizes and body
        on the default body. This can be useful to append additional loop transformations.
        """
    @staticmethod
    def _apply_loop_reordering(index_vars, support_vars, sizes, memory_addrs, priority_idx=None):
        """
        Shuffle the order of loops around to hopefully improve performance.
        """
    def get_reduction_size(self) -> Sequence[sympy.Expr]: ...
    def get_reduction_type(self) -> str | None: ...
    def is_no_op(self) -> bool: ...
    def should_allocate(self) -> bool: ...
    def constant_to_device(self, device: torch.device) -> IRNode:
        """Move this to a given device. Requires that all reads are to constants."""

class TemplateBuffer(OperationBuffer):
    """
    Represents a Triton (in the future other type) of template operator
    that we can fuse an epilogue onto.
    """
    inputs: Incomplete
    make_kernel_render: Incomplete
    name: Incomplete
    def __init__(self, layout: Layout, inputs: Sequence[IRNode], make_kernel_render: Callable[..., Any]) -> None: ...
    def get_read_writes(self) -> dependencies.ReadWrites: ...
    def extract_read_writes(self, normalize): ...
    def get_reduction_size(self) -> Sequence[sympy.Expr]: ...
    def get_reduction_type(self) -> str | None: ...
    def should_allocate(self) -> bool: ...
    def simplify_and_reorder(self, extra_indexing_constraints: tuple[dict[Any, Any], list[Any]] | None = None, recompute_sizes_body_func: Callable[..., Any] | None = None): ...

class TritonTemplateBuffer(TemplateBuffer):
    mutated_inputs: Incomplete
    outputs: list[Buffer]
    allowed_prologue_inps: Incomplete
    subgraph_inps: list[IRNode | sympy.Expr | None] | None
    subgraph_outs: list[IRNode | None] | None
    def __init__(self, layout, inputs, make_kernel_render, mutated_inputs: Iterable[IRNode] | None = None, allowed_prologue_inps: OrderedSet[str] | None = None) -> None:
        """
        NOTE:[TritonTemplates with multiple outputs]
        We want the ability for TritonTemplates to output multiple tensors. Triton
        kernels have no notion of outputs and this is done by creating tensors that
        are then mutated by the kernel. Currently our STORE_OUTPUT codegen doesn't
        support creating multinode outputs for triton templates.
        We work around this by creating an extra input buffer during the lowering
        and we mark them as mutated inputs.
        """
    def get_free_symbol_uses(self, unbacked_only: bool = False) -> OrderedSet[sympy.Symbol]: ...
    def get_outputs(self) -> list[Buffer]: ...
    def get_allowed_prologue_inps(self) -> OrderedSet[str]: ...
    def __str__(self) -> str: ...
PrimitiveInfoType = int | float | bool | str | list[int | str | float | bool]

class ChoiceCaller:
    """
    Represents a possible choice used in autotune_process.py.
    During autotuning, self.benchmark() is first called to get benchmark result,
    and if this choice is selected, self.output_node() is called to get the output_node.

    Children classes: TritonTemplateCaller, CUDATemplateCaller.
    """
    name: Incomplete
    layout: Incomplete
    input_nodes: Incomplete
    description: Incomplete
    def __init__(self, name: str, input_nodes: list[Buffer], layout: Layout, description: str) -> None: ...
    def benchmark(self, *args, out) -> float: ...
    def call_name(self) -> str: ...
    def to_callable(self) -> None: ...
    def kernel_hash_key(self) -> str:
        """
        Hash key for the underlying kernel. By default, we assume there are no
        runtime params, so kernel hash key defaults to choice caller's hash key.
        """
    def hash_key(self) -> str: ...
    def output_node(self) -> TensorBox: ...
    def info_dict(self) -> dict[str, PrimitiveInfoType | list[PrimitiveInfoType]]:
        """Information returned here is logged to the autotune log file when that is enabled."""
    def autoheuristic_id(self) -> str: ...

class TritonTemplateCallerBase(ChoiceCaller):
    def get_make_kernel_render(self) -> Any: ...

class MultiTemplateBuffer(TritonTemplateBuffer):
    """
    Represents a Buffer with multiple backing implementation choices.

    Choices can be TritonTemplates or ExternKernels. During scheduling if there is a potential
    epilogue we will benchmark each of the choices with the epilogue to determine an implementation.
    Otherwise, the fastest base choice will be chosen.
    """
    _choice_timings_fn: Incomplete
    _choice_timings: dict[ChoiceCaller, float] | None
    original_inputs: Incomplete
    _output_plannable: Incomplete
    def __init__(self, layout: Layout, inputs: list[IRNode], choice_timings_fn: Callable[[], dict[ChoiceCaller, float]], unfiltered_choices: list[ChoiceCaller], allowed_prologue_inps: OrderedSet[str]) -> None: ...
    @property
    def output_plannable(self) -> bool:
        """
        Are all possible choices TritonTemplates or Extern Kernels with out variants
        """
    @property
    def choice_timings(self) -> dict[ChoiceCaller, float]: ...
    make_kernel_render: Incomplete
    @contextlib.contextmanager
    def swap_as_triton_caller(self, caller: TritonTemplateCallerBase): ...
    def finalize_as_triton_caller(self, caller: TritonTemplateCallerBase) -> None: ...
    def get_min_choice(self) -> tuple[ChoiceCaller, float]: ...

class CUDATemplateBuffer(TemplateBuffer):
    workspace_size: Incomplete
    template: Incomplete
    supports_epilogue_fusion: Incomplete
    def __init__(self, layout, inputs, make_kernel_render, workspace_size: int, template: CUDATemplate, supports_epilogue_fusion: bool) -> None: ...
    def get_workspace_size(self): ...
    def emulate_store_fn(self) -> None: ...

class CppTemplateBuffer(TemplateBuffer):
    template: Incomplete
    choice: Incomplete
    outputs: list[Buffer] | None
    def __init__(self, layout, inputs, make_kernel_render, template, choice) -> None: ...
    def get_layout(self) -> Layout: ...

@ir_dataclass(frozen=False)
class InputsKernel(OperationBuffer):
    inputs: list[Buffer]
    def get_read_writes(self) -> dependencies.ReadWrites: ...
    def get_reads(self) -> OrderedSet[Dep]: ...
    @classmethod
    def unwrap_storage_for_input(cls, x: IRNode) -> IRNode: ...
    @staticmethod
    def unwrap_storage(inputs): ...
    def is_extern(self) -> bool: ...
    def num_reads(self) -> int: ...

class NopKernel(InputsKernel):
    def is_no_op(self) -> bool: ...
    def get_reads(self) -> OrderedSet[Dep]: ...

class ConcatKernel(NopKernel):
    """
    There isn't actually a real kernel for concat, we just change the
    storage for the upstream data.
    """
    @classmethod
    def create(cls, inputs, dim): ...
    @classmethod
    def can_realize_into_without_copy(cls, src, dst=None): ...
    @classmethod
    def realize_into(cls, src, dst): ...
    def should_allocate(self) -> bool: ...

@ir_dataclass(frozen=False)
class ExternKernel(InputsKernel):
    constant_args: tuple[Any, ...] = ...
    kwargs: dict[str, Any] = dataclasses.field(default_factory=dict)
    output_view: ReinterpretView | None = ...
    python_kernel_name: str | None = ...
    cpp_kernel_name: str | None = ...
    ordered_kwargs_for_cpp_kernel: Iterable[str] = dataclasses.field(default_factory=list)
    op_overload: torch._ops.OpOverload | torch._ops.HigherOrderOperator | None = ...
    arg_properties: list[dict[str, Any]] | None = ...
    kwarg_properties: dict[str, dict[str, Any]] | None = ...
    unbacked_bindings: dict[sympy.Symbol, pytree.KeyPath] = dataclasses.field(default_factory=dict)
    mutation_outputs: list[MutationOutput] = dataclasses.field(default_factory=list)
    fx_node = ...
    def __init__(self, name, layout, inputs, constant_args=(), kwargs=None, output_view=None, python_kernel_name=None, cpp_kernel_name=None, ordered_kwargs_for_cpp_kernel=(), op_overload=None) -> None: ...
    def get_outputs(self) -> list[Buffer]: ...
    def get_unbacked_symbol_defs(self) -> OrderedSet[sympy.Symbol]: ...
    allarg_properties = ...
    schema_kwargs = ...
    def collect_arg_kwarg_properties(self) -> None: ...
    def decide_layout(self) -> None: ...
    def codegen_comment(self, wrapper) -> None: ...
    def codegen(self, wrapper) -> None: ...
    def set_cpp_kernel_name(self, cpp_kernel_name: str | None = None) -> None: ...
    def set_python_kernel_name(self, python_kernel_name: str | None) -> None: ...
    def get_kernel_name(self): ...
    @staticmethod
    def copy_input(x): ...
    @classmethod
    def process_kernel(cls, kernel, *args, **kwargs) -> tuple[Any, list[Any], list[Any], Callable[[Any, Any], Any], dict[sympy.Symbol, pytree.KeyPath] | None]: ...
    @classmethod
    def convert_to_reinterpret_view(cls, x):
        """
        In order to pass this to an extern kernel we need a
        ReinterpretView not a View.  This allows us to avoid some
        unneeded copies.
        """
    @classmethod
    def realize_input(cls, x): ...
    @classmethod
    def require_stride1(cls, x): ...
    @classmethod
    def require_strides(cls, x, order: Sequence[int] | None = None, exact_strides: Sequence[_IntLike] | None = None, allow_padding: bool = False): ...
    @classmethod
    def require_exact_strides(cls, x, exact_strides, allow_padding: bool = False): ...
    @classmethod
    def require_stride_order(cls, x, order, allow_padding: bool = False): ...
    @classmethod
    def require_channels_last(cls, x): ...
    @classmethod
    def require_channels_last_3d(cls, x): ...
    @classmethod
    def require_contiguous(cls, x): ...
    @classmethod
    def require_contiguous_strides(cls, x): ...
    def apply_constraint(self) -> None: ...
    def fill_non_provided_args(self, args, kwargs): ...
    def codegen_const_args(self, names: list[str] | None = None): ...
    def codegen_args(self): ...
    def get_kwargs_value(self, arg_name, **kwargs):
        """Given an argument name, queries for values in (in order):
        1. any provided kwargs for this function.
        2. the class self.kwargs member.
        3. any available default arguments in self.allarg_properties."""
    def codegen_kwargs(self, skip_out: bool = False): ...
    def get_op_name(self) -> str: ...
    def codegen_size_asserts(self, wrapper) -> None: ...
    def codegen_alignment_asserts(self, wrapper) -> None: ...
    def get_group_stride(self):
        """
        get output sizes and strides, for template_codegen
        """
    def canonicalize(self):
        """
        Manually get canonicalization of the output index
        """
    def get_free_symbol_uses(self, unbacked_only: bool = False) -> OrderedSet[sympy.Symbol]: ...
    def __str__(self) -> str: ...
    __repr__ = __str__

@ir_dataclass(frozen=False)
class ExternKernelOut(ExternKernel):
    def codegen(self, wrapper) -> None: ...
    name = ...
    def __init__(self, layout, inputs, constant_args=(), kwargs=None, output_view=None, python_kernel_name=None, cpp_kernel_name=None, ordered_kwargs_for_cpp_kernel=(), op_overload=None) -> None: ...
    def should_allocate(self) -> bool: ...

class RandomSeeds(ExternKernelOut):
    def __init__(self, count: int, device: torch.device) -> None: ...

class ExternKernelAlloc(ExternKernel):
    def codegen(self, wrapper) -> None: ...
    outputs: Sequence[Any]
    name: Incomplete
    def __init__(self, layout, inputs, constant_args=(), kwargs=None, python_kernel_name=None, cpp_kernel_name=None, ordered_kwargs_for_cpp_kernel=(), op_overload=None) -> None: ...
    def should_allocate(self) -> bool: ...
    def apply_constraint(self) -> None: ...

class MutationOutput(Buffer):
    """
    An output buffer that represents the mutation of a pre-existing buffer
    """
    mutation_names: Incomplete
    mutating_node: Operation
    name: Incomplete
    def __init__(self, layout, mutated_node, mutating_node: Operation) -> None: ...
    def get_defining_op(self) -> Operation: ...
    def get_mutation_names(self) -> Sequence[str]: ...
    def should_allocate(self) -> bool: ...

class TMADescriptor(ExternKernel):
    """
    An IR node representing a generic host-side TMA descriptor in the Triton API
    Mostly useful for user-defined Triton kernels relying on host-side TMA;
    but can, in principle, be used for Inductor's Triton templates, too.

    See TMADescriptorExperimental and TMADescriptorStable for the two implementations
    (the old API and the new API)
    """
    _CACHE: dict[Any, TMADescriptor]
    @classmethod
    def _create_impl(cls, tensor: IRNode, tma_meta: tuple[str, tuple[Any, ...]]) -> TMADescriptor: ...
    @classmethod
    def create(cls, tensor: IRNode, tma_meta: tuple[str, tuple[Any, ...]]) -> TMADescriptor: ...
    tensor: Incomplete
    name: Incomplete
    def __init__(self, tensor: IRNode, inputs, constant_args) -> None: ...
    def codegen(self, wrapper) -> None: ...
    def get_tensor(self) -> IRNode: ...

class TMADescriptorExperimental(TMADescriptor):
    """
    the new host-side TMA Descriptor API:
    (the ones obtained via create_{1d,2d}_tma_descriptor calls).

    See also TMADescriptorStable for the new API.
    """
    dims: Incomplete
    block_dims: Incomplete
    element_size: Incomplete
    rank: Incomplete
    def __init__(self, tensor: IRNode, dims: list[int | torch.SymInt], block_dims: list[int | torch.SymInt], element_size: int | None = None) -> None: ...

class TMADescriptorStable(TMADescriptor):
    """
    the new host-side TMA descriptor API
    (the ones obtained via TensorDescriptor.from_tensor).

    See also TMADescriptorExperimental for the old API.
    """
    block_shape: Incomplete
    def __init__(self, tensor: IRNode, block_shape: list[int | torch.SymInt]) -> None: ...

class SubgraphBuffer(ExternKernel):
    gm: Incomplete
    example_inputs: Incomplete
    name: Incomplete
    subgraph: Incomplete
    sym_inputs: Incomplete
    def __init__(self, layout: Layout, input_nodes: list[Buffer], gm: torch.fx.GraphModule, example_inputs: list[Any], subgraph_name: str) -> None: ...
    graph: Incomplete
    def codegen(self, wrapper) -> None: ...

class UserDefinedTritonKernel(ExternKernel):
    def get_kernel_and_metadata(self): ...
    def codegen(self, wrapper) -> None: ...
    def get_free_symbol_uses(self, unbacked_only: bool = False) -> OrderedSet[sympy.Symbol]: ...
    def get_unbacked_symbol_defs(self) -> OrderedSet[sympy.Symbol]: ...
    device: Incomplete
    kernel_idx: Incomplete
    grid: Incomplete
    ordered_kwargs_for_cpp_kernel: Incomplete
    mutable_args: Incomplete
    mutation_outputs: Incomplete
    def __init__(self, *, kernel_idx, grid, tma_descriptor_metadata, kernel_args) -> None: ...
    def get_outputs(self) -> list[Buffer]: ...
    def get_device(self) -> torch.device | None: ...

class InplaceBernoulliFallback(ExternKernel):
    """
    This needs to be a custom class to handle mutation properly
    """
    def codegen(self, wrapper) -> None: ...
    def should_allocate(self) -> bool: ...
    def get_mutation_names(self) -> Sequence[str]: ...
    def get_unbacked_symbol_defs(self) -> OrderedSet[sympy.Symbol]: ...
    name: Incomplete
    def __init__(self, op_overload, x, *constant_args) -> None: ...

class InplaceCopyFallback(ExternKernel):
    """
    This needs to be a custom class to handle mutation properly
    """
    def codegen(self, wrapper) -> None: ...
    def should_allocate(self) -> bool: ...
    def get_mutation_names(self) -> Sequence[str]: ...
    def get_unbacked_symbol_defs(self) -> OrderedSet[sympy.Symbol]: ...
    name: Incomplete
    def __init__(self, layout, inputs, constant_args) -> None: ...
    @classmethod
    def create(cls, dst, src, non_blocking: bool = False): ...

class MutatingFirstArgExternKernel(ExternKernel):
    """
    This needs to be a custom class to handle mutation properly
    """
    def codegen(self, wrapper) -> None: ...
    def should_allocate(self) -> bool: ...
    def get_mutation_names(self) -> Sequence[str]: ...
    def get_unbacked_symbol_defs(self) -> OrderedSet[sympy.Symbol]: ...
    def has_side_effects(self) -> bool: ...

class ResizeStorageBytes(MutatingFirstArgExternKernel):
    name: Incomplete
    python_kernel_name: str
    cpp_kernel_name: str
    def __init__(self, variable, new_size) -> None: ...

class SetSourceTensorKernel(ExternKernelAlloc):
    mutation_outputs: Incomplete
    def __init__(self, self_tensor, storage_tensor) -> None: ...
    def get_inputs_that_alias_output(self) -> Sequence[str]: ...

class ScatterFallback(ExternKernel):
    """
    This needs to be a custom class to handle mutation properly.
    This class handles both aten.scatter_ and aten.scatter_reduce_.
    It also handle the case `src` being a scalar properly.
    """
    def codegen(self, wrapper) -> None: ...
    def should_allocate(self) -> bool: ...
    def get_mutation_names(self) -> Sequence[str]: ...
    def get_unbacked_symbol_defs(self) -> OrderedSet[sympy.Symbol]: ...
    src_is_tensor: Incomplete
    name: Incomplete
    def __init__(self, op_overload, x, dim: int, index, src, *, reduce: str | None = None, include_self: bool = True) -> None: ...

class IndexPutFallback(ExternKernel):
    """
    This needs to be a custom class to handle mutation and indices properly
    """
    def codegen(self, wrapper) -> None: ...
    def should_allocate(self) -> bool: ...
    def get_mutation_names(self) -> Sequence[str]: ...
    def get_unbacked_symbol_defs(self) -> OrderedSet[sympy.Symbol]: ...
    indices: Incomplete
    name: Incomplete
    def __init__(self, op_overload, x, indices, values, accumulate) -> None: ...

class DeviceCopy(ExternKernelOut):
    @classmethod
    def create(cls, x, device, non_blocking): ...
    def codegen(self, wrapper) -> None: ...

class DynamicScalar(ExternKernel):
    """
    The result of a call to aten._local_scalar_dense.
    """
    def get_reads(self) -> OrderedSet[Dep]: ...
    def should_allocate(self) -> bool: ...
    sym: Incomplete
    keypath: Incomplete
    def __init__(self, sym, keypath, data) -> None: ...
    def get_unbacked_symbol_defs(self) -> OrderedSet[sympy.Symbol]: ...
    def codegen(self, wrapper) -> None: ...

class AssertScalar(ExternKernel):
    """
    The result of a call to aten._assert_scalar
    """
    def get_reads(self) -> OrderedSet[Dep]: ...
    def should_allocate(self) -> bool: ...
    scalar: Incomplete
    msg: Incomplete
    def __init__(self, scalar, msg) -> None: ...
    def has_side_effects(self) -> bool: ...
    def get_free_symbol_uses(self, unbacked_only: bool = False): ...
    def codegen(self, wrapper) -> None: ...

@ir_dataclass(frozen=False)
class ExternKernelNode:
    name: str
    node: export_schema.Node

class FallbackKernel(ExternKernelAlloc):
    """
    A class that represents a fallback kernel for handling operators that are not
    directly support by inductor. It currently supports functional ops, view ops,
    inplace aten ops, and mutating ops that are auto-functionalizable.
    """
    use_runtime_dispatch: bool
    unbacked_bindings: Incomplete
    op_overload: Incomplete
    unflatten_args: Incomplete
    kwargs: Incomplete
    alias_names: list[str]
    mutation_names: list[str]
    def __init__(self, layout, kernel, tensor_args, nontensor_args, unflatten_args, kwargs=None, *, unbacked_bindings=None) -> None: ...
    def get_read_writes(self) -> dependencies.ReadWrites: ...
    def codegen_unbacked_symbol_defs(self, wrapper) -> None: ...
    def get_unbacked_symbol_defs(self) -> OrderedSet[sympy.Symbol]: ...
    def codegen_args(self): ...
    @staticmethod
    def find_device(tensor_args, example_output): ...
    def has_side_effects(self): ...
    def get_inputs_that_alias_output(self): ...
    def get_mutation_names(self) -> Sequence[str]: ...
    def export_extern_kernel_node(self):
        """
        ProxyExecutor Design Note
        We export the ExternFallbackNodes (for custom ops) into a serialized file
        and run it with a host side proxy executor to address the ABI problem
        This is currently only implemented for fbcode. Eventually, we will also make this work for OSS.
        Detailed design doc can be found at
        https://docs.google.com/document/d/1wC4DOZFaYym2t1Esz0X5yxlLI3RDnSiyRbUus3bkJ64/edit?usp=sharing
        """
    def codegen(self, wrapper) -> None: ...
    @staticmethod
    def tensor_to_layout(output: torch.Tensor): ...
    @classmethod
    def create(cls, kernel, *args, **kwargs): ...
    def apply_constraint(self): ...

@ir_dataclass(frozen=False)
class ComplexView(FallbackKernel):
    """View a complex number as two dtyped numbers or vice versa"""
    def should_allocate(self) -> bool: ...
    def get_inputs_that_alias_output(self) -> Sequence[str]: ...
    def __init__(self, layout, kernel, tensor_args, nontensor_args, unflatten_args, *, unbacked_bindings=None) -> None: ...

@ir_dataclass
class MultiOutputLayout(OutputSpec):
    device: torch.device
    def get_device(self) -> torch.device | None: ...

class MultiOutput(ExternKernel):
    def codegen(self, wrapper) -> None: ...
    name: Incomplete
    indices: Incomplete
    skip_size_stride_alignment_checks: Incomplete
    def __init__(self, layout: OutputSpec, input, indices: list[tuple[Any, ...]], skip_size_stride_alignment_checks: bool = False) -> None: ...
    def get_free_symbol_uses(self, unbacked_only: bool = False) -> OrderedSet[sympy.Symbol]: ...
    def should_allocate(self) -> bool: ...
    def get_inputs_that_alias_output(self) -> Sequence[str]: ...

@dataclasses.dataclass
class MutableBox(IRNode):
    """
    TensorBox / StorageBox allow in-place mutation of Tensors
    """
    data: IRNode
    def has_exceeded_max_reads(self) -> bool: ...
    def get_device(self) -> torch.device | None: ...
    def make_loader(self) -> Callable[[Sequence[Expr]], OpsValue]: ...
    def make_indexer(self) -> Callable[[Sequence[Expr]], Expr]: ...
    def get_stride(self) -> Sequence[_IntLike]: ...
    def get_name(self) -> str: ...
    def has_large_inner_fn(self, threshold: int | None = None) -> bool: ...
    def mark_reuse(self, users: int) -> None: ...
    def realize_hint(self) -> None: ...
    def unwrap_view(self) -> IRNode: ...
    def is_input_buffer(self) -> bool: ...
    def freeze_layout(self) -> None: ...
    def freeze_layout_with_stride_order(self, order: list[int], allow_padding: bool = False) -> None: ...
    def freeze_layout_with_fill_order(self, order: list[int]) -> None: ...
    def freeze_layout_with_same_order(self, stride: list[_IntLike]) -> None: ...
    def freeze_layout_with_exact_strides(self, exact_strides: list[_IntLike], allow_padding: bool = False) -> None: ...
    def get_read_writes(self) -> dependencies.ReadWrites: ...
    def get_reads(self) -> OrderedSet[Dep]: ...
    def num_reads(self) -> int: ...
    def get_storage_numel(self) -> _IntLike: ...
    def get_reduction_type(self) -> str | None: ...
    def get_reduction_size(self) -> Sequence[sympy.Expr]: ...
    def is_extern(self) -> bool: ...
    def is_no_op(self) -> bool: ...
    def constant_to_device(self, device: torch.device) -> IRNode: ...
    def get_mutation_names(self) -> Sequence[str]: ...
    def get_operation_name(self) -> str: ...
    def get_inputs_that_alias_output(self) -> Sequence[str]: ...
    def realize(self) -> str | None: ...
    def get_free_symbol_uses(self, unbacked_only: bool = False) -> OrderedSet[sympy.Symbol]: ...
    def get_read_names(self) -> OrderedSet[str]: ...
    def get_defining_op(self) -> Operation | None: ...
    def codegen_reference(self, writer: IndentedBuffer | None = None) -> str: ...
    @property
    def layout(self) -> OutputSpec: ...
    def get_layout(self) -> Layout: ...
    def get_output_spec(self) -> OutputSpec: ...
    def get_size(self) -> Sequence[Expr]: ...
    @property
    def dtype(self): ...
    def __str__(self) -> str: ...
    __repr__ = __str__

class TensorBox(MutableBox):
    @staticmethod
    def create(data): ...

class StorageBox(MutableBox):
    def is_input_buffer(self) -> bool: ...
    def is_module_buffer(self): ...
    data: Incomplete
    def realize(self) -> str | None: ...
    def realize_hint(self) -> None:
        """
        Called on buffers we expect to be forced to realize later.
        """
    def has_exceeded_max_reads(self) -> bool: ...
    def should_realize_on_reuse(self, users):
        """
        A heuristic to decide if we should realize a tensor
        that is used multiple times.
        """
    def mark_reuse(self, users: int) -> None: ...
    def num_reads(self): ...

@ir_dataclass(frozen=False)
class Subgraph(IRNode):
    name: str
    graph_module: torch.fx.GraphModule
    graph: GraphLowering | None = ...

def _has_aliased_buffers(buffers: Sequence[IRNode]) -> bool: ...

@ir_dataclass(frozen=False)
class InvokeSubgraph(ExternKernel):
    """
    Ir node for the invoke_subgraph HOP.
    """
    subgraph: Subgraph | None = ...
    operands: list[TensorBox] | None = ...
    outputs: list[MultiOutput] | None = ...
    name = ...
    def __init__(self, subgraph: Subgraph, operands: list[TensorBox], layout: MultiOutputLayout) -> None: ...
    @classmethod
    def create(cls, subgraph: Subgraph, *operands): ...
    def codegen(self, wrapper) -> None: ...

@ir_dataclass(frozen=False)
class Conditional(ExternKernel):
    predicate: IRNode | None = ...
    operands: list[TensorBox | ShapeAsConstantBuffer] | None = ...
    true_subgraph: Subgraph | None = ...
    false_subgraph: Subgraph | None = ...
    outputs: list[MultiOutput] | None = ...
    unbacked_bindings = ...
    name = ...
    def __init__(self, predicate: IRNode, operands: list[TensorBox | ShapeAsConstantBuffer], true_subgraph: Subgraph, false_subgraph: Subgraph, layout: MultiOutputLayout, unbacked_bindings: dict[sympy.Symbol, pytree.KeyPath] | None) -> None: ...
    @classmethod
    def create(cls, predicate: TensorBox, true_fn: Subgraph, false_fn: Subgraph, operands: list[TensorBox | ShapeAsConstantBuffer]): ...
    def codegen(self, wrapper) -> None: ...
    def get_unbacked_symbol_defs(self) -> OrderedSet[sympy.Symbol]: ...

def _split_by_sym_type(args: list[Any]) -> tuple[list[ShapeAsConstantBuffer], list[Any]]: ...

@ir_dataclass(frozen=False)
class WhileLoop(ExternKernel):
    carried_inputs: list[TensorBox | ShapeAsConstantBuffer] | None = ...
    additional_inputs: list[TensorBox | ShapeAsConstantBuffer] | None = ...
    cond_subgraph: Subgraph | None = ...
    body_subgraph: Subgraph | None = ...
    outputs: list[MultiOutput] | None = ...
    name = ...
    def __init__(self, carried_inputs: list[TensorBox | ShapeAsConstantBuffer], additional_inputs: list[TensorBox | ShapeAsConstantBuffer], cond_subgraph: Subgraph, body_subgraph: Subgraph, layout: MultiOutputLayout) -> None: ...
    @classmethod
    def create(cls, cond_fn: Subgraph, body_fn: Subgraph, carried_inputs: list[TensorBox | ShapeAsConstantBuffer], additional_inputs: list[TensorBox | ShapeAsConstantBuffer]): ...
    def codegen(self, wrapper) -> None: ...

class EffectfulKernel(FallbackKernel):
    effect_type: Incomplete
    prev_effect_buffer: Incomplete
    def __init__(self, layout, kernel, tensor_args, nontensor_args, unflatten_args, kwargs=None, *, unbacked_bindings=None) -> None: ...
    def get_read_writes(self) -> dependencies.ReadWrites: ...
    def has_side_effects(self) -> bool: ...

class NonTensorObj(IRNode): ...

@ir_dataclass
class TorchBindObject(NonTensorObj):
    name: str
    value: FakeScriptObject | torch.ScriptObject
    def get_name(self) -> str: ...
    def codegen_reference(self, writer: IndentedBuffer | None = None) -> str: ...
    def get_value(self) -> FakeScriptObject | torch.ScriptObject: ...
    def get_real_obj(self) -> torch.ScriptObject: ...
    def get_buf_bytes(self) -> int: ...

@ir_dataclass
class GeneratorState(NonTensorObj):
    name: str
    device: torch.device
    def get_name(self) -> str: ...
    def codegen_reference(self, writer: IndentedBuffer | None = None) -> str: ...

class _CollectiveKernel(FallbackKernel):
    def should_allocate(self) -> bool: ...
    def has_side_effects(self) -> bool: ...
    cpp_kernel_name: Incomplete
    ordered_kwargs_for_cpp_kernel: Incomplete
    def set_cpp_kernel_name(self, cpp_kernel_name: str | None = None) -> None: ...
    @classmethod
    def create_inplace(cls, kernel, inputs: TensorBox | list[TensorBox], *args, **kwargs) -> None: ...
    @classmethod
    def create_out_of_place(cls, kernel, inputs: TensorBox | list[TensorBox], *args, **kwargs): ...

class _WaitKernel(_CollectiveKernel):
    def get_volatile_reads(self): ...
    @classmethod
    def create_wait(cls, kernel, inp: TensorBox) -> None: ...
    def get_read_writes(self) -> dependencies.ReadWrites: ...

def maybe_free_unbacked_symbols(s: object) -> OrderedSet[Symbol]: ...
def maybe_free_symbols(s: object) -> OrderedSet[Symbol]: ...
