import contextlib
import dataclasses
import enum
import functools
import sympy
import torch
from . import config as config
from .codegen.common import WorkspaceArg as WorkspaceArg
from .codegen.wrapper import PythonWrapperCodegen as PythonWrapperCodegen
from .graph import GraphLowering as GraphLowering
from .ir import Buffer as Buffer, ExternKernel as ExternKernel, ExternKernelOut as ExternKernelOut, IRNode as IRNode, Layout as Layout, Operation as Operation, ReinterpretView as ReinterpretView
from .output_code import CompiledFxGraph as CompiledFxGraph
from .scheduler import BaseSchedulerNode as BaseSchedulerNode, SchedulerBuffer as SchedulerBuffer
from _typeshed import Incomplete
from collections.abc import Collection, Iterable, Iterator, Mapping, MutableMapping, MutableSet, Sequence, ValuesView
from torch import SymBool as SymBool, SymFloat as SymFloat, SymInt as SymInt
from torch._dynamo.device_interface import get_interface_for_device as get_interface_for_device
from torch._dynamo.utils import detect_fake_mode as detect_fake_mode
from torch._inductor.runtime.hints import DeviceProperties as DeviceProperties
from torch._prims_common import ELEMENTWISE_TYPE_PROMOTION_KIND as ELEMENTWISE_TYPE_PROMOTION_KIND
from torch.autograd import DeviceType as DeviceType
from torch.autograd.profiler_util import EventList as EventList
from torch.fx import GraphModule as GraphModule
from torch.fx.experimental.symbolic_shapes import ShapeEnv as ShapeEnv
from torch.fx.node import Node as Node
from torch.fx.passes.graph_transform_observer import GraphTransformObserver as GraphTransformObserver
from torch.fx.passes.shape_prop import ShapeProp as ShapeProp
from torch.utils._ordered_set import OrderedSet as OrderedSet
from torch.utils._pytree import tree_map_only as tree_map_only
from torch.utils._sympy.functions import CeilDiv as CeilDiv, CleanDiv as CleanDiv, FloorDiv as FloorDiv, Identity as Identity, ModularIndexing as ModularIndexing
from torch.utils._sympy.symbol import SymT as SymT, make_symbol as make_symbol
from torch.utils._sympy.value_ranges import ValueRanges as ValueRanges, bound_sympy as bound_sympy
from typing import Any, Callable, Generic, Literal, NamedTuple, Protocol, TypeVar
from typing_extensions import Concatenate, ParamSpec, Self, TypeAlias, TypeGuard, dataclass_transform

OPTIMUS_EXCLUDE_POST_GRAD: Incomplete
GPU_TYPES: Incomplete
T = TypeVar('T')

@functools.cache
def get_gpu_type() -> str: ...

_IS_WINDOWS: Incomplete
log: Incomplete
_T = TypeVar('_T')
VarRanges: Incomplete
InputType = torch.Tensor | int | torch.SymInt | None
GPU_KERNEL_BIN_EXTS: Incomplete
GPU_ALIGN_BYTES: int
ALIGNMENT: int
TMA_ALIGNMENT: int
TMA_DESCRIPTOR_SIZE: int
ALIGN_BYTES: int

def _align(nbytes: int) -> int:
    """Round up to the nearest multiple of ALIGN_BYTES"""
def _is_aligned(v: sympy.Expr) -> bool:
    """v can be statically proven to be a multiple of ALIGN_BYTES"""

class align(sympy.Function):
    """Symbolically round up to the nearest multiple of ALIGN_BYTES"""
    nargs: Incomplete
    is_integer: bool
    @classmethod
    def eval(cls, value: sympy.Expr) -> sympy.Expr | None: ...

@dataclasses.dataclass(frozen=True)
class GraphPartitionMap:
    """
    Mapping from the partition info (e.g., input/output) to the graph info
    """
    id: int
    input_index_mapping: list[int | None]
    output_index_mapping: list[int | None]
    constant_names: list[str]

def fp8_bench(fn: Callable[[], Any], warmup: int = 25, rep: int = 100) -> float:
    """
    Returns benchmark results by examining torch profiler events.
    This could be more accurate as it doesn't count CPU side overhead.
    However, this also requires manually excluding irrelevant event, e.g.
    vectorized_elementwise_kernel which is used to fill L2 cache,
    various CUDA events, etc, so could also be fragile.
    """
def do_bench_using_profiling(fn: Callable[[], Any], warmup: int = 25, rep: int = 100) -> float:
    """
    Returns benchmark results by examining torch profiler events.
    This could be more accurate as it doesn't count CPU side overhead.
    However, this also requires manually excluding irrelevant event, e.g.
    vectorized_elementwise_kernel which is used to fill L2 cache,
    various CUDA events, etc, so could also be fragile.
    """
@functools.cache
def has_torchvision_roi_align() -> bool: ...
def decode_device(device: torch.device | None | str) -> torch.device: ...
def sympy_product(it: Iterable[sympy.Expr]) -> sympy.Expr: ...
def sympy_dot(seq1: Sequence[sympy.Expr], seq2: Sequence[sympy.Expr]) -> sympy.Expr: ...
def unique(it: Iterable[_T]) -> ValuesView[_T]: ...
def ceildiv(number: int | sympy.Expr, denom: int | sympy.Expr) -> int | sympy.Expr: ...
def _type_of(key: torch.dtype | None) -> str: ...
def convert_shape_to_inductor(lst: Iterable[int | torch.SymInt]) -> list[sympy.Expr]:
    """
    Gets the shape and stride of a tensor. For non-symbolic tensors, this is
    trivial. But for symbolic tensors, we need to map from SymIntNode into
    sympy.Expr.
    """
def convert_to_symint(i: int | sympy.Expr) -> int | torch.SymInt:
    """
    Like convert_shape_to_symint, but operates on a single expression.
    """
def convert_shape_to_symint(lst: Iterable[int | sympy.Expr]) -> list[int | torch.SymInt]:
    """
    Takes a list of shapes from Inductor and converts them into symints (or just
    ints if all shapes are static).
    """
def is_view(op: torch._ops.OpOverload) -> bool:
    """
    Does this op overload have aliasing
    """
def is_pointwise_use(use: Node, is_pointwise_fn: Callable[[torch._ops.OpOverload], bool] = ...) -> bool:
    """
    Do all uses of this op have torch.Tag.pointwise or return True for optional `is_pointwise_fn`

    Uses in views ops will follow the views uses
    """
def gen_gm_and_inputs(target: Any, args: list[Any], kwargs: dict[str, Any]) -> tuple[GraphModule, list[torch.Tensor]]: ...
def synchronize(device: str = 'cuda') -> None: ...
def timed(model: Callable[..., Any], example_inputs: Sequence[Any], times: int = 1, device: str = 'cuda') -> float: ...
def print_performance(model: Callable[..., Any], example_inputs: Sequence[Any] = (), times: int = 10, repeat: int = 10, baseline: float = 1.0, device: str = 'cuda') -> float: ...
def precompute_method(obj: Any, method: str) -> None:
    """Replace obj.method() with a new method that returns a precomputed constant."""
def precompute_methods(obj: Any, methods: list[str]) -> None:
    """Replace methods with new methods that returns a precomputed constants."""
def cmp(a: int, b: int) -> int: ...
def pad_listlike(x: int | Sequence[int], size: int) -> Sequence[int]: ...
def tuple_sorted(x: tuple[_T, ...]) -> list[_T]: ...
P = ParamSpec('P')
RV = TypeVar('RV', covariant=True)

class CachedMethod(Protocol, Generic[P, RV]):
    @staticmethod
    def clear_cache(cache: Any) -> None: ...
    def __call__(self, *args: P.args, **kwargs: P.kwargs) -> RV: ...

def cache_on_self(fn: Callable[Concatenate[Any, P], RV]) -> CachedMethod[P, RV]: ...
def aggregate_origins(node_schedule: Sequence[BaseSchedulerNode] | ExternKernel) -> OrderedSet[Node]: ...
def get_fused_kernel_name(node_schedule: Sequence[BaseSchedulerNode], descriptive_names: Literal[True, 'torch', 'original_aten', 'inductor_node']) -> str: ...
def get_kernel_metadata(node_schedule: Sequence[BaseSchedulerNode] | ExternKernel, wrapper: PythonWrapperCodegen) -> tuple[str, str]: ...
def dominated_nodes(initial_queue: Iterable[torch.fx.Node], skip_filter: Callable[[Any], bool] | None = None) -> OrderedSet[torch.fx.Node]:
    """Returns the set of nodes whose values depend on those within initial_queue"""
def gather_origins(args: Sequence[IRNode], kwargs: dict[str, IRNode]) -> OrderedSet[IRNode]: ...
def sympy_str(expr: sympy.Expr) -> str:
    """
    Normal sympy str is very slow, this is a lot faster.  The result are
    somewhat worse, as it doesn't do as much simplification.  So don't
    use this for final codegen.
    """
def get_bounds_index_expr(index: sympy.Expr) -> ValueRanges[Any]: ...
def prefix_is_reduction(prefix: str) -> bool: ...
def sympy_index_symbol_with_prefix(prefix: SymT, idx: int) -> sympy.Symbol:
    """
    Used to generate an integer-nonnegative symbol.
    """
def generate_assert(check: bool) -> bool: ...
def sympy_index_symbol(name: str) -> sympy.Symbol:
    """
    Used to generate an integer-nonnegative symbol.
    """
def sympy_subs(expr: sympy.Expr, replacements: dict[sympy.Expr, Any]) -> sympy.Expr:
    """
    When the passed replacement symbol v is a string, it is converted to a symbol with name v that
    have the same replaced expression integer and nonnegative properties.
    """
def is_symbolic(a: Any) -> TypeGuard[torch.SymInt | torch.Tensor]: ...
def any_is_symbolic(*args: Any) -> bool: ...
def get_first_incompatible_cudagraph_node(gm: torch.fx.GraphModule) -> torch.fx.Node | None: ...
def output_node(gm: torch.fx.GraphModule) -> Node:
    """Get the output node from an FX graph"""
def get_all_devices(gm: torch.fx.GraphModule) -> OrderedSet[torch.device]: ...
def unload_xpu_triton_pyds() -> None: ...

_registered_caches: list[Any]

def clear_on_fresh_cache(obj: Any) -> Any:
    """
    Use this decorator to register any caches that should be cache_clear'd
    with fresh_cache().
    """
def clear_caches() -> None:
    """
    Clear all registered caches.
    """
@contextlib.contextmanager
def fresh_cache(cache_entries: dict[str, Any] | None = None, dir: str | None = None, delete: bool = True) -> Iterator[None]:
    """
    Contextmanager that provides a clean tmp cachedir for pt2 caches.

    Optionally, pass a dict as 'cache_entries' to get a list of filenames and sizes
    generated with this cache instance.
    """
clear_on_fresh_inductor_cache = clear_on_fresh_cache
clear_inductor_caches = clear_caches
fresh_inductor_cache = fresh_cache

def argsort(seq: Sequence[Any]) -> list[int]: ...
def argsort_sym(shape_env: ShapeEnv, seq: Sequence[int | torch.SymInt | sympy.Expr]) -> list[int]: ...
def get_dtype_size(dtype: torch.dtype) -> int: ...

class LineContext(NamedTuple):
    context: Any

@dataclasses.dataclass
class ValueWithLineMap:
    value: str
    line_map: list[tuple[int, LineContext]]

class IndentedBuffer:
    tabwidth: int
    _lines: list[DeferredLineBase | LineContext | str]
    _indent: Incomplete
    def __init__(self, initial_indent: int = 0) -> None: ...
    @contextlib.contextmanager
    def set_tabwidth(self, tabwidth: int) -> Iterator[None]: ...
    def getvaluewithlinemap(self) -> ValueWithLineMap: ...
    def getvalue(self) -> str: ...
    def getrawvalue(self) -> str: ...
    def clear(self) -> None: ...
    def __bool__(self) -> bool: ...
    def prefix(self) -> str: ...
    def newline(self) -> None: ...
    def writeline(self, line: LineContext | DeferredLineBase | str) -> None: ...
    def writelines(self, lines: Sequence[LineContext | DeferredLineBase | str]) -> None: ...
    def indent(self, offset: int = 1) -> contextlib.AbstractContextManager[None]: ...
    def do_indent(self, offset: int = 1) -> None: ...
    def do_unindent(self, offset: int = 1) -> None: ...
    def splice(self, other_code: IndentedBuffer | str, strip: bool = False) -> None: ...
    def map(self, func: Callable[[Any], Any]) -> IndentedBuffer: ...
    def __repr__(self) -> str: ...
    def __add__(self, other: Self) -> IndentedBuffer: ...

class FakeIndentedBuffer(IndentedBuffer):
    def __init__(self) -> None: ...
    def __getattribute__(self, name: str) -> Any: ...

@contextlib.contextmanager
def restore_stdout_stderr() -> Iterator[None]: ...

class DeferredLineBase:
    """A line that can be 'unwritten' at a later time"""
    line: Incomplete
    def __init__(self, line: str) -> None: ...
    def __call__(self) -> str | None:
        """Returns either self.line or None to indicate the line has been 'unwritten'"""
    def _new_line(self, line: str) -> Self:
        """Returns a new deferred line with the same condition"""
    def with_prefix(self, prefix: str) -> Self: ...
    def lstrip(self) -> Self: ...
    def __getitem__(self, index: int | slice) -> Self: ...
    def __bool__(self) -> bool: ...
    def __len__(self) -> int: ...

class DelayReplaceLine(DeferredLineBase):
    """At end of codegen call `line.replace(key, value_fn())`"""
    key: Incomplete
    value_fn: Incomplete
    def __init__(self, key: str, value_fn: Callable[[], str], line: str) -> None: ...
    def __call__(self) -> str: ...
    def _new_line(self, line: str) -> DelayReplaceLine: ...

@functools.cache
def is_big_gpu(index_or_device: int | torch.device = 0) -> bool: ...
@functools.lru_cache
def get_max_num_sms() -> int: ...
def get_num_sms() -> int:
    """Handle experimental carveout if set otherwise return hardware SM count"""
def get_tma_workspace_arg(num_tma_descriptors: int, device: torch.device, num_programs: int | None = None) -> WorkspaceArg:
    """Builds and returns a WorkspaceArg for the device side TMA workspace buffer."""
def _use_template_for_gpu(layout: Layout, allowed_layout_dtypes: list[torch.dtype]) -> bool: ...
def _use_autotune_backend(backend: str) -> bool: ...
def _use_conv_autotune_backend(backend: str) -> bool: ...
def use_triton_template(layout: Layout, *, enable_int32: bool = False, enable_float8: bool = False) -> bool: ...
def use_triton_tma_template(*matrices: IRNode) -> bool: ...
def use_cutlass_template(layout: Layout, m: int, n: int, k: int) -> bool: ...
def _use_cutlass_for_op(op_name: str) -> bool:
    """Check if CUTLASS should be used for the given operation."""

decompose_k_threshold: int
k_splits_limit: int
default_k_splits: Incomplete
_IntLike: TypeAlias

def use_decompose_k_choice(m: _IntLike, n: _IntLike, k: _IntLike) -> bool: ...
@functools.cache
def get_k_splits(m: _IntLike, n: _IntLike, k: _IntLike) -> list[int]: ...
@functools.cache
def _rocm_native_device_arch_name(device: str) -> str: ...
@functools.cache
def try_import_ck_lib() -> tuple[str | None, Callable[[], list[Any]], Callable[[], list[Any]], type[Any]]: ...
def use_ck_template(layout: Layout) -> bool: ...
def use_ck_gemm_template(layout: Layout, m: int, n: int, k: int) -> bool: ...
def use_ck_tile_gemm_template(layout: Layout, m: int, n: int, k: int) -> bool: ...
def use_ck_conv_template(layout: Layout) -> bool: ...
def _use_template_for_cpu(layout: Layout) -> bool: ...
def use_cpp_bmm_template(layout: Layout, mat1: ReinterpretView | Buffer, mat2: IRNode) -> bool: ...
def use_cpp_gemm_template(layout: Layout, mat1: IRNode, mat2: IRNode, mat2_transposed: bool = False, require_constant_mat2: bool = True, is_woq_int4: bool = False, q_group_size: int | None = None) -> bool: ...
def use_aten_gemm_kernels() -> bool: ...

class DebugDirManager:
    counter: Incomplete
    prev_debug_name: str
    id: Incomplete
    def __init__(self) -> None: ...
    new_name: Incomplete
    def __enter__(self) -> None: ...
    def __exit__(self, *args: Any) -> None: ...

def run_and_get_code(fn: Callable[P, _T], *args: P.args, **kwargs: P.kwargs) -> tuple[_T, list[str]]: ...
def run_and_get_kernels(fn: Callable[P, _T], *args: P.args, **kwargs: P.kwargs) -> tuple[_T, list[str]]: ...
def run_fw_bw_and_get_code(fn: Callable[..., Any]) -> tuple[Any, list[str]]: ...
def get_code(fn: Callable[P, _T], *args: P.args, **kwargs: P.kwargs) -> list[str]:
    """Get the inductor-generated code, but skip any actual compilation or running."""
def get_triton_code(fn: Callable[P, _T], *args: P.args, **kwargs: P.kwargs) -> str: ...
def run_and_get_triton_code(fn: Callable[P, _T], *args: P.args, **kwargs: P.kwargs) -> str: ...
def run_and_get_graph_lowering(fn: Callable[P, _T], *args: P.args, **kwargs: P.kwargs) -> tuple[Any, list[GraphLowering]]: ...
@contextlib.contextmanager
def override_lowering(aten_op: Callable[..., Any], override_fn: Callable[..., Any]) -> Iterator[None]:
    """
    Override the lowering of aten_op with override_fn.
    The first argument of override_fn is the original lowering fn.
    """
def add_scheduler_init_hook(pre_fn: Callable[..., Any], post_fn: Callable[..., Any] | None = None) -> Any:
    """
    Add hook functions to be called at the beginning and end of Scheduler.__init__.
    Used for unit tests.
    """
def developer_warning(msg: str) -> None:
    """
    Warnings that will be actionable for PyTorch developers, but not
    end users.  Allows us to easily disable them in stable releases but
    keep them on for nightly builds.
    """
def get_benchmark_name() -> str | None:
    """
    An experimental API used only when config.benchmark_kernel is true.

    The benchmark name is only available at codegen time. So we can not
    directly call it in benchmark_all_kernels which is run after codegen.

    The function assumes the argument after --only is the benchmark name.
    It works for torchbench.py/hugginface.py/timm_models.py. But for ad-hoc
    scripts, this function may return None.

    There are 2 flavors of --only argument we need handle:
    1. --only model_name
    2. --only=model_name
    """
def is_ones(items: Sequence[Any]) -> bool: ...
def is_zeros(items: Sequence[Any]) -> bool: ...
def is_cpu_device(inputs: Sequence[torch.Tensor]) -> bool: ...
def get_sympy_Expr_dtype(val: sympy.Expr) -> torch.dtype: ...
@contextlib.contextmanager
def maybe_profile(should_profile: bool, *args: Any, **kwargs: Any) -> Iterator[Any]: ...
def parallel_num_threads() -> int: ...
@functools.cache
def get_backend_num_stages() -> int: ...
@functools.cache
def get_device_tflops(dtype: torch.dtype) -> int: ...
@functools.cache
def get_gpu_dram_gbps() -> int: ...
def get_gpu_shared_memory() -> int: ...
def is_welford_reduction(reduction_type: str) -> bool: ...
def reduction_num_outputs(reduction_type: str) -> int: ...
def is_linux() -> bool: ...
def is_windows() -> bool: ...
def has_free_symbols(itr: Iterable[Any]) -> bool: ...
def is_dynamic(*args: Any) -> bool: ...

class Placeholder(enum.Enum):
    KERNEL_NAME = 'KERNEL_NAME'
    DESCRIPTIVE_NAME = 'DESCRIPTIVE_NAME'

def pass_execution_and_save(func: Callable[..., Any], gm: GraphModule, inp: Sequence[Any], msg: str) -> None: ...
def is_multi_outputs_template(input_buf: Buffer | Operation | None) -> bool:
    """
    Check if input buffer is a multi-outputs template buffer
    """
def is_output_of_multi_outputs_template(input_buf: Buffer | Operation | None) -> bool:
    """
    Check if input buffer is a output of multi-outputs template buffer
    """
def is_collective(node: Node | Operation | None, op: torch._ops.OperatorBase | None = None) -> bool: ...
def is_wait(node: IRNode | Operation | None) -> bool: ...
def contains_collective(snode: BaseSchedulerNode) -> bool: ...
def contains_wait(snode: BaseSchedulerNode) -> bool: ...
def is_fallback_op(node: Operation | None, op: torch._ops.OpOverload | Collection[torch._ops.OpOverload]) -> bool: ...
def buf_name_to_fused_snode(buf_name: str, name_to_buf: dict[str, Any], name_to_fused_node: dict[str, Any]) -> Any: ...
def find_recursive_deps_of_node(snode: BaseSchedulerNode, collected_node_set: MutableSet[BaseSchedulerNode], name_to_buf: dict[str, SchedulerBuffer], name_to_fused_node: dict[str, BaseSchedulerNode], criteria_cb: Callable[[Any], bool] = ...) -> None: ...
def find_recursive_users_of_node(snode: BaseSchedulerNode, collected_node_set: MutableSet[BaseSchedulerNode], name_to_buf: dict[str, SchedulerBuffer], name_to_fused_node: dict[str, BaseSchedulerNode], criteria_cb: Callable[[Any], bool] = ...) -> None: ...
def num_fw_fixed_arguments(dynamo_gm_num_inputs: int, aot_fw_gm_num_inputs: int) -> int:
    """Computes the number of inputs to the aot fw graph which have fixed addresses (params and buffers)"""
def count_tangents(fx_g: torch.fx.GraphModule) -> int:
    """
    Infers which inputs are static for a backwards graph
    """

@dataclasses.dataclass
class BoxedBool:
    value: bool
    def __bool__(self) -> bool: ...
    @staticmethod
    def disable(obj: Any) -> BoxedBool | bool: ...

@contextlib.contextmanager
def collect_defined_kernels(kernel_list: list[str]) -> Iterator[None]: ...
def get_cloned_parameter_buffer_name(name: str) -> str: ...
def is_gpu(device: str | None) -> bool: ...
def device_need_guard(device: str) -> bool: ...
def needs_fallback_due_to_atomic_add_limitations(dtype: torch.dtype) -> bool: ...
def use_scatter_fallback(op_overload: torch._ops.OpOverload, reduction_type: str | None, self_dtype: torch.dtype, src_dtype: torch.dtype, src_device_type: str, src_is_tensor: bool) -> bool: ...
def dump_node_schedule(node_schedule: Sequence[BaseSchedulerNode]) -> None:
    """
    An API that can be used in pdb to dump a node_schedule.
    Right mainly dump the read/write dependencies but can add more as needed.
    """
def tensor_is_aligned(tensor: torch.Tensor) -> bool: ...
def should_assume_input_aligned(example_input: torch.Tensor) -> bool: ...
def maybe_get_suppress_shape_guards_ctx() -> contextlib.AbstractContextManager[None]: ...
def run_and_get_cpp_code(fn: Callable[P, _T], *args: P.args, **kwargs: P.kwargs) -> tuple[_T, str]: ...
def shape_env_from_inputs(inputs: Sequence[InputType]) -> ShapeEnv | None: ...
def align_inputs_from_check_idxs(model: Callable[[list[InputType]], _T], inputs_to_check: Sequence[int], mutated_input_idxs: OrderedSet[int]) -> Callable[[list[InputType]], _T]: ...
def clone_preserve_strides(x: torch.Tensor) -> torch.Tensor: ...
def copy_misaligned_inputs(new_inputs: list[InputType], check_inputs_idxs: Sequence[int], return_pair_idxs: OrderedSet[int] | None = None) -> tuple[list[torch.Tensor], list[torch.Tensor]]:
    """
    Clones misaligned tensors which we inferred were aligned. Returns a tuple of [old_tensors], [new_tensors] for every
    cloned tensor which is in `return_pair_idxs`.
    """
def remove_unaligned_input_idxs(inputs: Sequence[InputType], static_input_idxs: Sequence[int]) -> Sequence[int]:
    """
    We require all inputs to be aligned, so introduce a copy for any
    that aren't.
    """
def expr_fits_within_32bit(e: sympy.Expr) -> bool: ...
def set_tracing_context_output_strides(example_inputs: Sequence[Any], compiled_graph: CompiledFxGraph) -> None: ...
def should_use_remote_fx_graph_cache() -> bool: ...
def normalize_name(name: str) -> str: ...

_triton_type_mapping: Incomplete
_torch_triton_mapping: Incomplete
_triton_type_re: Incomplete

def triton_type(dtype: torch.dtype) -> str:
    """Convert torch.dtype to triton type"""
def triton_type_to_torch(dtype: str) -> torch.dtype: ...
def is_same_tensor(data: torch.Tensor, value: torch.Tensor) -> bool: ...
def is_same_mkldnn_tensor(data: torch.Tensor, value: torch.Tensor) -> bool: ...
@functools.cache
def boolean_ops() -> tuple[str, ...]: ...

@dataclasses.dataclass
class OpDtypeRule:
    type_promotion_kind: ELEMENTWISE_TYPE_PROMOTION_KIND
    override_return_dtype: torch.dtype | None

op_dtype_propagation_rules: dict[str, OpDtypeRule]

def register_op_dtype_propagation_rules(name: str, type_promotion_kind: ELEMENTWISE_TYPE_PROMOTION_KIND, override_return_dtype: torch.dtype | None) -> None: ...

op_requires_libdevice_fp64: OrderedSet[str]

def register_op_requires_libdevice_fp64(name: str) -> None: ...
def get_current_backend() -> str: ...
def upcast_compute_type(dtype: torch.dtype) -> torch.dtype:
    """Maybe upcast [b]float16 to float32"""
KeyType = TypeVar('KeyType')
ValType = TypeVar('ValType')

class ScopedDict(MutableMapping[KeyType, ValType]):
    """
    A dictionary-like object that allows for scoped updates. It maintains
    an original dictionary and a set of new items that can override
    the original items within the scope.  The original dictionary is
    unmodified.
    """
    original_dict: Incomplete
    new_items: dict[KeyType, ValType]
    def __init__(self, original_dict: Mapping[KeyType, ValType]) -> None: ...
    def __getitem__(self, key: KeyType) -> ValType: ...
    def __setitem__(self, key: KeyType, value: ValType) -> None: ...
    def __contains__(self, key: object) -> bool: ...
    def get(self, key: KeyType, default: ValType | None = None) -> ValType | None: ...
    def __len__(self) -> int: ...
    def __iter__(self) -> Iterator[KeyType]: ...
    def __bool__(self) -> bool: ...
    def __delitem__(self, key: KeyType) -> None: ...

@dataclass_transform(frozen_default=True)
def ir_dataclass(cls=None, /, *, frozen: bool = True) -> Any: ...
def get_donated_idxs() -> list[int] | None: ...
def set_kernel_post_grad_provenance_tracing(node_schedule: Sequence[BaseSchedulerNode] | ExternKernelOut, kernel_name: str, is_extern: bool = False) -> None: ...

class TritonAttrsDescriptorVersion(enum.Enum):
    V0_NO_TRITON = 0
    V1_COMPILER = 1
    V2_BACKENDS = 2
    V3_BACKENDS_TUPLE = 3
    V4_DICT = 4

@functools.cache
def get_triton_attrs_descriptor_version() -> TritonAttrsDescriptorVersion: ...
def triton_version_uses_attrs_dict() -> bool: ...
def is_cudagraph_unsafe_op(node: Operation) -> bool:
    """
    Returns True if the node is an op that is not cudagraphable.
    Usually only custom ops have this tag.
    """
def get_ld_library_path() -> str: ...
def is_codegen_graph_partition_subgraph(wrapper: PythonWrapperCodegen) -> bool: ...
def dtype_from_size(size: int) -> torch.dtype: ...

SUPPORTED_MKLDNN_DEVICES: Incomplete

def is_mkldnn_bf16_supported(device_type: str) -> bool:
    """
    Returns True if the device supports MKL-DNN BF16.
    """
def is_mkldnn_fp16_supported(device_type: str) -> bool:
    """
    Returns True if the device supports MKL-DNN FP16.
    """
