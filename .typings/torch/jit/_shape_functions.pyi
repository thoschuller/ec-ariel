import torch
from typing import Any, Callable

number = int | float

def broadcast(a: list[int], b: list[int]): ...
def broadcast_three(a: list[int], b: list[int], c: list[int]): ...
def broadcast_one_three(a: list[int], b: Any, c: list[int]): ...
def adaptive_avg_pool2d(self, out: list[int]): ...
def _copy(self): ...
def unary(self): ...
def broadcast_inplace(a: list[int], b: list[int]): ...
def expand(self, sizes: list[int]): ...
def expand_one_unused(self, sizes: list[int], inp0: Any): ...
def infer_size_impl(shape: list[int], numel: int) -> list[int]: ...
def numel(sizes: list[int]): ...
def view(self, sizes: list[int]): ...
def view_one_unused(self, sizes: list[int], *, implicit: bool = False): ...
def sum_mean_dim(self, opt_dims: list[int] | None, keep_dim: bool, dt: Any): ...
def max_dim(self, dim: int, keep_dim: bool): ...
def div_rtn(x: int, y: int): ...
def pooling_output_shape_pad_lr(inputSize: int, kernelSize: int, pad_l: int, pad_r: int, stride: int, dilation: int, ceil_mode: bool): ...
def pooling_output_shape(inputSize: int, kernelSize: int, pad_l: int, stride: int, dilation: int, ceil_mode: bool): ...
def pool2d_shape_check(input: list[int], kH: int, kW: int, dH: int, dW: int, padH: int, padW: int, dilationH: int, dilationW: int, nInputPlane: int, inputHeight: int, inputWidth: int, outputHeight: int, outputWidth: int): ...
def max_pool2d(input: list[int], kernel_size: list[int], stride: list[int], padding: list[int], dilation: list[int], ceil_mode: bool): ...
def max_pool2d_with_indices(input: list[int], kernel_size: list[int], stride: list[int], padding: list[int], dilation: list[int], ceil_mode: bool): ...
def upsample_nearest2d(input: list[int], output_size: list[int] | None, scale_factors: list[float] | None): ...
def mm(self, mat2: list[int]): ...
def dot(self, tensor: list[int]): ...
def mv(self, vec: list[int]): ...
def unsqueeze(li: list[int], dim: int): ...
def squeeze_nodim(li: list[int]): ...
def squeeze(li: list[int], dim: int): ...
def squeeze_dims(li: list[int], dims: list[int]): ...
def index_select(self, dim: int, index: list[int]): ...
def embedding(weight: list[int], indices: list[int], padding_idx: int = -1, scale_grad_by_freq: bool = False, sparse: bool = False): ...
def max_int(): ...
def slice(self, dim: int, start: int | None, end: int | None, step: int): ...
def check_cat_no_zero_dim(tensors: list[list[int]]): ...
def legacy_cat_wrap_dim(dim: int, tensor_sizes: list[list[int]]): ...
def should_skip(tensor: list[int]): ...
def check_cat_shape_except_dim(first: list[int], second: list[int], dimension: int, index: int): ...
def cat(tensors: list[list[int]], dim: int): ...
def stack(tensors: list[list[int]], dim: int): ...
def select(self, dim: int, index: int): ...
def matmul(tensor1: list[int], tensor2: list[int]): ...
def t(self): ...
def transpose(self, dim0: int, dim1: int): ...
def linear(input: list[int], weight: list[int], bias: list[int] | None): ...
def addmm(self, mat1: list[int], mat2: list[int], beta: Any, alpha: Any): ...
def check_non_negative(array: list[int]) -> bool: ...
def check_shape_forward(input: list[int], weight_sizes: list[int], bias: list[int] | None, stride: list[int], padding: list[int], dilation: list[int], groups: int): ...
def conv_output_size(input_size: list[int], weight_size: list[int], bias: list[int] | None, stride: list[int], padding: list[int], dilation: list[int], groups: int): ...
def conv1d(input: list[int], weight: list[int], bias: list[int] | None, stride: list[int], padding: list[int], dilation: list[int], groups: int): ...
def conv2d(input: list[int], weight: list[int], bias: list[int] | None, stride: list[int], padding: list[int], dilation: list[int], groups: int): ...
def conv_backwards(grad_output: list[int], input: list[int], weight: list[int], biases: list[int] | None): ...
def conv_transpose2d_input(input: list[int], weight: list[int], bias: list[int] | None = None, stride: list[int] | None = None, padding: list[int] | None = None, output_padding: list[int] | None = None, groups: int = 1, dilation: list[int] | None = None) -> list[int]: ...
def conv_forwards(input: list[int], weight: list[int], bias: list[int] | None, stride: list[int], padding: list[int], dilation: list[int], transposed: bool, output_padding: list[int], groups: int) -> list[int]: ...
def _conv_forwards(input: list[int], weight: list[int], bias: list[int] | None, stride: list[int], padding: list[int], dilation: list[int], transposed: bool, output_padding: list[int], groups: int, benchmark: bool, deterministic: bool, cudnn_enabled: bool, allow_tf32: bool) -> list[int]: ...
def batch_norm(input: list[int], weight: list[int] | None, bias: list[int] | None, running_mean: list[int] | None, running_var: list[int] | None, training: bool, momentum: float, eps: float, cudnn_enabled: bool): ...
def conv3d(input: list[int], weight: list[int], bias: list[int] | None, stride: list[int], padding: list[int], dilation: list[int], groups: int): ...
def maybe_wrap_dim(dim: int, dim_post_expr: int, wrap_scalar: bool = True): ...
def zero_dim_tensor(input: Any): ...
def multiply_integers(li: list[int]): ...
def arange_end(end: number, inp0: Any, inp1: Any, inp2: Any, inp3: Any): ...
def arange_start(start: number, end: number, inp0: Any, inp1: Any, inp2: Any, inp3: Any): ...
def arange_start_step(start: number, end: number, step: number, inp0: Any, inp1: Any, inp2: Any, inp3: Any): ...
def permute(input: list[int], dims: list[int]): ...
def movedim(self, source: list[int], destination: list[int]) -> list[int]: ...
def flatten(input: list[int], start_dim: int, end_dim: int): ...
def nonzero_lower_bound(input: list[int]): ...
def nonzero_upper_bound(input: list[int]): ...
def _reduce_along_dim(self, dim: int, keepdim: bool): ...
def argmax(self, dim: int | None = None, keepdim: bool = False) -> list[int]: ...
def bmm(self, mat2: list[int]) -> list[int]: ...
def _shape_as_tensor(self) -> list[int]: ...
def topk(self, k: int, dim: int = -1) -> tuple[list[int], list[int]]: ...
def nll_loss_forward(self, target: list[int], weight: list[int] | None, reduction: int) -> tuple[list[int], list[int]]: ...
def native_layer_norm(input: list[int], normalized_shape: list[int]) -> tuple[list[int], list[int], list[int]]: ...
def native_batch_norm(input: list[int], weight: list[int] | None, bias: list[int] | None, running_mean: list[int] | None, running_var: list[int] | None, training: bool) -> tuple[list[int], list[int], list[int]]: ...
def _batch_norm_with_update(input: list[int], weight: list[int] | None, bias: list[int] | None, running_mean: list[int] | None, running_var: list[int] | None) -> tuple[list[int], list[int], list[int], list[int]]: ...
def cross_entropy_loss(self, target: list[int], weight: list[int] | None = None, reduction: int = 1, ignore_index: int = -100, label_smoothing: float = 0.0) -> list[int]: ...
ScriptFn = torch._C.ScriptFunction
shape_compute_graph_mapping: dict[str, ScriptFn]
bounded_compute_graph_mapping: dict[str, tuple[ScriptFn, ScriptFn]]
script_func_map: dict[Callable, ScriptFn]

def process_func(func: Callable): ...
def add_shape_compute_mapping(operator_schema: str, func: Callable): ...
def add_bounded_compute_mapping(operator_schema: str, lower_bound_func: Callable, upper_bound_func: Callable): ...
