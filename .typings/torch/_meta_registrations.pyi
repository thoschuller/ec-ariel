import torch
from _typeshed import Incomplete
from collections.abc import Sequence
from enum import Enum
from torch import SymBool as SymBool, SymFloat as SymFloat, Tensor as Tensor
from torch._decomp import _add_op_to_registry as _add_op_to_registry, _convert_out_params as _convert_out_params, global_decomposition_table as global_decomposition_table, meta_table as meta_table
from torch._ops import OpOverload as OpOverload
from torch._prims import ELEMENTWISE_PRIM_TYPE_PROMOTION_KIND as ELEMENTWISE_PRIM_TYPE_PROMOTION_KIND, _prim_elementwise_meta as _prim_elementwise_meta, view_of as view_of
from torch._prims_common import BoolLike as BoolLike, ELEMENTWISE_TYPE_PROMOTION_KIND as ELEMENTWISE_TYPE_PROMOTION_KIND, FloatLike as FloatLike, IntLike as IntLike, Number as Number, TensorLike as TensorLike, corresponding_complex_dtype as corresponding_complex_dtype, corresponding_real_dtype as corresponding_real_dtype, definitely_contiguous as definitely_contiguous, elementwise_dtypes as elementwise_dtypes, is_contiguous as is_contiguous, make_contiguous_strides_for as make_contiguous_strides_for, suggest_memory_format as suggest_memory_format
from torch._prims_common.wrappers import _maybe_convert_to_dtype as _maybe_convert_to_dtype, _maybe_resize_out as _maybe_resize_out, _resize_output_check as _resize_output_check, _safe_copy_out as _safe_copy_out, out_wrapper as out_wrapper
from torch._refs import _broadcast_shapes as _broadcast_shapes, _maybe_broadcast as _maybe_broadcast
from typing import Callable, TypeVar
from typing_extensions import ParamSpec

_T = TypeVar('_T')
_P = ParamSpec('_P')
aten: Incomplete
_meta_lib_dont_use_me_use_register_meta: Incomplete
MODE_SUM: Incomplete
MODE_MEAN: Incomplete
MODE_MAX: Incomplete

def register_meta(op) -> Callable[[Callable[_P, _T]], Callable[_P, _T]]: ...
def elementwise_meta(*args, type_promotion: ELEMENTWISE_TYPE_PROMOTION_KIND): ...
def toRealValueType(dtype): ...
def check_inplace_broadcast(self_shape, *args_shape): ...
def meta_linspace_logspace(start, end, steps, base=None, dtype=None, device=None, layout=..., pin_memory: bool = False, requires_grad: bool = False): ...
def meta_take(self, index): ...
def linalg_cross(self, other, *, dim: int = -1): ...
def _compute_stride(old_shape, old_stride, new_shape, size_oblivious: bool = False): ...
def _view_has_unbacked_input(a, shape): ...
def _view_unbacked_meta(a, shape, size_oblivious_enabled: bool = True): ...
def _view_meta(a, *shape): ...
def linalg_matrix_exp(self): ...
def cummaxmin(self, dim): ...
def logcumsumexp(self, dim): ...
def _exec_fft(out, self, out_sizes, dim, *, forward): ...
def _sort_dims(self, dim: list[int], exclude_last: bool = False): ...
def meta_fft_c2c(self, dim, normalization, forward): ...

cufft_max_ndim: int

def use_optimized_cufft_path(dim: list[int]): ...
def meta_fft_r2c(self, dim, normalization, onesided): ...
def meta_randperm(n, *, generator=None, out): ...
def meta_randperm_default(n, *, dtype=..., layout=None, device=None, pin_memory=None): ...
def meta_randint(high, size, *, dtype=..., layout=None, device=None, pin_memory=None): ...
def meta_randint_low(low, high, size, *, dtype=..., layout=None, device=None, pin_memory=None): ...
def meta_rand_default(size, *, dtype=None, layout=None, device=None, pin_memory=None): ...
def meta_fft_c2r(self, dim: list[int], normalization: int, lastdim: int): ...
def meta_copy_(self, src, non_blocking: bool = False): ...
def inferUnsqueezeGeometry(tensor, dim): ...
def meta_unsqueeze_(self, dim): ...
def meta_sparse_structured_linear(input: Tensor, weight: Tensor, _meta: Tensor, bias: Tensor | None = None, _activation_opt: str | None = None, out_dtype: torch.dtype | None = None): ...
def meta_sparse_structured_mm(mat1: Tensor, mat1_meta: Tensor, mat2: Tensor, out_dtype: torch.dtype | None = None): ...
def meta_sparse_structured_addmm(input: Tensor, mat1: Tensor, mat1_meta: Tensor, mat2: Tensor, *, alpha: int = 1, beta: int = 1, out_dtype: torch.dtype | None = None): ...
def meta__cslt_sparse_mm(compressed_A: torch.Tensor, dense_B: torch.Tensor, bias: Tensor | None = None, alpha: Tensor | None = None, out_dtype: torch.dtype | None = None, transpose_result: bool = False, alg_id: int = 0, split_k: int = 1, split_k_mode: int = -1): ...
def meta_index_reduce(self, dim: int, index: Tensor, source: torch.Tensor, reduce: str, *, include_self: bool = True) -> Tensor: ...
def meta_index_reduce_(self, dim: int, index: Tensor, source: torch.Tensor, reduce: str, *, include_self: bool = True) -> Tensor: ...
def meta_index_select(self, dim, index): ...
def meta_segment_reduce(data: Tensor, reduce: str, *, lengths: Tensor | None = None, indices: Tensor | None = None, offsets: Tensor | None = None, axis: int = 0, unsafe: bool = False, initial=None) -> Tensor: ...
def meta_max(self): ...
def meta_max_dim(self, dim, keepdim: bool = False): ...
def meta_min(self): ...
def meta_min_dim(self, dim, keepdim: bool = False): ...
def meta_angle(self): ...
def meta_angle_out(self, out): ...
def assert_async(val) -> None: ...
def assert_async_meta(val, assert_msg) -> None: ...
def print_meta(s) -> None: ...
def make_dep_token(*, dtype=None, layout=None, device=None, pin_memory=None, memory_format=None): ...
def sym_constrain_range(size, min=None, max=None) -> None: ...
def functional_sym_constrain_range(size, min=None, max=None, dep_token=None): ...
def sym_constrain_range_for_size(size, min=None, max=None) -> None: ...
def functional_sym_constrain_range_for_size(size, min, max, dep_token): ...
def functional_assert_async_meta(val, assert_msg, dep_token): ...
def squareCheckInputs(self, f_name: str): ...
def linearSolveCheckInputs(self, A: Tensor, name: str): ...
def checkFloatingOrComplex(t: Tensor, f_name: str, allow_low_precision_dtypes: bool = True): ...
def checkIsMatrix(A: Tensor, f_name: str, arg_name: str = 'A'): ...
def checkInputsSolver(A: Tensor, B: Tensor, left: bool, f_name: str): ...
def checkSameDevice(fn_name: str, result: Tensor, input: Tensor, result_name: str = 'result'): ...
def checkUplo(UPLO: str): ...
def meta__linalg_eigh(A: Tensor, UPLO: str = 'L', compute_v: bool = True): ...
def meta__linalg_eigvals(input: Tensor) -> Tensor: ...
def meta_linalg_eig(input: Tensor): ...
def cloneBatchedColumnMajor(src: Tensor) -> Tensor: ...
def _cholesky_solve_helper(self, A: Tensor, upper: bool) -> Tensor: ...
def cholesky_solve(self, A: Tensor, upper: bool = False) -> Tensor: ...
def cholesky(self, upper: bool = False) -> Tensor: ...
def cholesky_inverse(self, upper: bool = False) -> Tensor: ...
def linalg_cholesky_ex(A: Tensor, upper: bool = False, check_errors: bool = False): ...
def linalg_householder_product(input: Tensor, tau: Tensor) -> Tensor: ...
def linalg_inv_ex_meta(A: Tensor, check_errors: bool = False): ...
def linalg_ldl_factor_ex_meta(self, *, hermitian: bool = False, check_errors: bool = False) -> tuple[Tensor, Tensor, Tensor]: ...
def linalg_ldl_solve_meta(LD: Tensor, pivots: Tensor, B: Tensor, *, hermitian: bool = False) -> Tensor: ...
def linalg_lu_meta(A: Tensor, *, pivot: bool = True) -> tuple[Tensor, Tensor, Tensor]: ...
def linalg_lu_factor_ex_meta(A: Tensor, *, pivot: bool = True, check_errors: bool = False) -> tuple[Tensor, Tensor, Tensor]: ...
def linalg_lu_solve_meta(LU: Tensor, pivots: Tensor, B: Tensor, *, left: bool = True, adjoint: bool = False) -> Tensor: ...
def lu_unpack_meta(LU: Tensor, pivots: Tensor, unpack_data: bool = True, unpack_pivots: bool = True) -> tuple[Tensor, Tensor, Tensor]: ...
def _parse_qr_mode(mode: str) -> tuple[bool, bool]: ...
def linalg_qr_meta(A: Tensor, mode: str = 'reduced') -> tuple[Tensor, Tensor]: ...
def _linalg_slogdet(A: Tensor) -> tuple[Tensor, Tensor, Tensor, Tensor]: ...
def _linalg_svd_meta(A: Tensor, full_matrices: bool = False, compute_uv: bool = True, driver: str | None = None): ...
def _linalg_broadcast_batch_dims(arg1: Tensor, arg2: Tensor) -> tuple[list[int], list[int]]: ...
def _linalg_broadcast_batch_dims_name(arg1: Tensor, arg2: Tensor, name: str | None) -> tuple[Tensor, Tensor]: ...
def linalg_solve_is_vector_rhs(input: Tensor, other: Tensor) -> bool: ...
def _linalg_solve_ex(A: Tensor, B: Tensor, *, left: bool = True, check_errors: bool = False, result: Tensor | None = None, LU: Tensor | None = None, pivots: Tensor | None = None, info: Tensor | None = None) -> tuple[Tensor, Tensor, Tensor, Tensor]: ...
def linalg_solve_triangular_meta(A: Tensor, B: Tensor, *, upper: bool, left: bool = True, unitriangular: bool = False, out: Tensor | None = None) -> Tensor: ...
def triangular_solve_meta(self, A: Tensor, upper: bool = True, transpose: bool = False, unitriangular: bool = False) -> tuple[Tensor, Tensor]: ...
def _linalg_det_meta(A): ...
def ormqr(input: Tensor, tau: Tensor, other: Tensor, left: bool = True, transpose: bool = False) -> Tensor: ...
def _padding_check_valid_input(input, padding, *, dim): ...
def _pad1d_common(input, padding, *, is_reflection): ...
def meta_reflection_pad1d(input, padding): ...
def meta_replication_pad1d(input, padding): ...
def _pad1d_backward_common(grad_output, input, padding, *, is_reflection): ...
def meta_reflection_pad1d_backward(grad_output, input, padding): ...
def meta_replication_pad1d_backward(grad_output, input, padding): ...
def _pad2d_common(input, padding, *, is_reflection): ...
def meta_reflection_pad2d(input, padding): ...
def meta_replication_pad2d(input, padding): ...
def meta_pad2d_backward(grad_output, self, padding): ...
def _pad3d_common(input, padding, *, is_reflection): ...
def meta_reflection_pad3d(input, padding): ...
def meta_replication_pad3d(input, padding): ...
def meta_pad3d_backward(grad_output, input, padding): ...
def meta__pdist_forward(self, p: float = 2) -> Tensor: ...
def meta__pdist_backward(grad: Tensor, self: Tensor, p: float, pdist: Tensor) -> Tensor: ...
def meta_baddbmm(self, batch1, batch2, *, beta: int = 1, alpha: int = 1): ...
def meta_bernoulli(self, *, generator=None): ...
def meta_bernoulli_(self, p: float = 0.5, generator=None): ...
def meta_bernoulli_p(self, p: float = 0.5, generator=None): ...
def meta_poisson(self, generator=None): ...
def meta__fused_moving_avg_obs_fq_helper(self, observer_on, fake_quant_on, running_min, running_max, scale, zero_point, averaging_const, quant_min, quant_max, ch_axis, per_row_fake_quant: bool = False, symmetric_quant: bool = False): ...
def meta_mm(a, b): ...
def _compute_reduction_shape(self, dims, keepdim): ...
def device_hint(tensor) -> str: ...
def calc_conv_nd_return_shape(input_tensor: torch.Tensor, weight: torch.Tensor, stride: list[int] | int, padding: list[int] | int, dilation: list[int] | int, is_transposed: bool, groups: int, output_padding: list[int] | int | None = None): ...
def is_channels_last(ten): ...
def meta_miopen_batch_norm(input_tensor: torch.Tensor, weight: torch.Tensor, bias: torch.Tensor | None, running_mean: torch.Tensor | None, running_var: torch.Tensor | None, training: bool, exponential_average_factor: float, epsilon: float): ...
def meta_conv(input_tensor: torch.Tensor, weight: torch.Tensor, bias: torch.Tensor, stride: list[int], padding: list[int], dilation: list[int], is_transposed: bool, output_padding: list[int], groups: int): ...

_meta_lib_dont_use_me_use_register_meta_for_mkldnn: Incomplete

def meta_mkldnn_convolution_default(input_tensor, weight, bias, padding, stride, dilation, groups, attr, scalars, algorithm): ...
def meta_linear_pointwise_default(input_tensor, weight, bias, attr, scalars, algorithm): ...

_meta_lib_dont_use_me_use_register_meta_for_mkl: Incomplete

def meta_mkl_linear(input_tensor, packed_weight, orig_weight, bias, batch_size): ...

_meta_lib_dont_use_me_use_register_meta_for_onednn: Incomplete

def meta_qconv_pointwise(x, x_scale, x_zp, w, w_scale, w_zp, bias, stride, padding, dilation, groups, output_scale, output_zero_point, output_dtype, attr, scalars, algorithm): ...
def meta_qconv2d_pointwise_binary(x, x_scale, x_zp, w, w_scale, w_zp, accum, bias, stride, padding, dilation, groups, output_scale, output_zero_point, output_dtype, accum_scale, accum_zero_point, binary_op_name, alpha, unary_op_name, unary_op_args, unary_op_algorithm): ...
def meta_qlinear_pointwise(x, x_scale, x_zp, w, w_scale, w_zp, bias, output_scale, output_zero_point, output_dtype, post_op_name, post_op_args, post_op_algorithm): ...
def meta_qlinear_pointwise_binary(x, x_scale, x_zp, w, w_scale, w_zp, x_2, bias, output_scale, output_zero_point, output_dtype, x2_scale, x2_zp, binary_op_name, alpha, unary_op_name, unary_op_args, unary_op_algorithm): ...
def meta_linear_dynamic_fp16(x, w, bias): ...

_meta_lib_dont_use_me_use_register_meta_for_quantized: Incomplete

def meta_quantized_max_pool2d(input, kernel_size, stride=(), padding=(0,), dilation=(1,), ceil_mode: bool = False): ...
def meta_int4mm_packed_weight_cpu(x, w, q_group_size, q_scale_and_zeros): ...
def check_dim_size(tensor, dim, dim_size, size): ...
def meta_avg_pool2d(input, kernel_size, stride=(), padding=(0,), ceil_mode: bool = False, count_include_pad: bool = True, divisor_override=None): ...
def avg_pool2d_backward_shape_check(input, gradOutput, nbatch, kH, kW, dH, dW, padH, padW, nInputPlane, inputHeight, inputWidth, outputHeight, outputWidth, mem_format) -> None: ...
def meta_avg_pool2d_backward(gradOutput_, input, kernel_size, stride, padding, ceil_mode, count_include_pad, divisor_override): ...
def meta_avg_pool3d(input, kernel_size, stride=(), padding=(0,), ceil_mode: bool = False, count_include_pad: bool = True, divisor_override=None): ...
def meta_avg_pool3d_backward(grad_output, input, kernel_size, stride, padding, ceil_mode, count_include_pad, divisor_override): ...
def meta_adaptive_avg_pool2d(self, output_size): ...
def meta_adaptive_avg_pool3d(self, output_size): ...
def meta__adaptive_avg_pool2d_backward(grad_out, self): ...
def meta__adaptive_avg_pool3d_backward(grad_output, self): ...
def _adaptive_pool_empty_output_check(grad_output: Tensor, arg_name: str): ...
def meta_adaptive_max_pool2d(input, output_size): ...
def meta_adaptive_max_pool2d_backward(grad_output, input, indices): ...
def meta_adaptive_max_pool3d(input, output_size): ...
def meta_adaptive_max_pool3d_backward(grad_output, input, indices): ...
def meta_repeat_interleave_Tensor(repeats, output_size=None): ...
def meta_complex(real, imag): ...
def nonzero_static(self, *, size, fill_value: int = -1): ...
def nonzero(self): ...
def meta_index_Tensor(self, indices): ...
def meta_convolution_backward(grad_output_, input_, weight_, bias_sizes_opt, stride, padding, dilation, transposed, output_padding, groups, output_mask): ...
def meta_addbmm(self, batch1, batch2, *, beta: int = 1, alpha: int = 1): ...
def meta_randint_like(self, high, **kwargs): ...
def meta__fused_adam_(self, grads, exp_avgs, exp_avg_sqs, max_exp_avg_sqs, state_steps, *, lr, beta1, beta2, weight_decay, eps, amsgrad, maximize, grad_scale=None, found_inf=None): ...
def meta__fused_adam(self, grads, exp_avgs, exp_avg_sqs, max_exp_avg_sqs, state_steps, *, lr, beta1, beta2, weight_decay, eps, amsgrad, maximize, grad_scale=None, found_inf=None): ...
def meta__int_mm(a, b): ...
def meta__convert_weight_to_int4pack(w, inner_k_tiles): ...
def meta__convert_weight_to_int4pack_for_cpu(w, inner_k_tiles): ...
def meta__weight_int4pack_mm(x, w, q_group_size, q_scale_and_zeros): ...
def meta__weight_int4pack_mm_for_cpu(x, w, q_group_size, q_scale_and_zeros): ...
def _weight_int4pack_mm_with_scales_and_zeros(x, w, q_group_size, qScale, qZeros): ...
def kai_roundup(a: int, b: int) -> int: ...
def get_kai_packed_weight_size(n_bits, N, K, groupsize): ...
def meta__dyn_quant_pack_4bit_weight(weights, scales_zeros, bias: Tensor | None, block_size, in_features, out_features): ...
def meta__dyn_quant_matmul_4bit(inp, packed_weights, block_size, in_features, out_features): ...
def meta__weight_int8pack_mm(x, w, q_scales): ...
def meta_cdist_forward(x1, x2, p, compute_mode): ...
def meta_cdist_backward(grad, x1, x2, p, cdist): ...
def meta_embedding_bag(weight, indices, offsets, scale_grad_by_freq: bool = False, mode: int = 0, sparse: bool = False, per_sample_weights=None, include_last_offset: bool = False, padding_idx: int = -1): ...
def meta_embedding_bag_forward_only(weight, indices, offsets, *args): ...
def _get_reduction_dtype(input, dtype, promote_int_to_long: bool = True): ...
def meta_nansum(input, dims=None, keepdim: bool = False, *, dtype=None): ...
def meta_median(input): ...
def meta_median_mode_dim(input, dim: int = -1, keepdim: bool = False): ...
def meta_logical_not_(self): ...
def meta_repeat(self, repeats): ...
def meta_zero_(self): ...
def meta_binop_inplace(self, other): ...
def meta_binop_inplace_alpha(self, other, alpha: int = 1):
    """
    Some checks for inplace ops.
    Checks for promotion rules for some dtypes.
    int.add/sub_(float) and bool.add/sub_(others) are rejected.
    Promoting in these in-place operations would require reallocating
    and copying over elements, hence not allowed.
    Checks for alpha param.
    """
def meta_round(self, **kwargs): ...
def shift_dtype_check(fn_name, self, val): ...
def meta_rshifts(self, other): ...
def meta_lshifts(self, other): ...
def meta_zero(self): ...
def meta_fill_(self, val): ...
def meta_fill(self, val): ...
def meta_relu_(self): ...
def meta__add_relu(self, other, alpha: int = 1) -> Tensor: ...
def meta_rrelu_with_noise(self, noise, lower: float = 0.125, upper: float = 0.3333333333333333, training: bool = False, generator=None): ...
def meta_rrelu_with_noise_functional(self, noise, lower: float = 0.125, upper: float = 0.3333333333333333, training: bool = False, generator=None): ...
def meta_rrelu_with_noise_(self, lower: float = 0.125, upper: float = 0.3333333333333333, training: bool = False, generator=None): ...
def meta_index_put(self, indices, values, accumulate: bool = False): ...
def meta_masked_fill_(self, mask, value): ...
def meta__masked_scale(self, mask, scale): ...
def meta_masked_scatter_(self, mask, source): ...
def meta_masked_scatter(self, mask, source): ...
def meta_masked_scatter_backward(self, mask, sizes): ...
def meta_index_put_(self, indices, values, accumulate: bool = False): ...
def meta_alias(self): ...
def common_meta_baddbmm_bmm(batch1, batch2, is_bmm, self_baddbmm=None, out_dtype=None): ...
def meta_bmm(self, mat2): ...
def meta_bmm_dtype(self, mat2, out_dtype): ...
def div_rtn(x, y): ...
def pooling_output_shape_pad_lr(inputSize, kernelSize, pad_l, pad_r, stride, dilation, ceil_mode): ...
def pooling_output_shape(inputSize, kernelSize, pad, stride, dilation, ceil_mode): ...
def pool2d_shape_check(input, kH, kW, dH, dW, padH, padW, dilationH, dilationW, nInputPlane, inputHeight, inputWidth, outputHeight, outputWidth, memory_format): ...
def pool3d_shape_check(input: Tensor, nslices: int, kT: int, kH: int, kW: int, dT: int, dH: int, dW: int, pT: int, pH: int, pW: int, dilationT: int, dilationH: int, dilationW: int, itime: int, iheight: int, iwidth: int, otime: int, oheight: int, owidth: int, fn_name: str, check_input_size: bool = False): ...
def max_pool3d_backward_shape_check(input, grad_output, indices, nslices, kT, kH, kW, dT, dH, dW, pT, pH, pW, dilationT, dilationH, dilationW, itime, iheight, iwidth, otime, oheight, owidth, fn_name) -> None: ...
def avg_pool3d_backward_shape_check(input: Tensor, grad_output: Tensor, nslices: int, kT: int, kH: int, kW: int, dT: int, dH: int, dW: int, pT: int, pH: int, pW: int, itime: int, iheight: int, iwidth: int, otime: int, oheight: int, owidth: int, fn_name: str): ...
def max_pool2d_checks_and_compute_shape(input, kernel_size, stride, padding, dilation, ceil_mode): ...
def meta_max_pool2d_with_indices_backward(grad_output, self, kernel_size, stride, padding, dilation, ceil_mode, indices): ...
def meta_max_pool2d_with_indices(input, kernel_size, stride=(), padding=(0,), dilation=(1,), ceil_mode: bool = False): ...
def meta_fractional_max_pool2d(self, kernel_size, output_size, random_samples): ...
def meta_max_pool3d_with_indices(input, kernel_size, stride=(), padding=(0,), dilation=(1,), ceil_mode: bool = False): ...
def meta_max_pool3d_with_indices_backward(grad_output, input, kernel_size, stride, padding, dilation, ceil_mode, indices): ...
def check_grid_sampler_common(input: Tensor, grid: Tensor): ...

class GridSamplerInterpolation(Enum):
    BILINEAR = 0
    NEAREST = 1
    BICUBIC = 2

def check_grid_sampler_3d(input: Tensor, grid: Tensor, interpolation_mode: int): ...
def grid_sampler_2d_backward_meta(grad_output, input, grid, interpolation_mode, padding_mode, align_corners, output_mask): ...
def grid_sampler_3d(input, grid, interpolation_mode, padding_mode, align_corners): ...
def grid_sampler_3d_backward(grad_output, input, grid, interpolation_mode, padding_mode, align_corners, output_mask): ...
def full(size, fill_value, *args, **kwargs): ...
def zeros_like(self, dtype=None, layout=None, device=None, pin_memory=None, memory_format=None): ...
def meta_ones(size, *, dtype=None, layout=None, device=None, pin_memory=None, requires_grad: bool = False): ...
def meta_zeros(size, *, dtype=None, layout=None, device=None, pin_memory=None, requires_grad: bool = False): ...
def meta_select(self, dim, index): ...
def meta_select_scatter(self, src, dim, index): ...
def meta_slice_scatter(self, src, dim: int = 0, start=None, end=None, step: int = 1): ...
def maybe_wrap_dim(dim: int, dim_post_expr: int, wrap_scalar: bool = True): ...
def ensure_nonempty_size(t, dim): ...
def gather_shape_check(self, dim, index): ...
def meta_gather(self, dim, index, sparse_grad: bool = False): ...
def get_operator_enum(reduce_, use_new_options: bool = False): ...
def scatter_gather_dtype_check(method_name, self, index, src_opt=None): ...
def ensure_nonempty_dim(dim): ...
def scatter_shape_check(self, dim, index, src_opt=None): ...
def scatter_meta_impl(self, dim, index, src=None, reduce_=None, use_new_options: bool = False) -> None: ...
def meta_scatter_add(self, dim, index, src): ...
def meta_scatter_add_(self, dim, index, src): ...
def meta_scatter(self, dim, index, src_or_value, reduce=None): ...
def meta_scatter_(self, dim, index, src_or_value, reduce=None): ...
def meta__scaled_dot_product_flash_attention(query: Tensor, key: Tensor, value: Tensor, dropout_p: float = 0.0, is_causal: bool = False, return_debug_mask: bool = False, scale: float | None = None): ...
def alloc_with_matching_layout(query: Tensor, res_shape: tuple[int, ...]): ...
def meta__scaled_dot_product_cudnn_attention(query: Tensor, key: Tensor, value: Tensor, attn_bias: Tensor | None, compute_log_sumexp: bool, dropout_p: float = 0.0, is_causal: bool = False, return_debug_mask: bool = False, scale: float | None = None): ...
def meta__scaled_dot_product_fused_attention_overrideable(query: Tensor, key: Tensor, value: Tensor, attn_bias: Tensor | None = None, dropout_p: float = 0.0, is_causal: bool = False, return_debug_mask: bool = False, scale: float | None = None): ...
def meta__scaled_dot_product_flash_backward(grad_out: Tensor, query: Tensor, key: Tensor, value: Tensor, out: Tensor, logsumexp: Tensor, cum_seq_q: Tensor, cum_seq_k: Tensor, max_q: int, max_k: int, dropout_p: float, is_causal: bool, philox_seed: Tensor, philox_offset: Tensor, scale: float | None = None): ...
def meta__scaled_dot_product_flash_attention_for_cpu(query: Tensor, key: Tensor, value: Tensor, dropout_p: float = 0.0, is_causal: bool = False, attn_mask: Tensor | None = None, scale: float | None = None): ...
def meta__scaled_dot_product_flash_attention_for_cpu_backward(grad_out: Tensor, query: Tensor, key: Tensor, value: Tensor, out: Tensor, logsumexp: Tensor, dropout_p: float, is_causal: bool, attn_mask: Tensor | None = None, scale: float | None = None): ...
def meta__scaled_dot_product_efficient_attention(query: Tensor, key: Tensor, value: Tensor, attn_bias: Tensor | None, compute_log_sumexp: bool, dropout_p: float = 0.0, is_causal: bool = False, scale: float | None = None): ...
def meta__scaled_dot_product_efficient_backward(grad_out: Tensor, query: Tensor, key: Tensor, value: Tensor, attn_bias: Tensor | None, out: Tensor, logsumexp: Tensor, philox_seed: Tensor, philox_offset: Tensor, dropout_p: float, grad_input_mask: list[bool], is_causal: bool = False, scale: float | None = None): ...
def meta__scaled_dot_product_cudnn_backward(grad_out: Tensor, query: Tensor, key: Tensor, value: Tensor, out: Tensor, logsumexp: Tensor, philox_seed: Tensor, philox_offset: Tensor, attn_bias: Tensor, cum_seq_q: Tensor, cum_seq_k: Tensor, max_q: int, max_k: int, dropout_p: float, is_causal: bool, scale: float | None = None): ...
def meta__flash_attention_forward(query: Tensor, key: Tensor, value: Tensor, cum_seq_q: Tensor | None, cum_seq_k: Tensor | None, max_q: int, max_k: int, dropout_p: float, is_causal: bool, return_debug_mask: bool, scale: float | None = None, window_size_left: int | None = None, window_size_right: int | None = None, seqused_k: Tensor | None = None, alibi_slopes: Tensor | None = None): ...
def meta__flash_attention_backward(grad_out: Tensor, query: Tensor, key: Tensor, value: Tensor, out: Tensor, logsumexp: Tensor, cum_seq_q: Tensor, cum_seq_k: Tensor, max_q: int, max_k: int, dropout_p: float, is_causal: bool, philox_seed: Tensor, philox_offset: Tensor, scale: float | None = None, window_size_left: int | None = None, window_size_right: int | None = None): ...
def meta__efficient_attention_forward(query: Tensor, key: Tensor, value: Tensor, bias: Tensor | None, cu_seqlens_q: Tensor | None, cu_seqlens_k: Tensor | None, max_seqlen_q: int | None, max_seqlen_k: int | None, dropout_p: float, custom_mask_type: int, compute_log_sumexp: bool = False, scale: float | None = None, causal_diagonal: Tensor | None = None, seqlen_k: Tensor | None = None, window_size: int | None = None): ...
def meta__efficient_attention_backward(grad_out: Tensor, query: Tensor, key: Tensor, value: Tensor, bias: Tensor | None, cu_seqlens_q: Tensor | None, cu_seqlens_k: Tensor | None, max_seqlen_q: torch.SymInt, max_seqlen_k: torch.SymInt, logsumexp: Tensor, dropout_p: float, philox_seed: Tensor, philox_offset: Tensor, custom_mask_type: int, bias_requires_grad: bool, scale: float | None = None, num_splits_key: int | None = None, shared_storage_dqdkdv: bool = False): ...
def meta_scaled_mm(self, mat2: torch.Tensor, scale_a: torch.Tensor, scale_b: torch.Tensor, bias: torch.Tensor | None = None, scale_result: torch.Tensor | None = None, out_dtype: torch.dtype | None = None, use_fast_accum: bool = False): ...
def meta_scatter_reduce_two(self, dim, index, src, reduce, include_self: bool = True): ...
def meta_scatter_reduce__two(self, dim, index, src, reduce, include_self: bool = True): ...
def meta_multinomial(input, num_samples, replacement: bool = False, *, generator=None): ...
def multiply_integers(vs): ...
def upsample_common_check(input_size, output_size, num_spatial_dims): ...
def upsample_nearest1d(input, output_size, scales=None): ...
def upsample_nearest2d(input, output_size, scales_h=None, scales_w=None): ...
def upsample_nearest2d_backward(grad_output: Tensor, output_size: Sequence[int | torch.SymInt], input_size: Sequence[int | torch.SymInt], scales_h: float | None = None, scales_w: float | None = None): ...
def upsample_nearest3d(input, output_size, scales_d=None, scales_h=None, scales_w=None): ...
def meta_sort(self, stable=None, dim: int = -1, descending: bool = False, values=None, indices=None): ...
def rnn_cell_checkSizes(input_gates, hidden_gates, input_bias, hidden_bias, factor, prev_hidden): ...
def _thnn_fused_lstm_cell_meta(input_gates, hidden_gates, cx, input_bias=None, hidden_bias=None): ...
def _cudnn_rnn(input, weight, weight_stride0, weight_buf, hx, cx, mode, hidden_size, proj_size, num_layers, batch_first, dropout, train, bidirectional, batch_sizes, dropout_state): ...
def mkldnn_rnn_layer(input, w0, w1, w2, w3, hx_, cx_, reverse, batch_sizes, mode, hidden_size, num_layers, has_biases, bidirectional, batch_first, train): ...
def zero_numel_check_dims(self, dim, fn_name): ...
def check_argmax_argmin(name, self, dim): ...
def argmax_argmin_meta(self, dim=None, keepdim: bool = False): ...
def scalar_tensor(s, dtype=None, layout=None, device=None, pin_memory=None): ...
def topk_meta(self, k, dim: int = -1, largest: bool = True, sorted: bool = True): ...
def meta__segment_reduce_backward(grad, output, data, reduce, lengths=None, offsets=None, axis: int = 0, initial=None): ...
def kthvalue_meta(self, k, dim: int = -1, keepdim: bool = False): ...

legacy_contiguous_memory_format: Incomplete

def checkLSTMBackwardSizes(grad_hy, grad_cy, cx, cy, workspace): ...
def _thnn_fused_lstm_cell_backward_impl(grad_hy, grad_cy, cx, cy, workspace, has_bias): ...
def linear_backward(input_, grad_output_, weight_, output_mask): ...
def meta_pixel_shuffle(self, upscale_factor): ...
def mkldnn_rnn_layer_backward(input, weight0, weight1, weight2, weight3, hx_, cx_tmp, output, hy_, cy_, grad_output_r_opt, grad_hy_r_opt, grad_cy_r_opt, reverse, mode, hidden_size, num_layers, has_biases, train, bidirectional, batch_sizes, batch_first, workspace): ...
def meta_bucketize(self, boundaries, *, out_int32: bool = False, right: bool = False): ...
def meta_histc(input, bins: int = 100, min: int = 0, max: int = 0): ...
def meta_upsample_bimode2d_aa(input, output_size, align_corners, scales_h=None, scales_w=None): ...
def meta_upsample_bimode2d_aa_backward(grad_output, output_size, input_size, align_corners, scales_h=None, scales_w=None): ...
def _amp_foreach_non_finite_check_and_unscale_(self, found_inf, inv_scale): ...
def nan_to_num(self, nan=None, posinf=None, neginf=None): ...
def transpose_(self, dim0, dim1): ...
def t_(self): ...
def meta_searchsorted(sorted_sequence, self, *, out_int32: bool = False, right: bool = False, side=None, sorter=None): ...
def _check_for_unsupported_isin_dtype(dtype): ...
def meta_embedding_dense_backward(grad_output, indices, num_weights, padding_idx, scale_grad_by_freq): ...
def meta_embedding_bag_backward(grad, indices, offsets, offset2bag, bag_size, maximum_indices, num_weights, scale_grad_by_freq, mode, sparse, per_sample_weights, padding_idx: int = -1): ...
def meta_embedding_bag_dense_backward(grad, indices, offset2bag, bag_size, maximum_indices, num_weights, scale_grad_by_freq, mode, per_sample_weights, padding_idx: int = -1): ...
def meta_embedding_bag_per_sample_weights_backward(grad, weight, indices, offsets, offset2bag, mode, padding_idx: int = -1): ...
def meta_isin(elements, test_elements, *, assume_unique: bool = False, invert: bool = False): ...
def meta_polygamma(n: int, self: Tensor) -> Tensor: ...
def meta_local_scalar_dense(self): ...
def silu(self) -> Tensor: ...
def sigmoid(self) -> Tensor: ...
def _create_grouped_mm_output_tensor(mat1, mat2, offs, out_dtype): ...
def _meta_grouped_mm_common(mat_a: Tensor, mat_b: Tensor, scale_a: torch.Tensor | None, scale_b: torch.Tensor | None, offs: Tensor | None = None, bias: Tensor | None = None, scale_result: torch.Tensor | None = None, out_dtype: torch.dtype | None = None, use_fast_accum: bool = False): ...
def grouped_mm(mat_a: Tensor, mat_b: Tensor, offs: Tensor | None = None, bias: Tensor | None = None, out_dtype: torch.dtype | None = None) -> Tensor: ...
def meta_scaled_grouped_mm(mat_a: torch.Tensor, mat_b: torch.Tensor, scale_a: torch.Tensor, scale_b: torch.Tensor, offs: torch.Tensor | None = None, bias: torch.Tensor | None = None, scale_result: torch.Tensor | None = None, out_dtype: torch.dtype | None = None, use_fast_accum: bool = False): ...
def softmax(x: Tensor, dim: int, half_to_float: bool) -> Tensor: ...
def _constant_pad_nd_meta(input, pad, value: int = 0): ...
def embedding(weight: Tensor, indices: Tensor, padding_idx: int = -1, scale_grad_by_freq: bool = False, sparse: bool = False) -> Tensor: ...
def meta__jagged_to_padded_dense_forward(values: Tensor, offsets: list[Tensor], max_lengths: list[int], padding_value: float = 0.0): ...
def _create_unary_float_meta_func(func): ...
def _create_binary_float_meta_func(func): ...
def _register_inplace_meta(fn): ...
def lerp(start, end, weight): ...
def addcmul(input, tensor1, tensor2, *, value: int = 1): ...
def addcdiv(input, tensor1, tensor2, *, value: int = 1): ...

lerp_: Incomplete
addcmul_: Incomplete
addcdiv_: Incomplete

def activate_meta() -> None: ...
