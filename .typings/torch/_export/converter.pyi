import torch
from _typeshed import Incomplete
from collections.abc import Generator, Sequence
from contextlib import contextmanager
from torch import _C as _C
from torch._export.passes.replace_quantized_ops_with_standard_ops_pass import replace_quantized_ops_with_standard_ops as replace_quantized_ops_with_standard_ops
from torch.export.dynamic_shapes import Dim as Dim, _tree_map_with_path as _tree_map_with_path
from torch.export.exported_program import ExportedProgram as ExportedProgram
from torch.export.graph_signature import ConstantArgument as ConstantArgument, CustomObjArgument as CustomObjArgument, InputKind as InputKind, InputSpec as InputSpec, OutputKind as OutputKind, OutputSpec as OutputSpec, TensorArgument as TensorArgument
from torch.fx import subgraph_rewriter as subgraph_rewriter
from typing import Any, Callable

log: Incomplete

def _get_param_count_list(method_graph, args_params): ...
def _trace_and_get_graph_from_model(model, args): ...
def _create_jit_graph(model: torch.nn.Module | torch.jit.ScriptFunction, args: Sequence[Any]) -> tuple[torch.Graph, list['_C.IValue'], Any, torch.ScriptModule | None]: ...
def list_add(a, b): ...
def list_append(container, element): ...
def execute_subgraph_from_prim_loop(subgraph, iter_idx, len_loop_local_arguments, *args, **kwargs):
    """
    subgraph: GraphModule from sub-block.
    iter_idx: The index of interation.
    len_loop_local_arguments: The number of loop local arguments in args.
    """
def inplace_optimize_sym_size_div(gm: torch.fx.GraphModule): ...
def is_valid_for_codegen(name): ...
def normalize_name(name: str, prefix: str = 'rename') -> str: ...
def ir_name_to_func_name(name: str) -> str:
    """prim::If -> convert_prim_If"""
def get_node_as_placeholder_or_get_attr(fx_graph, name, is_top_level_graph): ...

_TORCH_DTYPE_TO_ENUM: Incomplete
_TORCH_ENUM_TO_DTYPE: Incomplete

def get_dtype_as_int(tensor):
    '''
    prim::dtype has the signature "Tensor a) -> int", where it gets the dtype of
    the tensor and returns the integer corresponding to this dtype based on the
    enum in ScalarType.h
    '''

kind_to_standard_operators: dict[str, Callable[..., Any]]

def get_ir_value_parent_name_and_attr_name(node): ...
def construct_fqn(ir, ref_map, name_map): ...
def get_block_to_lifted_attrs(graph: torch._C.Graph) -> tuple[dict[torch._C.Block, set[str]], dict[str, str]]:
    """
    Perform two passes to get a mapping of blocks to a set of FQNs of its lifted attributes.
    When a graph has control flow, the graph will be divided into multiple blocks. We want to convert
    each block to a graph which will be passed into torch.cond. A restriction for torch.cond is that model
    parameters/buffers are expected to be lifted as inputs to the subgraphs. Before converting the model,
    we will run this pass which will:
        1. Figure out which params/buffers are used within blocks through tracing the GetAttr calls.
        2. Process the graph bottom up to find the lifted attributes of each block by taking the union
        of the attributes used in the current block, and the lifted attributes of all its child blocks.

    Returns:
        A mapping of blocks to a set of FQNs of its lifted attributes, and a
        mapping of node names to the FQNs of its lifted attributes.
    """
def get_attribute_fqn_from_ts_node(name_to_attribute_fqn: dict[str, str], node: torch._C.Node) -> str: ...
def get_op_overload(node: torch._C.Node): ...

class TS2FXGraphConverter:
    ts_graph: Incomplete
    name_to_param: Incomplete
    name_to_buffer: Incomplete
    fx_graph: torch.fx.Graph
    input_specs: list[InputSpec]
    output_specs: list[OutputSpec]
    name_to_node: dict[str, torch.fx.Node | list[torch.fx.Node] | dict[Any, torch.fx.Node]]
    name_to_constant: dict[str, Any]
    name_to_attribute_fqn: dict[str, str]
    name_to_non_tensor_attribute_node: dict[str, Any]
    name_to_non_tensor_attribute: dict[str, Any]
    subgraphs: dict[str, torch.fx.GraphModule]
    blocks_to_lifted_attrs: Incomplete
    name_update_from_subblock_to_parent: set[str]
    def __init__(self, ts_graph: torch._C.Graph | torch._C.Block, name_to_param: dict[str, torch.Tensor], name_to_buffer: dict[str, torch.Tensor], blocks_to_lifted_attrs: dict[torch._C.Block, set[str]], name_to_non_tensor_attribute: dict[str, Any], name_to_constant: dict[str, Any], name_to_attribute_fqn: dict[str, str]) -> None: ...
    def _is_get_attr_node(self, fqn): ...
    def _convert_block_to_subgraph(self, node: torch._C.Node, arguments: list[str]): ...
    def _identify_inputs_as_arguments(self, entry):
        """
        Identify inputs from the innermost sub-block. This is needed
        for nested sub-blocks when the input is hidden in the nested sub-block.
        E.g., example IR of input is hidden in the nested sub-block.
        Graph[x.1]
        %1 = ...
            Block[]
                Block[x.1]
                    %2 = x.1 ...
        """
    def is_top_level_graph(self): ...
    def add_subgraph(self, subgraph) -> str: ...
    def get_args_kwargs(self, node: torch._C.Node, schema): ...
    def get_fx_value_by_ir_value(self, value: torch._C.Value): ...
    def get_fx_value_by_fqn(self, name): ...
    def convert(self) -> torch.fx.GraphModule: ...
    def convert_graph_inputs(self) -> None: ...
    def convert_aten_Float(self, node: torch._C.Node): ...
    def convert_aten_tensor(self, node: torch._C.Node):
        """aten::tensor creates a constant tensor ad-hoc --> GetAttr"""
    def convert_aten_append(self, node: torch._C.Node): ...
    def convert_prim_Constant(self, node: torch._C.Node): ...
    def convert_prim_CallMethod(self, node: torch._C.Node): ...
    def convert_prim_device(self, node: torch._C.Node): ...
    def convert_prim_GetAttr(self, node: torch._C.Node): ...
    def convert_prim_SetAttr(self, node: torch._C.Node): ...
    def convert_call_function_op(self, node: torch._C.Node): ...
    def convert_prim_TupleConstruct(self, node: torch._C.Node): ...
    def convert_prim_ListConstruct(self, node: torch._C.Node): ...
    def _convert_prim_iterator(self, node: torch._C.Node): ...
    def convert_prim_DictConstruct(self, node: torch._C.Node): ...
    def convert_prim_ListUnpack(self, node: torch._C.Node): ...
    def convert_prim_TupleUnpack(self, node: torch._C.Node): ...
    def _convert_prim_unpack_iterator(self, node: torch._C.Node): ...
    def convert_aten_Int(self, node: torch._C.Node): ...
    def convert_prim_NumToTensor(self, node: torch._C.Node): ...
    def convert_prim_CreateObject(self, node: torch._C.Node): ...
    def convert_aten__convolution(self, node: torch._C.Node): ...
    def convert_aten_div(self, node: torch._C.Node): ...
    def convert_aten___getitem__(self, node: torch._C.Node): ...
    def convert_aten_to(self, node: torch._C.Node): ...
    def convert_aten_add(self, node: torch._C.Node): ...
    def _check_prim_loop_support(self, node) -> None: ...
    def convert_prim_Loop(self, node: torch._C.Node): ...
    def _check_set_attr_in_if_block(self, if_node: torch._C.Node): ...
    def convert_prim_If(self, node: torch._C.Node): ...
    def convert_aten_Bool(self, node: torch._C.Node): ...
    def convert_prim_Enter(self, node: torch._C.Node): ...
    def convert_prim_Exit(self, node: torch._C.Node): ...
    def _convert_as_noop(self, node: torch._C.Node): ...
    def convert_profiler__record_function_exit(self, node: torch._C.Node): ...
    def convert_prim_tolist(self, node: torch._C.Node): ...
    def convert_prim_Uninitialized(self, node: torch._C.Node): ...
    def _convert_standard_operators(self, node: torch._C.Node): ...
    def convert_node(self, node: torch._C.Node): ...
    def convert_graph_outputs(self) -> None: ...

class ExplainTS2FXGraphConverter(TS2FXGraphConverter):
    """
    Run TS2FXGraphConverter in an explain mode. It collects all failed operators conversions
    and provide that information to users. In order to collect all failed conversions, it
    also mocks some internal attributes (e.g., name_to_node).
    """
    class _DictMock(dict):
        mock_value: Incomplete
        def __init__(self, dict_data, mock_value) -> None: ...
        def __getitem__(self, key): ...
        def __contains__(self, key) -> bool: ...
    unsupported_node_list: list[torch._C.Node]
    name_to_node: Incomplete
    def __init__(self, ts_graph: torch._C.Graph | torch._C.Block, name_to_param: dict[str, torch.Tensor], name_to_buffer: dict[str, torch.Tensor], blocks_to_lifted_attrs: dict[torch._C.Block, set[str]], name_to_non_tensor_attribute: dict[str, Any], name_to_constant: dict[str, Any], name_to_attribute_fqn: dict[str, str]) -> None: ...
    def explain(self) -> None: ...
    def convert_node(self, node) -> None: ...

@contextmanager
def disable_logging(log) -> Generator[None]: ...

class TS2EPConverter:
    ts_model: Incomplete
    sample_args: Incomplete
    sample_kwargs: Incomplete
    name_to_param: dict[str, torch.Tensor]
    name_to_buffer: dict[str, torch.Tensor]
    name_to_non_tensor_attributes: dict[str, Any]
    name_to_constant: dict[str, Any]
    def __init__(self, ts_model: torch.jit.ScriptModule | torch.jit.ScriptFunction, sample_args: tuple[Any, ...], sample_kwargs: dict[str, Any] | None = None) -> None: ...
    def convert(self) -> ExportedProgram: ...
    def explain(self, print_output: bool = True): ...
    def retrace_as_exported_program(self, gm: torch.fx.GraphModule, name_to_constant: dict[str, Any]): ...
    def lift_get_attr(self): ...
