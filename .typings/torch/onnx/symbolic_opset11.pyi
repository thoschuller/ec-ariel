import torch
from collections.abc import Sequence
from torch import _C
from torch.onnx._internal import jit_utils

__all__ = ['add', 'append', 'arange', 'argsort', 'atleast_1d', 'atleast_2d', 'atleast_3d', 'cat', 'chunk', 'clamp_max', 'clamp_min', 'clamp', 'constant_pad_nd', 'cumsum', 'Delete', 'embedding_bag', 'embedding_renorm', 'flatten', 'gather', 'hardtanh', 'hstack', 'im2col', 'index_fill', 'index', 'index_copy', 'index_put', 'insert', 'linalg_det', 'linalg_vector_norm', 'logdet', 'masked_scatter', 'masked_select', 'mm', 'narrow', 'normal', 'pad', 'pixel_shuffle', 'pop', 'prim_constant_chunk', 'reflection_pad', 'relu6', 'remainder', 'replication_pad', 'round', 'scatter', 'select', 'size', 'sort', 'split_with_sizes', 'split', 'squeeze', 'stack', 'topk', 'unbind', 'unique_dim', 'unsqueeze', 'vstack']

def hardtanh(g: jit_utils.GraphContext, self: _C.Value, min_val: float, max_val: float): ...
def clamp(g: jit_utils.GraphContext, self, min, max): ...
def clamp_min(g: jit_utils.GraphContext, self, min): ...
def clamp_max(g: jit_utils.GraphContext, self, max): ...
def relu6(g: jit_utils.GraphContext, input): ...
def select(g: jit_utils.GraphContext, self, dim, index): ...
def index_put(g: jit_utils.GraphContext, self, indices_list_value, values, accumulate: bool = False): ...
def pixel_shuffle(g: jit_utils.GraphContext, self, upscale_factor): ...
def gather(g: jit_utils.GraphContext, self, dim, index, sparse_grad: bool = False): ...
def scatter(g: jit_utils.GraphContext, self, dim, index, src): ...
def cumsum(g: jit_utils.GraphContext, self, dim, dtype=None): ...
def masked_select(g: jit_utils.GraphContext, self, mask): ...
def masked_scatter(g: jit_utils.GraphContext, self, mask, source): ...
def append(g: jit_utils.GraphContext, self, tensor): ...
def add(g: jit_utils.GraphContext, self, other, alpha=None): ...
def insert(g: jit_utils.GraphContext, self, pos, tensor): ...
def pop(g: jit_utils.GraphContext, tensor_list, dim): ...
def Delete(g: jit_utils.GraphContext, tensor_list, dim): ...
def cat(g: jit_utils.GraphContext, tensor_list, dim): ...
def stack(g: jit_utils.GraphContext, tensor_list, dim): ...
def unique_dim(g: jit_utils.GraphContext, self, dim, sorted, return_inverse, return_counts): ...
def topk(g: jit_utils.GraphContext, self, k, dim, largest, sorted, out=None): ...
def sort(g: jit_utils.GraphContext, self, dim, decending, out=None): ...
def argsort(g: jit_utils.GraphContext, self, dim, decending, out=None): ...
def round(g: jit_utils.GraphContext, self, decimals: int = 0): ...
def remainder(g: jit_utils.GraphContext, input, other): ...
def split(g: jit_utils.GraphContext, self, split_size_or_sizes, dim, _outputs=None): ...
def split_with_sizes(g: jit_utils.GraphContext, self, split_sizes, dim, _outputs=None): ...
def unbind(g: jit_utils.GraphContext, self, dim: int = 0, _outputs=None): ...
def constant_pad_nd(g: jit_utils.GraphContext, input, padding, value=None): ...
def reflection_pad(g: jit_utils.GraphContext, input, padding): ...
def replication_pad(g: jit_utils.GraphContext, input, padding): ...
def pad(g: jit_utils.GraphContext, input: _C.Value, pad: _C.Value, mode: _C.Value, value: _C.Value): ...
def linalg_det(g: jit_utils.GraphContext, self): ...
def logdet(g: jit_utils.GraphContext, input): ...
def arange(g: jit_utils.GraphContext, *args): ...
def size(g: jit_utils.GraphContext, self, dim=None): ...
def squeeze(g: jit_utils.GraphContext, self, dim=None): ...
def unsqueeze(g: jit_utils.GraphContext, self, dim): ...
def mm(g: jit_utils.GraphContext, self, other): ...
def index(g: jit_utils.GraphContext, self, index): ...
def index_fill(g: jit_utils.GraphContext, self, dim, index, value): ...
def index_copy(g: jit_utils.GraphContext, self, dim, index, source): ...
def im2col(g: jit_utils.GraphContext, input, kernel_size, dilation, padding, stride): ...
def narrow(g: jit_utils.GraphContext, input, dim, start, length): ...
def flatten(g: jit_utils.GraphContext, input, start_dim, end_dim): ...
def linalg_vector_norm(g: jit_utils.GraphContext, self, ord, dim: Sequence[int] | None, keepdim: bool, dtype): ...
def embedding_bag(g: jit_utils.GraphContext, embedding_matrix, indices, offsets, scale_grad_by_freq, mode, sparse, per_sample_weights, include_last_offset, padding_idx): ...
def embedding_renorm(g: jit_utils.GraphContext, weight, indices, max_norm, norm_type): ...
def chunk(g: jit_utils.GraphContext, self, chunks, dim): ...
def normal(g: jit_utils.GraphContext, mean, std, sizes=None, generator=None, dtype=None, layout=None, device=None, pin_memory=None): ...
def atleast_1d(g: jit_utils.GraphContext, self: torch._C.Value): ...
def atleast_2d(g: jit_utils.GraphContext, self: torch._C.Value): ...
def atleast_3d(g: jit_utils.GraphContext, self: torch._C.Value): ...
def prim_constant_chunk(g: jit_utils.GraphContext, self, chunks, dim): ...
def hstack(g: jit_utils.GraphContext, tensor_list: _C.Value): ...
def vstack(g: jit_utils.GraphContext, tensor_list: _C.Value): ...
