import torch
from _typeshed import Incomplete
from collections.abc import Sequence
from torch import _C
from torch.onnx._internal import jit_utils
from torch.types import Number
from typing import Callable

__all__ = ['abs', 'acos', 'add', 'addcmul', 'addmm', 'alias', 'amax', 'amin', 'aminmax', 'arange', 'argmax', 'argmin', 'as_strided', 'as_tensor', 'asin', 'atan', 'atan2', 'baddbmm', 'batch_norm', 'bernoulli', 'bitwise_not', 'bitwise_or', 'bmm', 'broadcast_tensors', 'broadcast_to', 'bucketize', 'cat', 'cdist', 'ceil', 'clamp_max', 'clamp_min', 'clamp', 'clone', 'constant_pad_nd', 'contiguous', 'conv_tbc', 'conv_transpose1d', 'conv_transpose2d', 'conv_transpose3d', 'conv1d', 'conv2d', 'conv3d', 'convert_element_type', 'convolution', 'cos', 'cosine_similarity', 'cross', 'cumsum', 'detach', 'dim', 'div', 'dot', 'dropout', 'elu', 'embedding_bag', 'embedding', 'empty_like', 'empty', 'eq', 'erf', 'exp', 'expand_as', 'expand', 'eye', 'fill', 'flatten', 'floor_divide', 'floor', 'floordiv', 'frobenius_norm', 'full_like', 'full', 'gather', 'ge', 'gelu', 'get_pool_ceil_padding', 'glu', 'group_norm', 'gt', 'hann_window', 'hardshrink', 'hardsigmoid', 'hardswish', 'hardtanh', 'index_add', 'index_copy', 'index_fill', 'index_put', 'index_select', 'index', 'instance_norm', 'is_floating_point', 'is_pinned', 'isnan', 'item', 'kl_div', 'layer_norm', 'le', 'leaky_relu', 'lerp', 'lift', 'linalg_cross', 'linalg_matrix_norm', 'linalg_norm', 'linalg_vector_norm', 'linear', 'linspace', 'log_sigmoid', 'log_softmax', 'log', 'log10', 'log1p', 'log2', 'logical_and', 'logical_not', 'logical_or', 'logical_xor', 'logit', 'logsumexp', 'lstm_cell', 'lstm', 'lt', 'masked_fill', 'masked_fill_', 'matmul', 'max_pool1d_with_indices', 'max_pool2d_with_indices', 'max_pool3d_with_indices', 'max', 'maximum', 'meshgrid', 'min', 'minimum', 'mish', 'mm', 'movedim', 'mse_loss', 'mul', 'multinomial', 'mv', 'narrow', 'native_layer_norm', 'ne', 'neg', 'new_empty', 'new_full', 'new_ones', 'new_zeros', 'nonzero_numpy', 'nonzero', 'norm', 'numel', 'numpy_T', 'one_hot', 'ones_like', 'ones', 'onnx_placeholder', 'pad', 'pairwise_distance', 'permute', 'pixel_shuffle', 'pixel_unshuffle', 'pow', 'prelu', 'prim_constant_chunk', 'prim_constant_split', 'prim_constant', 'prim_data', 'prim_device', 'prim_dtype', 'prim_if', 'prim_layout', 'prim_list_construct', 'prim_list_unpack', 'prim_loop', 'prim_max', 'prim_min', 'prim_shape', 'prim_tolist', 'prim_tuple_construct', 'prim_type', 'prim_unchecked_cast', 'prim_uninitialized', 'rand_like', 'rand', 'randint_like', 'randint', 'randn_like', 'randn', 'reciprocal', 'reflection_pad', 'relu', 'relu6', 'remainder', 'repeat_interleave', 'repeat', 'replication_pad', 'reshape_as', 'reshape', 'roll', 'rrelu', 'rsqrt', 'rsub', 'scalar_tensor', 'scatter_add', 'scatter', 'select', 'selu', 'sigmoid', 'sign', 'silu', 'sin', 'size', 'slice', 'softmax', 'softplus', 'softshrink', 'sort', 'split_with_sizes', 'split', 'sqrt', 'square', 'squeeze', 'stack', 'std_mean', 'std', 'sub', 't', 'take', 'tan', 'tanh', 'tanhshrink', 'tensor', 'threshold', 'to', 'topk', 'transpose', 'true_divide', 'type_as', 'unbind', 'unfold', 'unsafe_chunk', 'unsafe_split_with_sizes', 'unsafe_split', 'unsqueeze', 'unsupported_complex_operators', 'noop_complex_operators', 'unused', 'var_mean', 'var', 'view_as', 'view', 'where', 'wrap_logical_op_with_cast_to', 'wrap_logical_op_with_negation', 'zeros_like', 'zeros', 'zero', 'max_pool3d', 'max_pool2d', 'max_pool1d', 'avg_pool3d', 'avg_pool2d', 'avg_pool1d', 'adaptive_max_pool3d', 'adaptive_max_pool2d', 'adaptive_max_pool1d', 'adaptive_avg_pool3d', 'adaptive_avg_pool2d', 'adaptive_avg_pool1d', 'upsample_trilinear3d', 'upsample_bilinear2d', 'upsample_linear1d', 'upsample_nearest3d', 'upsample_nearest2d', 'upsample_nearest1d', 'rnn_relu', 'rnn_tanh', 'gru']

def unused(g):
    '''Represents "missing" optional inputs.'''
def reshape(g: jit_utils.GraphContext, self, shape): ...
def reshape_as(g: jit_utils.GraphContext, self, other): ...
def add(g: jit_utils.GraphContext, self, other, alpha=None):
    """
    This function takes the add function and returns the corresponding ONNX operator.

    This function is not meant to be called directly by the user.

    Args:
        g (GraphContext): The graph context.
        self (Tensor): The first operand.
        other (Tensor): The second operand.
        alpha (float, optional): The scaling factor for the second operand. Defaults to None.

    Returns:
        ONNX operator.
    """
def sub(g: jit_utils.GraphContext, self, other, alpha=None):
    """
    Consumes sub function and returns the corresponding ONNX operator.

    This function is not meant to be called directly by the user.

    Args:
        g (GraphContext): The graph context.
        self (Tensor): The first operand.
        other (Tensor): The second operand.
        alpha (Optional[Tensor]): A scaling factor to apply to the second operand.
            If `alpha` is not provided, it defaults to 1.

    Returns:
        ONNX operator
    """
def rsub(g: jit_utils.GraphContext, self, other, alpha=None): ...
def mul(g: jit_utils.GraphContext, self, other): ...
def div(g: jit_utils.GraphContext, self, other, *args): ...
def addcmul(g: jit_utils.GraphContext, self, tensor1, tensor2, value: float = 1.0): ...
def floor_divide(g: jit_utils.GraphContext, self, other): ...
def floordiv(g: jit_utils.GraphContext, self, other): ...
def true_divide(g: jit_utils.GraphContext, self, other):
    """Division where both inputs are cast to floating types

    If both inputs are floating, performs div as usual
    If only one input is a floating type, the other input is cast to its type
    If neither input is a floating type, both inputs are cast to the default scalar type
    """
def reciprocal(g: jit_utils.GraphContext, self): ...
def cat(g: jit_utils.GraphContext, tensor_list, dim):
    """Implement concatenation of pytorch tensors in ONNX along the specified `dim` dimension.

    Parameters:
        g (jit_utils.GraphContext): Graph context.
        tensor_list (List[torch.Tensor]): List of tensors to concatenate.
        dim (int): Dimension along which to concatenate the tensors.

    Returns:
        ONNX graph node representing the concatenated tensor.
    """
def stack(g: jit_utils.GraphContext, tensor_list, dim): ...
def mm(g: jit_utils.GraphContext, self, other): ...
def bmm(g: jit_utils.GraphContext, self, other): ...
def matmul(g: jit_utils.GraphContext, self, other): ...
def addmm(g: jit_utils.GraphContext, self, mat1, mat2, beta, alpha): ...
def neg(g: jit_utils.GraphContext, self): ...
def sqrt(g: jit_utils.GraphContext, self): ...
def rsqrt(g: jit_utils.GraphContext, self): ...
def tanh(g: jit_utils.GraphContext, self): ...
def sin(g: jit_utils.GraphContext, self): ...
def cos(g: jit_utils.GraphContext, self): ...
def tan(g: jit_utils.GraphContext, self): ...
def asin(g: jit_utils.GraphContext, self): ...
def acos(g: jit_utils.GraphContext, self): ...
def atan(g: jit_utils.GraphContext, self): ...
def atan2(g: jit_utils.GraphContext, self, other): ...
def sigmoid(g: jit_utils.GraphContext, self):
    """Converts the corresponding PyTorch function into ONNX operators.

    It is not meant to be called directly by a user.

    Args:
        g (jit_utils.GraphContext): Graph context.
        self (Tensor): the input tensor.
    Returns:
        ONNX operator
    """
def sign(g: jit_utils.GraphContext, self): ...
def cumsum(g: jit_utils.GraphContext, input, dim, dtype): ...
def t(g: jit_utils.GraphContext, self): ...
def numpy_T(g: jit_utils.GraphContext, input): ...
def expand(g: jit_utils.GraphContext, self, size, implicit):
    """Implement the expand function for a pytorch tensor in ONNX according to specified `size`"""
def broadcast_to(g: jit_utils.GraphContext, self, size): ...
def expand_as(g: jit_utils.GraphContext, self, other): ...
def embedding(g: jit_utils.GraphContext, weight, indices, padding_idx, scale_grad_by_freq, sparse): ...
def embedding_bag(g: jit_utils.GraphContext, embedding_matrix, indices, offsets, scale_grad_by_freq, mode, sparse, per_sample_weights, include_last_offset, padding_idx): ...
def size(g: jit_utils.GraphContext, self, dim=None): ...
def transpose(g: jit_utils.GraphContext, self, dim0, dim1): ...
def permute(g: jit_utils.GraphContext, self, dims): ...
def view(g: jit_utils.GraphContext, self, size): ...
def view_as(g: jit_utils.GraphContext, self, other): ...
def unsafe_chunk(g: jit_utils.GraphContext, self, chunks, dim, _outputs=None): ...
def split(g: jit_utils.GraphContext, self, split_size_or_sizes, dim, _outputs=None): ...
def unsafe_split(g: jit_utils.GraphContext, self, split_size_or_sizes, dim, _outputs=None): ...
def split_with_sizes(g: jit_utils.GraphContext, self, split_sizes, dim, _outputs=None): ...
def unsafe_split_with_sizes(g: jit_utils.GraphContext, self, split_sizes, dim, _outputs=None): ...
def unbind(g: jit_utils.GraphContext, self, dim: int = 0, _outputs=None): ...
def select(g: jit_utils.GraphContext, self, dim, index):
    """Implement the select functionality for a pytorch tensor in ONNX.

    Selects elements from the input tensor along the specified `dim` dimension based on the `index` tensor.
    """
def square(g: jit_utils.GraphContext, self): ...
def squeeze(g: jit_utils.GraphContext, self, dim=None): ...
def prelu(g: jit_utils.GraphContext, self, weight): ...
def silu(g: jit_utils.GraphContext, input): ...
def mish(g: jit_utils.GraphContext, input): ...
def relu(g: jit_utils.GraphContext, input): ...
def relu6(g: jit_utils.GraphContext, input): ...
def ceil(g: jit_utils.GraphContext, input): ...
def floor(g: jit_utils.GraphContext, input): ...
def threshold(g: jit_utils.GraphContext, self, threshold, value): ...
def leaky_relu(g: jit_utils.GraphContext, input: _C.Value, negative_slope: float, inplace: bool = False): ...
def glu(g: jit_utils.GraphContext, input, dim): ...
def softmax(g: jit_utils.GraphContext, input, dim, dtype=None): ...
def softplus(g: jit_utils.GraphContext, self, beta, threshold): ...
def get_pool_ceil_padding(input, kernel_size, stride, padding): ...

max_pool1d_with_indices: Incomplete
max_pool2d_with_indices: Incomplete
max_pool3d_with_indices: Incomplete

def constant_pad_nd(g: jit_utils.GraphContext, input, padding, value): ...
def reflection_pad(g: jit_utils.GraphContext, input, padding): ...
def replication_pad(g: jit_utils.GraphContext, input, padding): ...
def pad(g: jit_utils.GraphContext, input: _C.Value, pad: _C.Value, mode: _C.Value, value: _C.Value): ...
def bitwise_not(g: jit_utils.GraphContext, input): ...
def bitwise_or(g, self, other): ...
def wrap_logical_op_with_cast_to(to_type): ...
def wrap_logical_op_with_negation(func: Callable) -> Callable: ...
def eq(g: jit_utils.GraphContext, self, other): ...
@wrap_logical_op_with_negation
def ne(g: jit_utils.GraphContext, self, other): ...
def gt(g: jit_utils.GraphContext, input, other): ...
def lt(g: jit_utils.GraphContext, input, other): ...
@wrap_logical_op_with_negation
def ge(g: jit_utils.GraphContext, input, other): ...
@wrap_logical_op_with_negation
def le(g: jit_utils.GraphContext, input, other): ...
def logical_and(g: jit_utils.GraphContext, input, other): ...
def logical_or(g: jit_utils.GraphContext, input, other): ...
def logical_xor(g: jit_utils.GraphContext, input, other): ...
def logical_not(g: jit_utils.GraphContext, input): ...
def where(g: jit_utils.GraphContext, condition, self=None, other=None, _outputs=None): ...
def log_softmax(g: jit_utils.GraphContext, input, dim, dtype=None): ...
def convolution(g: jit_utils.GraphContext, input, weight, bias, stride, padding, dilation, transposed, output_padding, groups): ...
def conv1d(g: jit_utils.GraphContext, input, weight, bias, stride, padding, dilation, groups): ...
def conv2d(g: jit_utils.GraphContext, input, weight, bias, stride, padding, dilation, groups): ...
def conv3d(g: jit_utils.GraphContext, input, weight, bias, stride, padding, dilation, groups): ...
def conv_transpose1d(g: jit_utils.GraphContext, input, weight, bias, stride, padding, output_padding, groups, dilation): ...
def conv_transpose2d(g: jit_utils.GraphContext, input, weight, bias, stride, padding, output_padding, groups, dilation): ...
def conv_transpose3d(g: jit_utils.GraphContext, input, weight, bias, stride, padding, output_padding, groups, dilation): ...
def batch_norm(g: jit_utils.GraphContext, input, weight, bias, running_mean, running_var, training, momentum, eps, cudnn_enabled): ...
def native_layer_norm(g: jit_utils.GraphContext, input: _C.Value, normalized_shape: Sequence[int], weight: _C.Value, bias: _C.Value, eps: float) -> tuple[_C.Value, _C.Value, _C.Value]: ...
def layer_norm(g: jit_utils.GraphContext, input: _C.Value, normalized_shape: Sequence[int], weight: _C.Value, bias: _C.Value, eps: float, cudnn_enable: bool) -> _C.Value: ...
def instance_norm(g: jit_utils.GraphContext, input, weight, bias, running_mean, running_var, use_input_stats: bool, momentum: Number, eps: Number, cudnn_enabled: bool): ...
def unfold(g: jit_utils.GraphContext, input, dimension, size, step): ...
def elu(g: jit_utils.GraphContext, input, alpha, scale, input_scale): ...
def selu(g: jit_utils.GraphContext, input): ...
def index_select(g: jit_utils.GraphContext, self, dim, index): ...
def index_put(g: jit_utils.GraphContext, self, indices_list_value, values, accumulate): ...
def index_fill(g: jit_utils.GraphContext, self, dim, index, value): ...
def index_copy(g: jit_utils.GraphContext, self, dim, index, source): ...
def bucketize(g: jit_utils.GraphContext, self, boundaries, out_int32: bool = False, right: bool = False): ...
def type_as(g: jit_utils.GraphContext, self, other): ...
def cosine_similarity(g: jit_utils.GraphContext, x1, x2, dim, eps): ...
def pairwise_distance(g: jit_utils.GraphContext, input1, input2, p, eps, keepdim): ...
def clone(g: jit_utils.GraphContext, input, unused_memory_format): ...
def abs(g: jit_utils.GraphContext, self): ...
def log(g: jit_utils.GraphContext, self): ...
def log1p(g: jit_utils.GraphContext, self): ...
def log10(g: jit_utils.GraphContext, self): ...
def pow(g: jit_utils.GraphContext, self, exponent): ...
def clamp(g: jit_utils.GraphContext, self, min, max): ...
def clamp_min(g: jit_utils.GraphContext, self, min): ...
def clamp_max(g: jit_utils.GraphContext, self, max): ...
def max(g: jit_utils.GraphContext, self, dim_or_y=None, keepdim=None): ...
def maximum(g: jit_utils.GraphContext, input, other): ...
def min(g: jit_utils.GraphContext, self, dim_or_y=None, keepdim=None): ...
def minimum(g: jit_utils.GraphContext, input, other): ...
def amax(g: jit_utils.GraphContext, self, dim, keepdim): ...
def amin(g: jit_utils.GraphContext, self, dim, keepdim): ...
def aminmax(g: jit_utils.GraphContext, self, dim, keepdim): ...
def exp(g: jit_utils.GraphContext, self): ...
def dropout(g: jit_utils.GraphContext, input, p, train): ...
def norm(g: jit_utils.GraphContext, self, p, dim, keepdim, dtype=None): ...
def conv_tbc(g: jit_utils.GraphContext, input, weight, bias, pad): ...
def empty(g: jit_utils.GraphContext, sizes, dtype, layout, device, pin_memory: bool = False, memory_format=None): ...
def empty_like(g: jit_utils.GraphContext, input, dtype=None, layout=None, device=None, pin_memory: bool = False, memory_format=None): ...
def new_empty(g: jit_utils.GraphContext, self, sizes, dtype, layout, device, pin_memory: bool = False): ...
def scalar_tensor(g: jit_utils.GraphContext, scalar, dtype, *options): ...
def tensor(g: jit_utils.GraphContext, data, dtype=None, device=None, requires_grad: bool = False): ...
def as_tensor(g: jit_utils.GraphContext, data, dtype=None, device=None): ...
def zeros(g: jit_utils.GraphContext, sizes, dtype, layout, device, pin_memory: bool = False): ...
def zeros_like(g: jit_utils.GraphContext, input, dtype=None, layout=None, device=None, pin_memory: bool = False, memory_format=None): ...
def new_zeros(g: jit_utils.GraphContext, self, sizes, dtype, layout, device, pin_memory: bool = False): ...
def zero(g: jit_utils.GraphContext, self): ...
def ones(g: jit_utils.GraphContext, sizes, dtype, layout, device, pin_memory: bool = False): ...
def ones_like(g: jit_utils.GraphContext, input, dtype=None, layout=None, device=None, pin_memory: bool = False, memory_format=None): ...
def new_ones(g: jit_utils.GraphContext, self, sizes, dtype, layout, device, pin_memory: bool = False): ...
def full(g: jit_utils.GraphContext, sizes, value, dtype, layout, device, pin_memory: bool = False): ...
def full_like(g: jit_utils.GraphContext, input, fill_value, dtype=None, layout=None, device=None, pin_memory: bool = False, memory_format=None): ...
def new_full(g: jit_utils.GraphContext, self, size, fill_value, dtype, layout, device, pin_memory: bool = False): ...
def eye(g: jit_utils.GraphContext, *args): ...
def slice(g: jit_utils.GraphContext, self, *args): ...
def hardtanh(g: jit_utils.GraphContext, self: _C.Value, min_val: float, max_val: float): ...
def hardswish(g: jit_utils.GraphContext, self): ...
def hardsigmoid(g: jit_utils.GraphContext, self): ...
def tanhshrink(g: jit_utils.GraphContext, self): ...
def hardshrink(g: jit_utils.GraphContext, self, lambd): ...
def softshrink(g: jit_utils.GraphContext, self, lambd): ...
def alias(g: jit_utils.GraphContext, self): ...
def unsqueeze(g: jit_utils.GraphContext, self, dim):
    """Implement unsqueezing a pytorch tensor in ONNX by inserting a new dimension at the specified `dim`"""
def sort(g: jit_utils.GraphContext, self, dim, decending, out=None): ...
def numel(g: jit_utils.GraphContext, self): ...
def topk(g: jit_utils.GraphContext, self, k, dim, largest, sorted, out=None): ...
def convert_element_type(g: jit_utils.GraphContext, self, *args): ...
def to(g: jit_utils.GraphContext, self, *args): ...
def repeat(g: jit_utils.GraphContext, self, repeats): ...
def repeat_interleave(g: jit_utils.GraphContext, self, repeats, dim=None, output_size=None): ...
def pixel_shuffle(g: jit_utils.GraphContext, self, upscale_factor): ...
def pixel_unshuffle(g: jit_utils.GraphContext, self, downscale_factor): ...
def lstm(g: jit_utils.GraphContext, *args): ...
def lstm_cell(g: jit_utils.GraphContext, self, hidden, w_ih, w_hh, b_ih, b_hh): ...
def detach(g: jit_utils.GraphContext, input): ...
def contiguous(g: jit_utils.GraphContext, input, memory_format): ...
def randint(g: jit_utils.GraphContext, low, high, shapes, dtype, *options): ...
def randint_like(g: jit_utils.GraphContext, self, low, high, dtype, *options): ...
def randn(g: jit_utils.GraphContext, shapes, dtype, *options): ...
def rand(g: jit_utils.GraphContext, shapes, dtype, *options): ...
def randn_like(g: jit_utils.GraphContext, self, dtype, layout=None, device=None, pin_memory: bool = False, memory_format=None): ...
def rand_like(g: jit_utils.GraphContext, self, dtype, layout=None, device=None, pin_memory: bool = False, memory_format=None): ...
def rrelu(g: jit_utils.GraphContext, input, lower, upper, training, generator): ...
def bernoulli(g: jit_utils.GraphContext, input, p=None, generator=None, out=None): ...
def log_sigmoid(g: jit_utils.GraphContext, input): ...
def erf(g: jit_utils.GraphContext, input): ...
def flatten(g: jit_utils.GraphContext, input, start_dim, end_dim): ...
def nonzero(g: jit_utils.GraphContext, input):
    """Emitted from `torch.nonzero(x, as_tuple=False)`"""
def nonzero_numpy(g: jit_utils.GraphContext, input, _outputs=None): ...
def isnan(g: jit_utils.GraphContext, input): ...
def narrow(g: jit_utils.GraphContext, input, dim, start, length): ...
def argmax(g: jit_utils.GraphContext, input: torch._C.Value, dim: torch._C.Value, keepdim: bool): ...
def argmin(g: jit_utils.GraphContext, input: torch._C.Value, dim: torch._C.Value, keepdim: bool): ...
def scatter(g: jit_utils.GraphContext, self, dim, index, src): ...
def scatter_add(g: jit_utils.GraphContext, self, dim, index, src): ...
def log2(g: jit_utils.GraphContext, self): ...
def is_floating_point(g: jit_utils.GraphContext, self): ...
def one_hot(g: jit_utils.GraphContext, self, num_classes): ...
def gather(g: jit_utils.GraphContext, self, dim, index, sparse_grad: bool = False): ...
def std(g: jit_utils.GraphContext, input, *args): ...
def var(g: jit_utils.GraphContext, input, *args): ...
def var_mean(g: jit_utils.GraphContext, input, *args): ...
def std_mean(g: jit_utils.GraphContext, input, *args): ...
def logsumexp(g: jit_utils.GraphContext, input, dim, keepdim): ...
def arange(g: jit_utils.GraphContext, *args): ...
def linspace(g: jit_utils.GraphContext, start, end, steps, dtype, layout, device, pin_memory): ...
def lift(g: jit_utils.GraphContext, self): ...
def masked_fill(g: jit_utils.GraphContext, self, mask, value):
    """Implement the masked_fill functionality available for a pytorch tensor in ONNX.

    Fills elements of the input tensor with `value` where `mask` is True.
    """
def masked_fill_(g: jit_utils.GraphContext, self, mask, value): ...
def index(g: jit_utils.GraphContext, self, index): ...
def linalg_norm(g: jit_utils.GraphContext, self: torch._C.Value, ord: torch._C.Value, dim: Sequence[int] | None, keepdim: bool, dtype: torch._C.Value): ...
def linalg_vector_norm(g: jit_utils.GraphContext, self: torch._C.Value, ord: float, dim: Sequence[int] | None, keepdim: bool, dtype: torch._C.Value): ...
def linalg_matrix_norm(g: jit_utils.GraphContext, self: torch._C.Value, ord: torch._C.Value, dim: list[int], keepdim: bool, dtype: torch._C.Value): ...
def linalg_cross(g: jit_utils.GraphContext, input, other, dim: int = -1): ...
def frobenius_norm(g: jit_utils.GraphContext, self, dim=None, keepdim: bool = False): ...
def multinomial(g: jit_utils.GraphContext, input, num_samples, replacement: bool = False, generator=None): ...
def baddbmm(g: jit_utils.GraphContext, self, batch1, batch2, beta, alpha): ...
def meshgrid(g: jit_utils.GraphContext, tensor_list, indexing: str | None = None): ...
def remainder(g: jit_utils.GraphContext, input, other): ...
def gelu(g: jit_utils.GraphContext, self: torch._C.Value, approximate: str = 'none'): ...
def group_norm(g: jit_utils.GraphContext, input, num_groups, weight, bias, eps, cudnn_enabled): ...
def dim(g: jit_utils.GraphContext, self):
    """Implement the dim functionality available for a pytorch tensor in ONNX"""
def item(g: jit_utils.GraphContext, self): ...
def take(g: jit_utils.GraphContext, self, index): ...
def kl_div(g: jit_utils.GraphContext, input, target, reduction, log_target): ...
def mse_loss(g: jit_utils.GraphContext, input, target, reduction): ...
def as_strided(g: jit_utils.GraphContext, self, sizes, strides, offset=None): ...
def linear(g: jit_utils.GraphContext, input, weight, bias): ...
def hann_window(g: jit_utils.GraphContext, window_length, periodic: bool = True, dtype: int | None = None, layout=None, device=None, pin_memory=None, requires_grad: bool = False): ...
def mv(g: jit_utils.GraphContext, self, vec): ...
def dot(g: jit_utils.GraphContext, self, other): ...
def movedim(g: jit_utils.GraphContext, self, source, destination): ...
def fill(g: jit_utils.GraphContext, self, value): ...
def index_add(g: jit_utils.GraphContext, self, dim, index, other, alpha=None): ...
def roll(g: jit_utils.GraphContext, self, shifts, dims): ...
def cross(g: jit_utils.GraphContext, input, other, dim=None): ...
def cdist(g: jit_utils.GraphContext, x1, x2, p: float = 2.0, compute_mode: str = 'use_mm_for_euclid_dist_if_necessary'): ...
def lerp(g: jit_utils.GraphContext, self, end, weight): ...
def broadcast_tensors(g: jit_utils.GraphContext, self): ...
def is_pinned(g: jit_utils.GraphContext, self, device=None): ...
def prim_constant_split(g: jit_utils.GraphContext, self, split_size, dim): ...
def prim_constant_chunk(g: jit_utils.GraphContext, self, chunks, dim): ...
def prim_shape(g: jit_utils.GraphContext, self): ...
def prim_max(g: jit_utils.GraphContext, self, other): ...
def prim_min(g: jit_utils.GraphContext, self, other=None): ...
def prim_data(g: jit_utils.GraphContext, self): ...
def prim_layout(g: jit_utils.GraphContext, self): ...
def prim_list_construct(g: jit_utils.GraphContext, *inputs, **kwargs): ...
def prim_list_unpack(g: jit_utils.GraphContext, *inputs, **kwargs) -> list[_C.Value] | None: ...
def prim_tuple_construct(g: jit_utils.GraphContext, *inputs, **kwargs): ...
def prim_uninitialized(g: jit_utils.GraphContext, *inputs, **kwargs): ...
def prim_unchecked_cast(g: jit_utils.GraphContext, self): ...
def prim_dtype(g: jit_utils.GraphContext, self): ...
def prim_tolist(g: jit_utils.GraphContext, input, dim_val, elem_ty_val):
    """tolist is currently supported only for 1D input tensors.

    dim_val and elem_ty_val represent dimension and type annotations
    that need to match dimension and type of the input tensor.
    """
def prim_device(g: jit_utils.GraphContext, *inputs, **kwargs) -> None: ...
def prim_loop(g: jit_utils.GraphContext, *inputs, **attrs) -> list[_C.Value]: ...
def prim_if(g: jit_utils.GraphContext, *inputs, **attrs) -> list[_C.Value]: ...
def prim_constant(g: jit_utils.GraphContext, *inputs, **attrs): ...
def prim_type(g: jit_utils.GraphContext, device_value: _C.Value, *args, **kwargs): ...
def onnx_placeholder(g: jit_utils.GraphContext, *inputs, **attrs): ...
def noop_complex_operators(g: jit_utils.GraphContext, input: _C.Value): ...
def unsupported_complex_operators(g: jit_utils.GraphContext, input: _C.Value): ...
def logit(g: jit_utils.GraphContext, self: torch._C.Value, eps: torch._C.Value): ...

# Names in __all__ with no definition:
#   adaptive_avg_pool1d
#   adaptive_avg_pool2d
#   adaptive_avg_pool3d
#   adaptive_max_pool1d
#   adaptive_max_pool2d
#   adaptive_max_pool3d
#   avg_pool1d
#   avg_pool2d
#   avg_pool3d
#   gru
#   max_pool1d
#   max_pool2d
#   max_pool3d
#   rnn_relu
#   rnn_tanh
#   upsample_bilinear2d
#   upsample_linear1d
#   upsample_nearest1d
#   upsample_nearest2d
#   upsample_nearest3d
#   upsample_trilinear3d
