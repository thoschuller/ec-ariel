from _typeshed import Incomplete
from torch.onnx import _constants as _constants, _type_utils as _type_utils, errors as errors, symbolic_helper as symbolic_helper, utils as utils
from torch.onnx._internal import jit_utils as jit_utils, registration as registration

_onnx_symbolic: Incomplete

def softmax(g: jit_utils.GraphContext, input, dim, dtype=None): ...
def log_softmax(g: jit_utils.GraphContext, input, dim, dtype=None): ...
def frobenius_norm(g: jit_utils.GraphContext, self, dim=None, keepdim: bool = False): ...
def split(g: jit_utils.GraphContext, self, split_size_or_sizes, dim, _outputs=None): ...
def split_with_sizes(g: jit_utils.GraphContext, self, split_sizes, dim, _outputs=None): ...
def unsafe_split(g: jit_utils.GraphContext, self, split_size_or_sizes, dim, _outputs=None): ...
def unsafe_split_with_sizes(g: jit_utils.GraphContext, self, split_sizes, dim, _outputs=None): ...
def tensor_split(g: jit_utils.GraphContext, self, indices_or_sections, dim, _outputs=None): ...
def unbind(g: jit_utils.GraphContext, self, dim: int = 0, _outputs=None): ...
def nonzero_numpy(g: jit_utils.GraphContext, input, _outputs=None): ...
def where(g: jit_utils.GraphContext, condition, self=None, other=None, _outputs=None): ...
def fake_quantize_per_channel_affine(g: jit_utils.GraphContext, inputs, scale, zero_point, axis, quant_min: int = -128, quant_max: int = 127): ...
def fake_quantize_per_tensor_affine(g: jit_utils.GraphContext, inputs, scale, zero_point, quant_min: int = -128, quant_max: int = 127): ...
def _reduce_op_symbolic(onnx_op_name): ...
def _reduce_with_dtype(onnx_op, name): ...
def unflatten(g: jit_utils.GraphContext, input, dim, unflattened_size): ...
def unsafe_chunk(g: jit_utils.GraphContext, self, chunks, dim, _outputs=None): ...
def tile(g: jit_utils.GraphContext, self, dims): ...
def repeat_interleave(g: jit_utils.GraphContext, self, repeats, dim=None, output_size=None): ...
def diagonal(g: jit_utils.GraphContext, self, offset, dim1, dim2): ...
def quantized_linear(g: jit_utils.GraphContext, q_input, q_weight, bias, op_scale, op_zero_point): ...
def quantized_linear_relu(g: jit_utils.GraphContext, q_input, q_weight, bias, op_scale, op_zero_point): ...
def quantized_conv1d_relu(g: jit_utils.GraphContext, q_input, q_weight, bias, stride, padding, dilation, groups, op_scale, op_zero_point): ...
def quantized_conv2d_relu(g: jit_utils.GraphContext, q_input, q_weight, bias, stride, padding, dilation, groups, op_scale, op_zero_point): ...
def quantized_conv3d_relu(g: jit_utils.GraphContext, q_input, q_weight, bias, stride, padding, dilation, groups, op_scale, op_zero_point): ...
def quantized_conv1d(g: jit_utils.GraphContext, q_input, q_weight, bias, stride, padding, dilation, groups, op_scale, op_zero_point): ...
def quantized_conv2d(g: jit_utils.GraphContext, q_input, q_weight, bias, stride, padding, dilation, groups, op_scale, op_zero_point): ...
def quantized_conv3d(g: jit_utils.GraphContext, q_input, q_weight, bias, stride, padding, dilation, groups, op_scale, op_zero_point): ...
def quantized_conv_transpose1d(g: jit_utils.GraphContext, q_input, q_weight, bias, stride, padding, output_padding, dilation, groups, op_scale, op_zero_point): ...
def quantized_conv_transpose2d(g: jit_utils.GraphContext, q_input, q_weight, bias, stride, padding, output_padding, dilation, groups, op_scale, op_zero_point): ...
def quantized_conv_transpose3d(g: jit_utils.GraphContext, q_input, q_weight, bias, stride, padding, output_padding, dilation, groups, op_scale, op_zero_point): ...
