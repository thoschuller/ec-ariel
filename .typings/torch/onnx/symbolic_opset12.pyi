import torch
from torch.onnx._internal import jit_utils

__all__ = ['argmax', 'argmin', 'binary_cross_entropy_with_logits', 'celu', 'cross_entropy_loss', 'dropout', 'einsum', 'ge', 'le', 'native_dropout', 'nll_loss', 'nll_loss2d', 'nll_loss_nd', 'outer', 'pow', 'tensordot', 'unfold']

def einsum(g: jit_utils.GraphContext, equation, tensor_list, path=None): ...
def outer(g: jit_utils.GraphContext, input, other): ...
def dropout(g: jit_utils.GraphContext, input, p, train): ...
def native_dropout(g: jit_utils.GraphContext, input, p, train): ...
def nll_loss(g: jit_utils.GraphContext, self, target, weight, reduction, ignore_index): ...
def nll_loss2d(g: jit_utils.GraphContext, self, target, weight, reduction, ignore_index): ...
def nll_loss_nd(g: jit_utils.GraphContext, self, target, weight, reduction, ignore_index): ...
def cross_entropy_loss(g: jit_utils.GraphContext, self, target, weight, reduction, ignore_index, label_smoothing): ...
def binary_cross_entropy_with_logits(g: jit_utils.GraphContext, input, target, weight, pos_weight, reduction): ...
def celu(g: jit_utils.GraphContext, self, alpha): ...
def argmax(g: jit_utils.GraphContext, input: torch._C.Value, dim: torch._C.Value, keepdim: bool): ...
def argmin(g: jit_utils.GraphContext, input: torch._C.Value, dim: torch._C.Value, keepdim: bool): ...
def pow(g: jit_utils.GraphContext, self, exponent): ...
def ge(g: jit_utils.GraphContext, input, other): ...
def le(g: jit_utils.GraphContext, input, other): ...
def unfold(g: jit_utils.GraphContext, input, dimension, size, step): ...
def tensordot(g: jit_utils.GraphContext, input_a, input_b, dims_a, dims_b, out=None): ...
