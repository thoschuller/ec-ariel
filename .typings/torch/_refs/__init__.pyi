import torch
from _typeshed import Incomplete
from collections.abc import Sequence
from functools import singledispatch
from torch._prims_common import DeviceLikeType, DimsSequenceType, DimsType, NumberType, RealNumberType, ShapeType, StrideType, TensorLike, TensorLikeType, TensorOrNumberLikeType, TensorSequenceType
from typing import overload

__all__ = ['abs', 'acos', 'acosh', 'asinh', 'asin', 'atan', 'atanh', 'bitwise_not', 'ceil', 'conj_physical', 'cos', 'cosh', 'count_nonzero', 'deg2rad', 'digamma', 'erf', 'erfinv', 'erfc', 'exp', 'expm1', 'exponential', 'exp2', 'fill', 'fill_', 'floor', 'frac', 'geometric', 'index_add', 'index_copy', 'index_copy_', 'index_select', 'index_fill', 'index_fill_', 'isfinite', 'isinf', 'isposinf', 'isneginf', 'isnan', 'isreal', 'i0', 'lerp', 'lgamma', 'log', 'log1p', 'log2', 'log10', 'log_normal', 'log_softmax', 'mvlgamma', 'norm', 'normal', 'nan_to_num', 'neg', 'positive', 'rad2deg', 'reciprocal', 'round', 'sigmoid', 'sgn', 'sign', 'signbit', 'sin', 'sinc', 'sinh', 'softmax', 'sqrt', 'square', 'tan', 'tanh', 'trace', 'trunc', 'add', 'atan2', 'bitwise_and', 'bitwise_left_shift', 'bitwise_or', 'bitwise_right_shift', 'bitwise_xor', 'clamp_min', 'clamp_max', 'copysign', 'div', 'eq', 'float_power', 'floor_divide', 'fmax', 'fmin', 'fmod', 'gcd', 'ge', 'gt', 'heaviside', 'hypot', 'igamma', 'igammac', 'imag', 'isclose', 'lcm', 'le', 'logaddexp', 'logaddexp2', 'logical_and', 'logical_not', 'logical_or', 'logical_xor', 'logsumexp', 'lt', 'maximum', 'minimum', 'mul', 'ne', 'nextafter', 'pow', 'real', 'rpow', 'remainder', 'rsub', 'rtruediv', 'rfloordiv', 'sub', 'true_divide', 'trunc_divide', 'xlogy', 'addcdiv', 'addcmul', 'clamp', 'masked_fill', 'masked_fill_', 'where', 'clone', 'copy_to', 'item', 'to', 'all', 'amax', 'amin', 'any', 'cumsum', 'cumprod', 'mean', 'dot', 'vdot', 'std', 'std_mean', 'sum', 'sum_to_size', 'prod', 'var', 'var_mean', 'addr', 'alias', 'alias_copy', 'atleast_1d', 'atleast_2d', 'atleast_3d', 'as_strided', 'as_strided_copy', 'as_strided_scatter', 'block_diag', 'broadcast_shapes', 'broadcast_tensors', 'broadcast_to', 'cat', 'chunk', 'column_stack', 'conj', 'constant_pad_nd', 'contiguous', 'diag_embed', 'diag', 'diagonal', 'diagonal_copy', 'diagonal_scatter', 'dsplit', 'dstack', 'expand', 'expand_as', 'expand_copy', 'flatten', 'flip', 'fliplr', 'flipud', 'hsplit', 'hstack', 'meshgrid', 'movedim', 'narrow', 'narrow_copy', 'native_group_norm', 'native_layer_norm', 'permute', 'permute_copy', 'ravel', 'repeat', 'reshape', 'reshape_as', 'roll', 'rot90', 'rsqrt', 'split_with_sizes', 'stack', 'swap_axes', 'squeeze', 'squeeze_copy', 't', 't_copy', 'T', 'take_along_dim', 'tensor_split', 'transpose', 'transpose_copy', 'unbind_copy', 'unfold', 'unfold_copy', 'unsqueeze', 'unsqueeze_copy', 'view', 'view_as', 'view_copy', 'vsplit', 'vstack', 'view_as_complex', 'unflatten', 'unbind', 'triu', 'tril', 'triu_indices', 'tril_indices', 'arange', 'cauchy', 'empty', 'empty_like', 'empty_permuted', 'empty_strided', 'eye', 'full', 'full_like', 'linspace', 'logspace', 'new_empty', 'new_empty_strided', 'new_full', 'new_ones', 'new_zeros', 'ones', 'ones_like', 'randn', 'scalar_tensor', 'zero', 'zeros', 'zeros_like', 'allclose', 'equal', 'bucketize', 'is_complex', 'renorm', 'stft', 'istft', 'abs_', 'acos_', 'acosh_', 'add_', 'addcmul_', 'addcdiv_', 'asin_', 'asinh_', 'atan_', 'atanh_', 'atan2_', 'bitwise_and_', 'bitwise_left_shift_', 'bitwise_not_', 'bitwise_or_', 'bitwise_right_shift_', 'bitwise_xor_', 'ceil_', 'clamp_', 'clamp_min_', 'clamp_max_', 'conj_physical_', 'copysign_', 'cos_', 'cosh_', 'cumsum_', 'cumprod_', 'deg2rad_', 'digamma_', 'div_', 'eq_', 'erf_', 'erfc_', 'erfinv_', 'exp_', 'exp2_', 'expm1_', 'float_power_', 'floor_', 'floor_divide_', 'fmod_', 'frac_', 'gcd_', 'ge_', 'gt_', 'heaviside_', 'hypot_', 'igamma_', 'igammac_', 'i0_', 'lcm_', 'le_', 'lerp_', 'lgamma_', 'log10_', 'log1p_', 'log2_', 'log_', 'logical_and_', 'logical_not_', 'logical_or_', 'logical_xor_', 'lt_', 'mul_', 'mvlgamma_', 'nan_to_num_', 'ne_', 'neg_', 'nextafter_', 'pow_', 'rad2deg_', 'reciprocal_', 'remainder_', 'rsqrt_', 'sgn_', 'sigmoid_', 'sign_', 'sin_', 'sinc_', 'sinh_', 'sqrt_', 'square_', 'sub_', 'tan_', 'tanh_', 'tril_', 'triu_', 'true_divide_', 'trunc_', 'xlogy_', 'cauchy_', 'exponential_', 'geometric_', 'log_normal_', 'zero_']

Tensor = torch.Tensor
DispatchKey = torch._C.DispatchKey

def abs(a): ...
def acos(a): ...
def acosh(a): ...
def asin(a): ...
def asinh(a): ...
def atan(a): ...
def atanh(a): ...
def bitwise_not(a): ...
def ceil(a): ...
def is_complex(input: TensorLikeType): ...
def conj_physical(input: TensorLikeType): ...
def cos(a): ...
def cosh(a): ...
def digamma(a): ...
def erf(a): ...
def erfinv(a): ...
def erfc(a): ...
def exp(a): ...
def expm1(a): ...
def exp2(a): ...
def fill(a: TensorLikeType, value: NumberType) -> TensorLikeType: ...
def fill_(a: TensorLikeType, value: NumberType) -> TensorLikeType: ...
def zero(input: TensorLikeType) -> TensorLikeType: ...
def floor(a): ...
def frac(x: TensorLikeType) -> TensorLikeType: ...
def imag(a: TensorLikeType) -> TensorLikeType: ...
def isfinite(a: TensorLikeType) -> TensorLikeType: ...
def isinf(a: TensorLikeType) -> TensorLikeType: ...
def isposinf(a: TensorLikeType) -> TensorLikeType: ...
def isneginf(a: TensorLikeType) -> TensorLikeType: ...
def isnan(a: TensorLikeType) -> TensorLikeType: ...

mvlgamma: Incomplete

def isreal(a: TensorLikeType) -> TensorLikeType: ...
def i0(a): ...
def lgamma(a): ...
def log(a): ...
def log1p(a): ...
def log2(a): ...
def log10(a): ...
def log_softmax(a: TensorLikeType, dim: int, dtype: torch.dtype | None = None) -> TensorLikeType: ...
def logsumexp(self, dim: DimsType, keepdim: bool = False) -> TensorLikeType: ...
def nan_to_num(a: TensorLikeType, nan: NumberType | None = 0.0, posinf: NumberType | None = None, neginf: NumberType | None = None) -> TensorLikeType: ...
def neg(a): ...
def positive(a: TensorLikeType) -> TensorLikeType: ...
def real(a: TensorLikeType) -> TensorLikeType: ...
def reciprocal(a): ...
def round(a: TensorLikeType, *, decimals: int = 0) -> TensorLikeType: ...
def rsqrt(a): ...
def sigmoid(a: TensorLikeType) -> TensorLikeType: ...
def sgn(a): ...
def sign(a): ...
def signbit(a): ...
def sin(a): ...
def sinc(a): ...
def sinh(a): ...
def sqrt(a): ...
def square(a: TensorLikeType) -> TensorLikeType: ...
def tan(a): ...
def tanh(a): ...
def trunc(a): ...
def view_as_complex(self) -> TensorLikeType: ...
def add(a: TensorLikeType | NumberType, b: TensorLikeType | NumberType, *, alpha: NumberType | None = None):
    """
    Reference implementation of torch.add
    """
def atan2(a, b): ...
def bitwise_and(a: TensorLikeType, b: TensorLikeType) -> TensorLikeType: ...
def bitwise_left_shift(a: TensorLikeType, b: TensorLikeType) -> TensorLikeType: ...
def bitwise_or(a: TensorLikeType, b: TensorLikeType) -> TensorLikeType: ...
def bitwise_right_shift(a: TensorLikeType, b: TensorLikeType) -> TensorLikeType: ...
def bitwise_xor(a: TensorLikeType, b: TensorLikeType) -> TensorLikeType: ...
def copysign(a: TensorLikeType | NumberType, b: TensorLikeType | NumberType): ...
def div(a: TensorLikeType | NumberType, b: TensorLikeType | NumberType, *, rounding_mode: str | None = None):
    """
    Reference implementation of torch.div
    """
def eq(a: TensorLikeType, b: TensorLikeType) -> TensorLikeType: ...
def pow(a: TensorLikeType | NumberType, b: TensorLikeType | NumberType) -> TensorLikeType: ...
def float_power(a: TensorLikeType | NumberType, b: TensorLikeType | NumberType) -> Tensor: ...
def floor_divide(a: TensorLikeType | NumberType, b: TensorLikeType | NumberType): ...
def fmax(a: TensorLikeType, b: TensorLikeType) -> TensorLikeType: ...
def fmin(a: TensorLikeType, b: TensorLikeType) -> TensorLikeType: ...
def fmod(a: TensorLikeType, b: TensorLikeType) -> TensorLikeType: ...
def gcd(a: TensorLikeType, b: TensorLikeType) -> TensorLikeType: ...
def ge(a: TensorLikeType, b: TensorLikeType) -> TensorLikeType: ...
def gt(a: TensorLikeType, b: TensorLikeType) -> TensorLikeType: ...
def heaviside(input: TensorLikeType, values: TensorLikeType) -> TensorLikeType: ...
def hypot(a: TensorLikeType, b: TensorLikeType) -> TensorLikeType: ...
def igamma(a: TensorLikeType, b: TensorLikeType) -> TensorLikeType: ...
def igammac(a: TensorLikeType, b: TensorLikeType) -> TensorLikeType: ...
def isclose(a: TensorLikeType, b: TensorLikeType, rtol: float = 1e-05, atol: float = 1e-08, equal_nan: bool = False) -> TensorLikeType: ...
def lcm(a: TensorLikeType, b: TensorLikeType): ...
def le(a: TensorLikeType, b: TensorLikeType) -> TensorLikeType: ...
def logaddexp(a: TensorLikeType, b: TensorLikeType) -> TensorLikeType: ...
def logaddexp2(a: TensorLikeType, b: TensorLikeType) -> TensorLikeType: ...
def logical_and(a: TensorLikeType, b: TensorLikeType): ...
def logical_not(a: TensorLikeType): ...
def logical_or(a: TensorLikeType, b: TensorLikeType): ...
def logical_xor(a: TensorLikeType, b: TensorLikeType): ...
def lt(a: TensorLikeType, b: TensorLikeType) -> TensorLikeType: ...
def maximum(a: TensorLikeType, b: TensorLikeType) -> TensorLikeType: ...
def minimum(a: TensorLikeType, b: TensorLikeType) -> TensorLikeType: ...
def mul(a: TensorLikeType, b: TensorLikeType) -> TensorLikeType: ...
def ne(a: TensorLikeType, b: TensorLikeType) -> TensorLikeType: ...
def nextafter(a: TensorLikeType, b: TensorLikeType) -> TensorLikeType: ...
def remainder(a: TensorLikeType, b: TensorLikeType) -> TensorLikeType: ...
def rsub(a: TensorLikeType | NumberType, b: TensorLikeType | NumberType, alpha: NumberType = 1): ...
def sub(a: TensorLikeType | NumberType, b: TensorLikeType | NumberType, *, alpha: NumberType = 1):
    """
    Reference implementation of torch.sub
    """
def true_divide(a: TensorLikeType, b: TensorLikeType) -> TensorLikeType: ...
def xlogy(a: TensorLikeType | NumberType, b: TensorLikeType | NumberType): ...
def trunc_divide(a: TensorLikeType | NumberType, b: TensorLikeType | NumberType): ...
def addcdiv(self, tensor1: TensorLikeType, tensor2: TensorLikeType, *, value: NumberType = 1) -> TensorLikeType:
    """
    Reference implementation of torch.addcdiv
    """
def addcmul(self, tensor1: TensorLikeType, tensor2: TensorLikeType, *, value: NumberType = 1) -> TensorLikeType:
    """
    Reference implementation of torch.addcmul
    """
def clamp(a: TensorLikeType, min: TensorOrNumberLikeType | None = None, max: TensorOrNumberLikeType | None = None) -> TensorLikeType: ...
def clamp_min(self, min: TensorOrNumberLikeType | None = None) -> TensorLikeType: ...
def clamp_max(self, max: TensorOrNumberLikeType | None = None) -> TensorLikeType: ...
def where(pred: Tensor, a: TensorOrNumberLikeType | None = None, b: TensorOrNumberLikeType | None = None):
    """ """
def clone(a: TensorLikeType, *, memory_format: torch.memory_format = ...) -> TensorLikeType: ...
def copy_to(a: Tensor, b: Tensor, *, allow_cross_device: bool = True): ...
def item(a: TensorLikeType) -> NumberType: ...
def to(a: TensorLikeType, *args, **kwargs) -> TensorLikeType: ...
def all(a: TensorLikeType, dim: DimsType | None = None, keepdim: bool = False) -> TensorLikeType: ...
def any(a: TensorLikeType, dim: DimsType | None = None, keepdim: bool = False) -> TensorLikeType: ...
def sum(a: TensorLikeType, dim: int | None | list[int] | None = None, keepdim: bool = False, *, dtype: torch.dtype | None = None, out: Tensor | None = None) -> TensorLikeType: ...
def sum_to_size(a: Tensor, *shape) -> Tensor: ...
def prod(a: TensorLikeType, dim: int | None | list[int] | None = None, keepdim: bool = False, *, dtype=None, out: Tensor | None = None) -> TensorLikeType: ...
def amin(a: TensorLikeType, dim: DimsType | None = None, keepdim: bool = False, *, out: Tensor | None = None) -> TensorLikeType: ...
def amax(a: TensorLikeType, dim: DimsType | None = None, keepdim: bool = False, *, out: Tensor | None = None) -> TensorLikeType: ...
def var(a: TensorLikeType, dim: DimsType | None = None, unbiased: bool | None = None, keepdim: bool = False, *, correction: NumberType | None = None) -> TensorLikeType: ...
def std(a: TensorLikeType, dim: int | None | list[int] | None = None, unbiased: bool | None = None, keepdim: bool = False, *, correction: NumberType | None = None) -> TensorLikeType: ...
def mean(a: TensorLikeType, dim: DimsType | None = None, keepdim: bool = False, *, dtype=None, out=None) -> TensorLikeType: ...
def std_mean(a: TensorLikeType, dim: DimsType | None = None, *, unbiased: bool | None = None, keepdim: bool = False, correction: NumberType | None = None): ...
def var_mean(a: TensorLikeType, dim: DimsType | None = None, unbiased: bool | None = None, keepdim: bool = False, *, correction: NumberType | None = None): ...
def addr(self, vec1: TensorLikeType, vec2: TensorLikeType, *, beta: NumberType = 1, alpha: NumberType = 1) -> TensorLikeType: ...
def atleast_1d(arg: TensorLikeType | Sequence[TensorLikeType], *args: TensorLikeType) -> TensorLikeType | tuple[TensorLikeType, ...]:
    """Reference implementation of :func:`torch.atleast_1d`."""
def atleast_2d(arg: TensorLikeType | Sequence[TensorLikeType], *args: TensorLikeType) -> TensorLikeType | tuple[TensorLikeType, ...]:
    """Reference implementation of :func:`torch.atleast_2d`."""
def atleast_3d(arg: TensorLikeType | Sequence[TensorLikeType], *args: TensorLikeType) -> TensorLikeType | tuple[TensorLikeType, ...]:
    """Reference implementation of :func:`torch.atleast_3d`."""
def as_strided(a: TensorLikeType, size: ShapeType, stride: StrideType, storage_offset: int | None = None) -> TensorLikeType: ...
def as_strided_scatter(input: TensorLikeType, src: TensorLikeType, size: ShapeType, stride: StrideType, storage_offset: int | None = None) -> TensorLikeType: ...
def broadcast_shapes(*shapes) -> ShapeType: ...
def broadcast_tensors(*tensors) -> list[TensorLikeType]: ...
def broadcast_to(a: TensorLikeType, size: ShapeType) -> TensorLikeType: ...
def cat(tensors: TensorSequenceType, dim: int = 0) -> TensorLikeType: ...
def column_stack(tensors: TensorSequenceType) -> TensorLikeType: ...
def conj(input: TensorLikeType) -> TensorLikeType: ...
def constant_pad_nd(input: TensorLikeType, pad: list[int], value: NumberType = 0) -> TensorLikeType: ...
def contiguous(a: Tensor, *, memory_format: torch.memory_format = ...) -> Tensor: ...
def dstack(tensors: TensorSequenceType) -> TensorLikeType: ...
def expand(a: Tensor, *shape) -> Tensor: ...
def expand_as(a: Tensor, b: Tensor) -> Tensor: ...
def chunk(a: TensorLikeType, chunks: int, dim: int = 0) -> tuple[TensorLikeType, ...]: ...
def flatten(a: TensorLikeType, start_dim: int = 0, end_dim: int = -1) -> TensorLikeType: ...
def flip(a: TensorLikeType, dims: DimsSequenceType) -> TensorLikeType: ...
def fliplr(a: TensorLikeType) -> TensorLikeType: ...
def flipud(a: TensorLikeType) -> TensorLikeType: ...
def narrow(a: TensorLikeType, dim: int, start: int | TensorLikeType, length: int) -> TensorLikeType: ...
def native_group_norm(input: Tensor, weight: Tensor | None, bias: Tensor | None, batch_size: int, num_channels: int, flattened_inner_size: int, num_groups: int, eps: float) -> tuple[Tensor, Tensor, Tensor]: ...
def native_layer_norm(input: Tensor, normalized_shape: ShapeType, weight: Tensor | None, bias: Tensor | None, eps: float) -> tuple[Tensor, Tensor, Tensor]: ...
def permute(a: TensorLikeType, *dims) -> TensorLikeType: ...
def renorm(input: TensorLikeType, p: RealNumberType, dim: int, maxnorm: RealNumberType) -> TensorLikeType: ...
def stft(input: Tensor, n_fft: int, hop_length: int | None = None, win_length: int | None = None, window: Tensor | None = None, center: bool = True, pad_mode: str = 'reflect', normalized: bool = False, onesided: bool | None = None, return_complex: bool | None = None, align_to_window: bool | None = None) -> Tensor: ...
def istft(input: Tensor, n_fft: int, hop_length: int | None = None, win_length: int | None = None, window: Tensor | None = None, center: bool = True, normalized: bool = False, onesided: bool | None = None, length: int | None = None, return_complex: bool = False) -> Tensor: ...
def repeat(a: Tensor, *repeat_shape) -> Tensor: ...
def reshape(a: TensorLikeType, *shape: ShapeType) -> TensorLikeType: ...
def reshape_as(self, other: TensorLikeType) -> TensorLikeType: ...
def roll(a: TensorLikeType, shifts: DimsType, dims: DimsType = ()) -> TensorLikeType:
    """Reference implementation of :func:`torch.roll`."""
def rot90(a: TensorLikeType, k: int = 1, dims: DimsSequenceType = (0, 1)) -> TensorLikeType:
    """Reference implementation of :func:`torch.rot90`."""
def stack(tensors: TensorSequenceType, dim: int = 0) -> TensorLikeType: ...
def softmax(a: TensorLikeType, dim: int, dtype: torch.dtype | None = None) -> TensorLikeType: ...
def hstack(tensors: TensorSequenceType) -> TensorLikeType: ...
def vstack(tensors: TensorSequenceType) -> TensorLikeType: ...
def unflatten(a: TensorLikeType, dim: int, sizes: ShapeType) -> TensorLikeType: ...
def unbind(t: TensorLikeType, dim: int = 0) -> TensorSequenceType: ...
def index_copy(x: TensorLike, dim: int, index: TensorLike, tensor: TensorLike): ...
def index_copy_(x: TensorLike, dim: int, index: TensorLike, tensor: TensorLike): ...
def index_fill(x: TensorLike, dim: int, index: TensorLike, value: NumberType | TensorLike): ...
def index_fill_(x: TensorLike, dim: int, index: TensorLike, value: NumberType | TensorLike): ...
def index_add(x: TensorLike, dim: int, index: TensorLike, tensor: TensorLike, *, alpha: NumberType = 1): ...
def index_select(x: TensorLike, dim: int, index: TensorLike): ...
def squeeze(a: TensorLikeType, dim: DimsType | None = None) -> TensorLikeType: ...
def split_with_sizes(self, split_sizes: list[int], dim: int = 0) -> list[Tensor]: ...
def tensor_split(a: TensorLikeType, indices_or_sections: Tensor | DimsType, dim: int = 0) -> tuple[TensorLikeType, ...]: ...
def hsplit(a: TensorLikeType, indices_or_sections: DimsType) -> tuple[TensorLikeType, ...]: ...
def vsplit(a: TensorLikeType, indices_or_sections: DimsType) -> tuple[TensorLikeType, ...]: ...
def diag(self, offset: int = 0) -> TensorLikeType: ...
def diagonal_scatter(input: TensorLikeType, src: TensorLikeType, offset: int = 0, dim1: int = 0, dim2: int = 1) -> TensorLikeType: ...
def diagonal(self, offset: int = 0, dim1: int = 0, dim2: int = 1) -> TensorLikeType:
    """
    Reference implementation of torch.diagonal
    """
def diag_embed(t: TensorLikeType, offset: int = 0, dim1: int = -2, dim2: int = -1) -> TensorLikeType:
    """
    Reference implementation of torch.diag_embed
    """
def block_diag(*tensors: list[TensorLikeType]) -> TensorLikeType:
    """
    This is used as an input to PythonRefInfo. `torch.block_diag`
    expects arguments splatted, but `aten.block_diag` expects only
    one argument that is a list of Tensors.
    """
def dsplit(a: TensorLikeType, sections: DimsType) -> TensorSequenceType: ...
def t(a: TensorLikeType): ...
def T(a: TensorLikeType) -> TensorLikeType: ...
def alias(a: TensorLikeType) -> TensorLikeType: ...
def transpose(a: TensorLikeType, dim0: int, dim1: int) -> TensorLikeType: ...
swap_axes = transpose

def unfold(self, dimension: int, size: int, step: int) -> TensorLikeType: ...
def unfold_copy(self, dimension: int, size: int, step: int): ...
def cumsum(a: TensorLikeType, dim: int, *, dtype: torch.dtype | None = None, out: Tensor | None = None) -> TensorLikeType: ...
def cumprod(a: TensorLikeType, dim: int, *, dtype: torch.dtype | None = None, out: Tensor | None = None) -> TensorLikeType: ...
def unsqueeze(a: TensorLikeType, dim: int) -> TensorLikeType: ...
def view(a: TensorLikeType, *shape: ShapeType) -> TensorLikeType: ...
def view_as(self, other: TensorLikeType) -> TensorLikeType: ...
def ravel(a: TensorLikeType) -> TensorLikeType: ...
def take_along_dim(a: torch.Tensor, indices: torch.Tensor, dim: int | None = None) -> torch.Tensor: ...
def empty(*shape, dtype: torch.dtype | None = None, layout: torch.layout = ..., device: DeviceLikeType | None = None, requires_grad: bool = False, pin_memory: bool = False, memory_format: torch.memory_format = ...) -> TensorLikeType: ...
def empty_permuted(shape, physical_layout, dtype: torch.dtype | None = None, layout: torch.layout = ..., device: DeviceLikeType | None = None, requires_grad: bool = False, pin_memory: bool = False) -> TensorLikeType: ...
def new_empty(a: TensorLikeType, size: ShapeType, *, dtype: torch.dtype | None = None, layout: torch.layout | None = None, device: DeviceLikeType | None = None, pin_memory: bool = False) -> TensorLikeType: ...
def new_empty_strided(a: TensorLikeType, size: ShapeType, stride: StrideType, *, dtype: torch.dtype | None = None, layout: torch.layout | None = None, device: DeviceLikeType | None = None, pin_memory: bool = False) -> TensorLikeType:
    """
    Reference implementation of torch.Tensor.new_empty_strided
    """
def zeros(*size, dtype: torch.dtype | None = None, layout: torch.layout = ..., device: DeviceLikeType | None = None, pin_memory: bool = False, requires_grad: bool = False) -> TensorLikeType: ...
def new_zeros(a: TensorLikeType, size: ShapeType, *, dtype: torch.dtype | None = None, layout: torch.layout | None = None, device: DeviceLikeType | None = None, pin_memory: bool = False, requires_grad: bool = False) -> TensorLikeType: ...
def ones(*size, dtype: torch.dtype | None = None, layout: torch.layout = ..., device: DeviceLikeType | None = None, pin_memory: bool = False, requires_grad: bool = False) -> TensorLikeType: ...
def new_ones(a: TensorLikeType, size: ShapeType, *, dtype: torch.dtype | None = None, layout: torch.layout | None = None, device: DeviceLikeType | None = None, pin_memory: bool = False, requires_grad: bool = False) -> TensorLikeType: ...
def new_full(a: TensorLikeType, size: ShapeType, fill_value: NumberType, *, dtype: torch.dtype | None = None, layout: torch.layout | None = None, device: DeviceLikeType | None = None, pin_memory: bool = False) -> TensorLikeType: ...
def empty_like(a: TensorLikeType, *, dtype: torch.dtype | None = None, device: DeviceLikeType | None = None, layout: torch.layout | None = None, pin_memory: bool = False, requires_grad: bool = False, memory_format: torch.memory_format = ...) -> TensorLikeType: ...
def arange(start: NumberType = 0, end: NumberType | None = None, step: NumberType = 1, *, dtype: torch.dtype | None = None, layout: torch.layout = ..., device: DeviceLikeType | None = None, pin_memory: bool = False, requires_grad: bool = False) -> TensorLikeType: ...
def lerp(start: Tensor, end: Tensor, weight: Tensor | NumberType): ...
def linspace(start: NumberType | TensorLikeType, end: NumberType | TensorLikeType, steps: NumberType, *, dtype: torch.dtype | None = None, device: DeviceLikeType | None = None, layout: torch.layout = ..., pin_memory: bool = False, requires_grad: bool = False) -> TensorLikeType: ...
def logspace(start: NumberType | TensorLikeType, end: NumberType | TensorLikeType, steps: NumberType, base: NumberType = 10, *, dtype: torch.dtype | None = None, device: DeviceLikeType | None = None, layout: torch.layout = ..., pin_memory: bool = False, requires_grad: bool = False) -> TensorLikeType: ...
@overload
def meshgrid(tensors: Sequence[TensorLikeType], indexing: str): ...
@overload
def meshgrid(*tensors: TensorLikeType, indexing: str): ...
def movedim(input: TensorLikeType, source: int | DimsSequenceType, destination: int | DimsSequenceType) -> TensorLikeType:
    """
    Reference implementation of torch.movedim
    """
def empty_strided(shape: ShapeType | tuple[ShapeType], strides: StrideType, *, dtype: torch.dtype | None = None, device: DeviceLikeType | None = None, layout: torch.layout = ..., requires_grad: bool = False, pin_memory: bool = False) -> TensorLikeType: ...
def eye(n: int, m: int | None = None, *, dtype: torch.dtype | None = None, layout: torch.layout = ..., device: DeviceLikeType | None = None, pin_memory: bool = False, requires_grad: bool = False) -> TensorLikeType:
    """
    Reference implementation of torch.eye
    """
def full(shape: ShapeType, fill_value: NumberType, *, dtype: torch.dtype | None = None, layout: torch.layout = ..., device: DeviceLikeType | None = None, pin_memory: bool = False, requires_grad: bool = False) -> TensorLikeType: ...
def full_like(a: TensorLikeType, fill_value: NumberType, *, dtype: torch.dtype | None = None, layout: torch.layout | None = None, device: DeviceLikeType | None = None, pin_memory: bool = False, requires_grad: bool = False, memory_format: torch.memory_format = ...) -> TensorLikeType: ...
def zeros_like(a: TensorLikeType, *, dtype: torch.dtype | None = None, layout: torch.layout | None = None, device: DeviceLikeType | None = None, pin_memory: bool = False, requires_grad: bool = False, memory_format: torch.memory_format = ...) -> TensorLikeType: ...
def ones_like(a: TensorLikeType, *, dtype: torch.dtype | None = None, layout: torch.layout | None = None, device: DeviceLikeType | None = None, pin_memory: bool = False, requires_grad: bool = False, memory_format: torch.memory_format = ...) -> TensorLikeType: ...
def randn(*shape, dtype: torch.dtype | None = None, device: DeviceLikeType | None = None, layout: torch.layout | None = None, requires_grad: bool = False, pin_memory: bool = False) -> TensorLikeType: ...
def scalar_tensor(a: NumberType, *, dtype: torch.dtype | None = None, layout: torch.layout = ..., device: DeviceLikeType | None = None, pin_memory: bool = False) -> TensorLikeType: ...
def masked_fill(a: TensorLikeType, mask: TensorLikeType, value: TensorOrNumberLikeType): ...
def masked_fill_(a: TensorLikeType, mask: TensorLikeType, value: TensorOrNumberLikeType) -> TensorLikeType: ...
def allclose(a: TensorLikeType, b: TensorLikeType, rtol: float = 1e-05, atol: float = 1e-08, equal_nan: bool = False) -> bool:
    """
    Reference implementation of torch.allclose
    """
def equal(a: TensorLikeType, b: TensorLikeType) -> bool: ...
def norm(input: TensorLikeType, p: float | str | None = 'fro', dim: DimsType | None = None, keepdim: bool = False, *, dtype: torch.dtype | None = None) -> TensorLikeType: ...
def trace(self) -> TensorLikeType: ...

rtruediv: Incomplete
rfloordiv: Incomplete
rpow: Incomplete

def triu(a: TensorLikeType, diagonal: int = 0) -> TensorLikeType: ...
def tril(a: TensorLikeType, diagonal: int = 0) -> TensorLikeType: ...
def tril_indices(row: int, col: int, offset: int = 0, *, dtype: torch.dtype = ..., layout: torch.layout = ..., device: DeviceLikeType = 'cpu', pin_memory: bool = False) -> TensorLikeType: ...
def triu_indices(row: int, col: int, offset: int = 0, *, dtype: torch.dtype = ..., layout: torch.layout = ..., device: DeviceLikeType = 'cpu', pin_memory: bool = False) -> TensorLikeType: ...
def bucketize(a: TensorOrNumberLikeType, boundaries: TensorLikeType, *, out_int32: bool = False, right: bool = False): ...
def cauchy(self, median: int = 0, sigma: int = 1, generator=None): ...
def exponential(self, rate: int = 1, generator=None): ...
def geometric(self, p, generator=None): ...
def log_normal(self, mean: int = 1, std: int = 2, generator=None): ...
def normal(mean: int = 0, std: int = 1, size=None, *, generator=None, dtype=None, layout=None, device=None, pin_memory=None): ...
def rad2deg(self): ...
def deg2rad(self): ...
def count_nonzero(self, dim: DimsType | None = None): ...
@_dot_check_wrapper
def dot(self, other): ...
@_dot_check_wrapper
def vdot(self, other): ...

abs_: Incomplete
acos_: Incomplete
acosh_: Incomplete
add_: Incomplete
addcmul_: Incomplete
addcdiv_: Incomplete
asin_: Incomplete
asinh_: Incomplete
atan_: Incomplete
atanh_: Incomplete
atan2_: Incomplete
bitwise_and_: Incomplete
bitwise_left_shift_: Incomplete
bitwise_not_: Incomplete
bitwise_or_: Incomplete
bitwise_right_shift_: Incomplete
bitwise_xor_: Incomplete
ceil_: Incomplete
clamp_: Incomplete
clamp_min_: Incomplete
clamp_max_: Incomplete
conj_physical_: Incomplete
copysign_: Incomplete
cos_: Incomplete
cosh_: Incomplete
cumsum_: Incomplete
cumprod_: Incomplete
deg2rad_: Incomplete
digamma_: Incomplete
div_: Incomplete
eq_: Incomplete
erf_: Incomplete
erfc_: Incomplete
erfinv_: Incomplete
exp_: Incomplete
exp2_: Incomplete
expm1_: Incomplete
float_power_: Incomplete
floor_: Incomplete
floor_divide_: Incomplete
fmod_: Incomplete
frac_: Incomplete
gcd_: Incomplete
ge_: Incomplete
gt_: Incomplete
heaviside_: Incomplete
hypot_: Incomplete
igamma_: Incomplete
igammac_: Incomplete
i0_: Incomplete
lcm_: Incomplete
le_: Incomplete
lerp_: Incomplete
lgamma_: Incomplete
log10_: Incomplete
log1p_: Incomplete
log2_: Incomplete
log_: Incomplete
logical_and_: Incomplete
logical_not_: Incomplete
logical_or_: Incomplete
logical_xor_: Incomplete
lt_: Incomplete
mul_: Incomplete
mvlgamma_: Incomplete
nan_to_num_: Incomplete
ne_: Incomplete
neg_: Incomplete
nextafter_: Incomplete
pow_: Incomplete
rad2deg_: Incomplete
reciprocal_: Incomplete
remainder_: Incomplete
rsqrt_: Incomplete
sgn_: Incomplete
sigmoid_: Incomplete
sign_: Incomplete
sin_: Incomplete
sinc_: Incomplete
sinh_: Incomplete
sqrt_: Incomplete
square_: Incomplete
sub_: Incomplete
tan_: Incomplete
tanh_: Incomplete
tril_: Incomplete
triu_: Incomplete
true_divide_: Incomplete
trunc_: Incomplete
xlogy_: Incomplete
cauchy_: Incomplete
exponential_: Incomplete
geometric_: Incomplete
log_normal_: Incomplete
zero_: Incomplete
alias_copy: Incomplete
as_strided_copy: Incomplete
diagonal_copy: Incomplete
expand_copy: Incomplete
narrow_copy: Incomplete
squeeze_copy: Incomplete
permute_copy: Incomplete
t_copy: Incomplete
transpose_copy: Incomplete
unbind_copy: Incomplete
unsqueeze_copy: Incomplete
view_copy: Incomplete
